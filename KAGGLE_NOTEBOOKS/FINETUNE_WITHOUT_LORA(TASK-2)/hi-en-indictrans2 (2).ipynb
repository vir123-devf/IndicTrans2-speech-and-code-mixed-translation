{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# In this Notebook:- \n* We fine-tuned the Indictrans2-indic-en-dist-200M model on the mini-IITB English–Hindi parallel corpus for the HI->EN translation task. The resulting model was evaluated using a comprehensive suite of MT quality metrics, including BLEU, ChrF, COMET, BERTScore, and BLEURT.","metadata":{}},{"cell_type":"code","source":"#Checking wheather GPU is working or not\n!nvidia-smi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:46:37.661195Z","iopub.execute_input":"2025-12-12T11:46:37.661536Z","iopub.status.idle":"2025-12-12T11:46:38.026342Z","shell.execute_reply.started":"2025-12-12T11:46:37.661512Z","shell.execute_reply":"2025-12-12T11:46:38.025544Z"}},"outputs":[{"name":"stdout","text":"Fri Dec 12 11:46:37 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   71C    P0             31W /   70W |    6145MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   70C    P0             30W /   70W |     103MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# installing dataset and transformer\n!pip install datasets transformers[sentencepiece] sacrebleu -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:46:38.028009Z","iopub.execute_input":"2025-12-12T11:46:38.028274Z","iopub.status.idle":"2025-12-12T11:46:41.664472Z","shell.execute_reply.started":"2025-12-12T11:46:38.028249Z","shell.execute_reply":"2025-12-12T11:46:41.663629Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# to remove version conflict of Protobuf so, downgrade version of Protobuf\n!pip install protobuf==3.20.3 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:46:41.665535Z","iopub.execute_input":"2025-12-12T11:46:41.665806Z","iopub.status.idle":"2025-12-12T11:46:45.714662Z","shell.execute_reply.started":"2025-12-12T11:46:41.665779Z","shell.execute_reply":"2025-12-12T11:46:45.713652Z"}},"outputs":[{"name":"stdout","text":"Collecting protobuf==3.20.3\n  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\nDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 4.25.8\n    Uninstalling protobuf-4.25.8:\n      Successfully uninstalled protobuf-4.25.8\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nunbabel-comet 2.2.7 requires protobuf<5.0.0,>=4.24.4, but you have protobuf 3.20.3 which is incompatible.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.20.3\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"!pip uninstall -y pyarrow datasets\n!pip cache purge\n\n# install fresh, modern, compatible versions\n!pip install --no-cache-dir \"datasets>=2.21.0\" \"pyarrow>=15.0.0\"\n!pip uninstall -y numpy\n!pip install numpy==1.26.4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:46:45.717020Z","iopub.execute_input":"2025-12-12T11:46:45.717321Z","iopub.status.idle":"2025-12-12T11:47:01.890543Z","shell.execute_reply.started":"2025-12-12T11:46:45.717290Z","shell.execute_reply":"2025-12-12T11:47:01.889587Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: pyarrow 22.0.0\nUninstalling pyarrow-22.0.0:\n  Successfully uninstalled pyarrow-22.0.0\nFound existing installation: datasets 4.4.1\nUninstalling datasets-4.4.1:\n  Successfully uninstalled datasets-4.4.1\nFiles removed: 16\nCollecting datasets>=2.21.0\n  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\nCollecting pyarrow>=15.0.0\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0) (3.20.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0) (1.26.4)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0) (0.28.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.21.0) (2025.10.0)\nRequirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.21.0) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.21.0) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.21.0) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.21.0) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.21.0) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.21.0) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=2.21.0) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=2.21.0) (1.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0) (2.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.21.0) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.21.0) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.21.0) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.21.0) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.21.0) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.21.0) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.21.0) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets>=2.21.0) (1.3.1)\nDownloading datasets-4.4.1-py3-none-any.whl (511 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m287.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow, datasets\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-4.4.1 pyarrow-22.0.0\nFound existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\nCollecting numpy==1.26.4\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nunbabel-comet 2.2.7 requires protobuf<5.0.0,>=4.24.4, but you have protobuf 3.20.3 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.26.4\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"# Importing all required modules\nimport os\nimport sys\nimport transformers\nimport torch  # pytorch Import\nimport sacrebleu\nfrom torch.amp import autocast, GradScaler\nfrom tqdm.auto import tqdm\nfrom transformers import DataCollatorForSeq2Seq\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset # for loading the dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM # For getting Embedding\nfrom transformers import DataCollatorForSeq2Seq #getting sequential model and collator for loading batchwise of data\nfrom torch.optim import AdamW # Optimizer\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:01.891590Z","iopub.execute_input":"2025-12-12T11:47:01.891875Z","iopub.status.idle":"2025-12-12T11:47:01.898832Z","shell.execute_reply.started":"2025-12-12T11:47:01.891843Z","shell.execute_reply":"2025-12-12T11:47:01.898277Z"}},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":"# Indictrans2-indic-en-dist-200M Model\n* source: https://huggingface.co/ai4bharat/indictrans2-indic-en-dist-200M","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login # \nlogin(new_session=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:01.899507Z","iopub.execute_input":"2025-12-12T11:47:01.899709Z","iopub.status.idle":"2025-12-12T11:47:01.924359Z","shell.execute_reply.started":"2025-12-12T11:47:01.899684Z","shell.execute_reply":"2025-12-12T11:47:01.923648Z"}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"# Note:\n* I was using the free version of Kaggle, and the memory limit was getting exhausted while training the 1B-parameter model. Because of this constraint, I switched to using the 200M-parameter model instead.","metadata":{}},{"cell_type":"code","source":"ckpt = \"ai4bharat/indictrans2-indic-en-dist-200M\" # Model Checkpoint \n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    ckpt,\n    trust_remote_code=True,                                         \n)\n\n# Move safely to GPU\nmodel = model.to(torch.float16).to(\"cuda\")   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:01.925124Z","iopub.execute_input":"2025-12-12T11:47:01.925308Z","iopub.status.idle":"2025-12-12T11:47:03.396806Z","shell.execute_reply.started":"2025-12-12T11:47:01.925293Z","shell.execute_reply":"2025-12-12T11:47:03.396182Z"}},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":"# The Dataset¶\n\n* Source: https://huggingface.co/datasets/atrisaxena/mini-iitb-english-hindi","metadata":{}},{"cell_type":"code","source":"raw_dataset = load_dataset(\"atrisaxena/mini-iitb-english-hindi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:03.397606Z","iopub.execute_input":"2025-12-12T11:47:03.397835Z","iopub.status.idle":"2025-12-12T11:47:04.462467Z","shell.execute_reply.started":"2025-12-12T11:47:03.397818Z","shell.execute_reply":"2025-12-12T11:47:04.461881Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"raw_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:04.463137Z","iopub.execute_input":"2025-12-12T11:47:04.463377Z","iopub.status.idle":"2025-12-12T11:47:04.468629Z","shell.execute_reply.started":"2025-12-12T11:47:04.463354Z","shell.execute_reply":"2025-12-12T11:47:04.467930Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['translation'],\n        num_rows: 20000\n    })\n    validation: Dataset({\n        features: ['translation'],\n        num_rows: 520\n    })\n    test: Dataset({\n        features: ['translation'],\n        num_rows: 2507\n    })\n})"},"metadata":{}}],"execution_count":50},{"cell_type":"markdown","source":"# Observation for Statistics related to dataset","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport numpy as np\nimport math\nimport nltk\nnltk.download(\"punkt\")  # one-time\n\ndef add_stats(example):\n    text = example[\"translation\"][\"en\"]\n    # guard\n    if text is None: text = \"\"\n    text = text.strip() # Removes unwanted spacing\n    words = text.split()\n    # sentence count (approx)\n    sents = nltk.tokenize.sent_tokenize(text) if text else []\n    example[\"num_words\"] = len(words)\n    example[\"num_chars\"] = len(text)\n    example[\"num_sentences\"] = len(sents)\n    return example\n\nraw_dataset = raw_dataset.map(add_stats, batched=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:04.471650Z","iopub.execute_input":"2025-12-12T11:47:04.471865Z","iopub.status.idle":"2025-12-12T11:47:04.495559Z","shell.execute_reply.started":"2025-12-12T11:47:04.471841Z","shell.execute_reply":"2025-12-12T11:47:04.495024Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"# Obtaining the Statistics:\n\ndef summary_stats(arr):\n    arr = np.array(arr)\n    return {\n        \"count\": int(arr.size),\n        \"min\": int(arr.min()) if arr.size>0 else None,\n        \"p1\": int(np.percentile(arr, 1)) if arr.size>0 else None,\n        \"p10\": int(np.percentile(arr, 10)) if arr.size>0 else None,\n        \"median\": float(np.median(arr)) if arr.size>0 else None,\n        \"mean\": float(arr.mean()) if arr.size>0 else None,\n        \"std\": float(arr.std(ddof=0)) if arr.size>0 else None,\n        \"p90\": int(np.percentile(arr, 90)) if arr.size>0 else None,\n        \"p99\": int(np.percentile(arr, 99)) if arr.size>0 else None,\n        \"max\": int(arr.max()) if arr.size>0 else None,\n    }\n\nfor split in raw_dataset:\n    d = raw_dataset[split]\n    print(f\"\\n=== {split.upper()} ===\")\n    print(\"Words:\", summary_stats(d[\"num_words\"]))\n    print(\"Chars:\", summary_stats(d[\"num_chars\"]))\n    print(\"Sentences:\", summary_stats(d[\"num_sentences\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:04.496335Z","iopub.execute_input":"2025-12-12T11:47:04.496877Z","iopub.status.idle":"2025-12-12T11:47:05.335793Z","shell.execute_reply.started":"2025-12-12T11:47:04.496860Z","shell.execute_reply":"2025-12-12T11:47:05.335218Z"}},"outputs":[{"name":"stdout","text":"\n=== TRAIN ===\nWords: {'count': 20000, 'min': 0, 'p1': 1, 'p10': 1, 'median': 9.0, 'mean': 13.01335, 'std': 14.90193852414846, 'p90': 30, 'p99': 68, 'max': 335}\nChars: {'count': 20000, 'min': 0, 'p1': 4, 'p10': 8, 'median': 49.0, 'mean': 74.78915, 'std': 85.15344263315195, 'p90': 173, 'p99': 387, 'max': 1950}\nSentences: {'count': 20000, 'min': 0, 'p1': 1, 'p10': 1, 'median': 1.0, 'mean': 1.14165, 'std': 0.5593614908983278, 'p90': 1, 'p99': 4, 'max': 18}\n\n=== VALIDATION ===\nWords: {'count': 520, 'min': 3, 'p1': 5, 'p10': 8, 'median': 16.0, 'mean': 17.71153846153846, 'std': 8.598382089452237, 'p90': 29, 'p99': 42, 'max': 62}\nChars: {'count': 520, 'min': 24, 'p1': 28, 'p10': 47, 'median': 97.5, 'mean': 105.025, 'std': 52.31679894568356, 'p90': 170, 'p99': 266, 'max': 358}\nSentences: {'count': 520, 'min': 1, 'p1': 1, 'p10': 1, 'median': 1.0, 'mean': 1.0576923076923077, 'std': 0.2851087200499499, 'p90': 1, 'p99': 2, 'max': 4}\n\n=== TEST ===\nWords: {'count': 2507, 'min': 3, 'p1': 5, 'p10': 9, 'median': 18.0, 'mean': 19.704427602712407, 'std': 9.606497536338624, 'p90': 33, 'p99': 47, 'max': 81}\nChars: {'count': 2507, 'min': 17, 'p1': 29, 'p10': 51, 'median': 107.0, 'mean': 117.75149581172717, 'std': 59.090514837102944, 'p90': 198, 'p99': 289, 'max': 505}\nSentences: {'count': 2507, 'min': 1, 'p1': 1, 'p10': 1, 'median': 1.0, 'mean': 1.015556441962505, 'std': 0.13307042436031571, 'p90': 1, 'p99': 2, 'max': 3}\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"from datasets import DatasetDict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:05.336632Z","iopub.execute_input":"2025-12-12T11:47:05.336875Z","iopub.status.idle":"2025-12-12T11:47:05.340487Z","shell.execute_reply.started":"2025-12-12T11:47:05.336858Z","shell.execute_reply":"2025-12-12T11:47:05.339765Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# Train has min length of sentences as 0 ('min': 0) so, we Remove these row from dataset\ndef not_empty(example):\n    text = example[\"translation\"][\"en\"]\n    return text is not None and len(text.strip()) > 0\n    \nclean_train = raw_dataset[\"train\"].filter(not_empty)\nclean_val   = raw_dataset[\"validation\"].filter(not_empty)\nclean_test  = raw_dataset[\"test\"].filter(not_empty)\n\nraw_dataset = DatasetDict({\n    \"train\": clean_train,\n    \"validation\": clean_val,\n    \"test\": clean_test\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:05.341320Z","iopub.execute_input":"2025-12-12T11:47:05.341568Z","iopub.status.idle":"2025-12-12T11:47:05.365906Z","shell.execute_reply.started":"2025-12-12T11:47:05.341543Z","shell.execute_reply":"2025-12-12T11:47:05.365229Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# Train has min length of sentences as 0 ('min': 0) so, we Remove these row from dataset\ndef not_empty(example):\n    text = example[\"translation\"][\"hi\"]\n    return text is not None and len(text.strip()) > 0\n    \nclean_train = raw_dataset[\"train\"].filter(not_empty)\nclean_val   = raw_dataset[\"validation\"].filter(not_empty)\nclean_test  = raw_dataset[\"test\"].filter(not_empty)\n\nraw_dataset = DatasetDict({\n    \"train\": clean_train,\n    \"validation\": clean_val,\n    \"test\": clean_test\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:05.366736Z","iopub.execute_input":"2025-12-12T11:47:05.366964Z","iopub.status.idle":"2025-12-12T11:47:05.379638Z","shell.execute_reply.started":"2025-12-12T11:47:05.366945Z","shell.execute_reply":"2025-12-12T11:47:05.379081Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"type(raw_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:05.380409Z","iopub.execute_input":"2025-12-12T11:47:05.380648Z","iopub.status.idle":"2025-12-12T11:47:05.385470Z","shell.execute_reply.started":"2025-12-12T11:47:05.380632Z","shell.execute_reply":"2025-12-12T11:47:05.384860Z"}},"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"datasets.dataset_dict.DatasetDict"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"# compute p99 threshold so, removing other outliers(longer than p99 words)\nword_lengths = np.array(raw_dataset[\"train\"][\"num_words\"])\np99_threshold = int(np.percentile(word_lengths, 99))\nprint(\"Removing sentences longer than:\", p99_threshold, \"words\")\nraw_dataset[\"train\"] = raw_dataset[\"train\"].filter(\n    lambda ex: ex[\"num_words\"] <= p99_threshold\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:05.386161Z","iopub.execute_input":"2025-12-12T11:47:05.386409Z","iopub.status.idle":"2025-12-12T11:47:06.148714Z","shell.execute_reply.started":"2025-12-12T11:47:05.386387Z","shell.execute_reply":"2025-12-12T11:47:06.147951Z"}},"outputs":[{"name":"stdout","text":"Removing sentences longer than: 68 words\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"# Obtaining the desired Statistics\ndef summary_stats(arr):\n    arr = np.array(arr)\n    return {\n        \"count\": int(arr.size),\n        \"min\": int(arr.min()) if arr.size>0 else None,\n        \"p1\": int(np.percentile(arr, 1)) if arr.size>0 else None,\n        \"p10\": int(np.percentile(arr, 10)) if arr.size>0 else None,\n        \"median\": float(np.median(arr)) if arr.size>0 else None,\n        \"mean\": float(arr.mean()) if arr.size>0 else None,\n        \"std\": float(arr.std(ddof=0)) if arr.size>0 else None,\n        \"p90\": int(np.percentile(arr, 90)) if arr.size>0 else None,\n        \"p99\": int(np.percentile(arr, 99)) if arr.size>0 else None,\n        \"max\": int(arr.max()) if arr.size>0 else None,\n    }\n\nfor split in raw_dataset:\n    d = raw_dataset[split]\n    print(f\"\\n=== {split.upper()} ===\")\n    print(\"Words:\", summary_stats(d[\"num_words\"]))\n    print(\"Chars:\", summary_stats(d[\"num_chars\"]))\n    print(\"Sentences:\", summary_stats(d[\"num_sentences\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:06.149507Z","iopub.execute_input":"2025-12-12T11:47:06.149732Z","iopub.status.idle":"2025-12-12T11:47:08.725302Z","shell.execute_reply.started":"2025-12-12T11:47:06.149705Z","shell.execute_reply":"2025-12-12T11:47:08.724705Z"}},"outputs":[{"name":"stdout","text":"\n=== TRAIN ===\nWords: {'count': 19735, 'min': 1, 'p1': 1, 'p10': 1, 'median': 9.0, 'mean': 12.21950848745883, 'std': 12.135597141252765, 'p90': 29, 'p99': 54, 'max': 68}\nChars: {'count': 19735, 'min': 1, 'p1': 4, 'p10': 8, 'median': 48.0, 'mean': 70.34436280719534, 'std': 69.7382299024331, 'p90': 168, 'p99': 309, 'max': 493}\nSentences: {'count': 19735, 'min': 1, 'p1': 1, 'p10': 1, 'median': 1.0, 'mean': 1.120749936660755, 'std': 0.4527456368108368, 'p90': 1, 'p99': 3, 'max': 8}\n\n=== VALIDATION ===\nWords: {'count': 520, 'min': 3, 'p1': 5, 'p10': 8, 'median': 16.0, 'mean': 17.71153846153846, 'std': 8.598382089452237, 'p90': 29, 'p99': 42, 'max': 62}\nChars: {'count': 520, 'min': 24, 'p1': 28, 'p10': 47, 'median': 97.5, 'mean': 105.025, 'std': 52.31679894568356, 'p90': 170, 'p99': 266, 'max': 358}\nSentences: {'count': 520, 'min': 1, 'p1': 1, 'p10': 1, 'median': 1.0, 'mean': 1.0576923076923077, 'std': 0.2851087200499499, 'p90': 1, 'p99': 2, 'max': 4}\n\n=== TEST ===\nWords: {'count': 2507, 'min': 3, 'p1': 5, 'p10': 9, 'median': 18.0, 'mean': 19.704427602712407, 'std': 9.606497536338624, 'p90': 33, 'p99': 47, 'max': 81}\nChars: {'count': 2507, 'min': 17, 'p1': 29, 'p10': 51, 'median': 107.0, 'mean': 117.75149581172717, 'std': 59.090514837102944, 'p90': 198, 'p99': 289, 'max': 505}\nSentences: {'count': 2507, 'min': 1, 'p1': 1, 'p10': 1, 'median': 1.0, 'mean': 1.015556441962505, 'std': 0.13307042436031571, 'p90': 1, 'p99': 2, 'max': 3}\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"# Sample Example\nraw_dataset['train'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:08.725980Z","iopub.execute_input":"2025-12-12T11:47:08.726258Z","iopub.status.idle":"2025-12-12T11:47:08.731892Z","shell.execute_reply.started":"2025-12-12T11:47:08.726239Z","shell.execute_reply":"2025-12-12T11:47:08.731121Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"{'translation': {'en': 'The occupation of keeping bees.',\n  'hi': 'मधुमक्खियों को पालने का कार्य। '},\n 'num_words': 5,\n 'num_chars': 31,\n 'num_sentences': 1}"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"from datasets import DatasetDict\n\n# New desired sizes\nN_TRAIN = 1000\nN_VAL   = 150\nN_TEST  = 250\n\n# Downsample using .select()\nsmall_train = raw_dataset[\"train\"].select(range(N_TRAIN))\nsmall_val   = raw_dataset[\"validation\"].select(range(N_VAL))\nsmall_test  = raw_dataset[\"test\"].select(range(N_TEST))\n\n# Create a new DatasetDict\nsmall_dataset = DatasetDict({\n    \"train\": small_train,\n    \"validation\": small_val,\n    \"test\": small_test\n})\n\nsmall_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:08.732729Z","iopub.execute_input":"2025-12-12T11:47:08.733031Z","iopub.status.idle":"2025-12-12T11:47:08.753770Z","shell.execute_reply.started":"2025-12-12T11:47:08.733013Z","shell.execute_reply":"2025-12-12T11:47:08.753186Z"}},"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['translation', 'num_words', 'num_chars', 'num_sentences'],\n        num_rows: 1000\n    })\n    validation: Dataset({\n        features: ['translation', 'num_words', 'num_chars', 'num_sentences'],\n        num_rows: 150\n    })\n    test: Dataset({\n        features: ['translation', 'num_words', 'num_chars', 'num_sentences'],\n        num_rows: 250\n    })\n})"},"metadata":{}}],"execution_count":60},{"cell_type":"code","source":"raw_dataset = small_dataset  # 1000 pairs of Sentences","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:08.754412Z","iopub.execute_input":"2025-12-12T11:47:08.754970Z","iopub.status.idle":"2025-12-12T11:47:08.767131Z","shell.execute_reply.started":"2025-12-12T11:47:08.754953Z","shell.execute_reply":"2025-12-12T11:47:08.766435Z"}},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":"# Applying Tokenization:- How we obtain Embedding\n* Attention Mask = Padding Mask x look Ahead Mask\n* Input_ids = input_ids are tokenized text converted into numeric indices from tokenizer vocabulary.\n* The model converts input_ids to embeddings internally through an embedding layer.\n\n# Pipeline\n* Text → Tokens → IDs → Embeddings → Transformer\n* \"I love India\"\n*      ↓              (tokenization)\n* [\"I\",\"love\",\"India\"]\n*      ↓              (vocab lookup)\n* [34, 91, 2563]  ← input_ids\n*      ↓\n* [embedding vectors] ← actual embeddings used by model","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(ckpt) # Enter the Token and Rerun","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:08.767947Z","iopub.execute_input":"2025-12-12T11:47:08.768189Z","iopub.status.idle":"2025-12-12T11:47:25.213844Z","shell.execute_reply.started":"2025-12-12T11:47:08.768167Z","shell.execute_reply":"2025-12-12T11:47:25.213216Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"The repository ai4bharat/indictrans2-indic-en-dist-200M contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/ai4bharat/indictrans2-indic-en-dist-200M .\n You can inspect the repository content at https://hf.co/ai4bharat/indictrans2-indic-en-dist-200M.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"}],"execution_count":62},{"cell_type":"code","source":"# sample Example;\ntext = \"नमस्कार, मेरा नाम वीरेंद्र है। मैं एनआईटी सूरत में अंतिम वर्ष का छात्र हूँ।\"\ntokenizer(\"hin_Deva eng_Latn \" + text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:25.214648Z","iopub.execute_input":"2025-12-12T11:47:25.215273Z","iopub.status.idle":"2025-12-12T11:47:25.220708Z","shell.execute_reply.started":"2025-12-12T11:47:25.215247Z","shell.execute_reply":"2025-12-12T11:47:25.220169Z"}},"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [8, 4, 24655, 94148, 2262, 352, 31664, 11, 77634, 152, 1255, 20430, 27701, 19, 2055, 576, 41, 2586, 1940, 77634, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"},"metadata":{}}],"execution_count":63},{"cell_type":"code","source":"# Tokenize the Target language(English - eng_Latn) - Example\nwith tokenizer.as_target_tokenizer():\n    print(tokenizer(\"My name is Virendra, I am Currently a AI/ML Researcher\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:25.221377Z","iopub.execute_input":"2025-12-12T11:47:25.221586Z","iopub.status.idle":"2025-12-12T11:47:25.242458Z","shell.execute_reply.started":"2025-12-12T11:47:25.221561Z","shell.execute_reply":"2025-12-12T11:47:25.241772Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [345, 264, 12, 11233, 5909, 3, 18, 161, 1134, 13, 3960, 2423, 5734, 26156, 9230, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3951: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":64},{"cell_type":"markdown","source":" # Preprocess Fxn for Tokenization:","metadata":{}},{"cell_type":"code","source":"# tags for IndicTrans2 English->Hindi\nSRC_TAG = \"hin_Deva\"\nTGT_TAG = \"eng_Latn\"\n\nmax_input_length = 128\nmax_target_length = 128\nsource_lang = \"hi\"\ntarget_lang = \"en\"\n\ndef preprocess_function(examples):\n\n    inputs = [f\"{SRC_TAG} {TGT_TAG} {ex[source_lang].strip() if ex[source_lang] else ''}\"\n              for ex in examples[\"translation\"]]\n    targets = [ex[target_lang].strip() if ex[target_lang] else \"\" \n               for ex in examples[\"translation\"]]\n\n    # tokenize source (each string already prefixed with tags)\n    model_inputs = tokenizer(\n        inputs,\n        max_length=max_input_length,\n        truncation=True,\n        padding=True,   \n    )\n\n    # Tokenizer for Target lang(Hindi)\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            [f\"{TGT_TAG} {t}\" if t else f\"{TGT_TAG} \" for t in targets],\n            max_length=max_target_length,\n            truncation=True,\n            padding=True,\n        )\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:25.243309Z","iopub.execute_input":"2025-12-12T11:47:25.243529Z","iopub.status.idle":"2025-12-12T11:47:25.258812Z","shell.execute_reply.started":"2025-12-12T11:47:25.243513Z","shell.execute_reply":"2025-12-12T11:47:25.258288Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"# Obtaining token of two rows of Train dataset\nprint(preprocess_function(raw_dataset[\"train\"][:2])) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:25.259392Z","iopub.execute_input":"2025-12-12T11:47:25.259574Z","iopub.status.idle":"2025-12-12T11:47:25.276337Z","shell.execute_reply.started":"2025-12-12T11:47:25.259560Z","shell.execute_reply":"2025-12-12T11:47:25.275639Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 4, 54023, 62686, 475, 33, 55808, 41, 370, 77634, 2], [8, 4, 40068, 12, 1053, 19, 1111, 36, 710, 10654, 12, 10318, 56733, 1701, 5274, 2969, 119, 12, 189, 10318, 12665, 29, 1269, 115, 41, 1561, 1983, 92, 77634, 2]], 'attention_mask': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[7580, 25232, 817, 516, 443, 15, 7348, 7, 2868, 21006, 71, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [7580, 25232, 817, 516, 443, 21913, 5, 326, 7, 18152, 3, 332, 16624, 767, 1320, 8, 166, 1382, 31, 5, 14884, 224, 5, 2204, 1731, 8058, 89, 868, 10, 8723, 71, 2]]}\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"# Setting  hyperparameter Values\nbatch_size = 16\nlearning_rate = 2e-5\nweight_decay = 0.01\nnum_train_epochs = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:25.277147Z","iopub.execute_input":"2025-12-12T11:47:25.277484Z","iopub.status.idle":"2025-12-12T11:47:25.293787Z","shell.execute_reply.started":"2025-12-12T11:47:25.277460Z","shell.execute_reply":"2025-12-12T11:47:25.293224Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:25.294481Z","iopub.execute_input":"2025-12-12T11:47:25.294703Z","iopub.status.idle":"2025-12-12T11:47:25.310126Z","shell.execute_reply.started":"2025-12-12T11:47:25.294681Z","shell.execute_reply":"2025-12-12T11:47:25.309289Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"# 1) Tokenization of dataset and DataCollator\ntokenized_datasets = raw_dataset.map(preprocess_function, batched=True,\n                                    remove_columns=raw_dataset[\"train\"].column_names)\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")\n\n# 2)train_dataloader and test_dataloader\n\ntrain_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=batch_size,\n                              shuffle=True, collate_fn=data_collator, num_workers=4)\nvalidation_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=batch_size,\n                                   shuffle=False, collate_fn=data_collator, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:25.313284Z","iopub.execute_input":"2025-12-12T11:47:25.313528Z","iopub.status.idle":"2025-12-12T11:47:29.780168Z","shell.execute_reply.started":"2025-12-12T11:47:25.313513Z","shell.execute_reply":"2025-12-12T11:47:29.779597Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aafd5f18c09946128987e45f4ecb359b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85e3001812754d7892259880a5283655"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b6a216384394b4088d41a5467a8cf87"}},"metadata":{}}],"execution_count":69},{"cell_type":"code","source":"model.float()        # make sure parameters are float32\nmodel.to(device)\n\n# 3) Recreate optimizer (must be created after model param dtypes are finalized)\noptimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\n# 4) Mixed precision utilities\nscaler = GradScaler()\ngrad_accum = 2\nnum_epochs = max(1, int(num_train_epochs))\n\nallowed_keys = {\"input_ids\", \"attention_mask\", \"labels\", \"decoder_input_ids\", \"decoder_attention_mask\"}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:29.780967Z","iopub.execute_input":"2025-12-12T11:47:29.781241Z","iopub.status.idle":"2025-12-12T11:47:29.834325Z","shell.execute_reply.started":"2025-12-12T11:47:29.781216Z","shell.execute_reply":"2025-12-12T11:47:29.833807Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"# 5) Training loop\nmodel.train()\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n    running_loss = 0.0\n    steps = 0\n    for batch in tqdm(train_dataloader, desc=\"Training\"):\n        # keep only model inputs and move to device\n        model_batch = {k: v.to(device) for k, v in batch.items() if k in allowed_keys and isinstance(v, torch.Tensor)}\n\n        with autocast(\"cuda\"):                  # activations in fp16, params stay fp32\n            outputs = model(**model_batch)\n            loss = outputs.loss / grad_accum\n\n        scaler.scale(loss).backward()\n\n        if (steps + 1) % grad_accum == 0:\n            # Unscale the Optimizer\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n\n        running_loss += loss.item() * grad_accum\n        steps += 1\n\n    avg_train = running_loss / max(1, steps)\n    print(f\"Train loss: {avg_train:.4f}\")\n\n    # 6) Quick validation\n    model.eval()\n    vloss = 0.0\n    vsteps = 0\n    with torch.no_grad():\n        for vb in tqdm(validation_dataloader, desc=\"Validation\"):\n            vb_batch = {k: v.to(device) for k, v in vb.items() if k in allowed_keys and isinstance(v, torch.Tensor)}\n            with autocast(\"cuda\"):\n                out = model(**vb_batch)\n            vloss += out.loss.item()\n            vsteps += 1\n    vloss = vloss / max(1, vsteps)\n    print(f\"Validation loss: {vloss:.4f}\")\n    model.train()\n\nprint(\"Training finished.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:29.835025Z","iopub.execute_input":"2025-12-12T11:47:29.835256Z","iopub.status.idle":"2025-12-12T11:47:48.856359Z","shell.execute_reply.started":"2025-12-12T11:47:29.835233Z","shell.execute_reply":"2025-12-12T11:47:48.855312Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c87ed64be25f4558b82743ce805d8073"}},"metadata":{}},{"name":"stdout","text":"Train loss: 8.4179\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2e4251e1a024f6e9b81489b9405f2ad"}},"metadata":{}},{"name":"stdout","text":"Validation loss: 6.1535\nTraining finished.\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"# simple inference (model already on GPU)\nSRC_TAG = \"hin_Deva\"\nTGT_TAG = \"eng_Latn\"\ntext = \"नमस्ते, यह नोटबुक अनुवाद से संबंधित है।\"\ntagged = f\"{SRC_TAG} {TGT_TAG} {text}\"\n\ninputs = tokenizer(tagged, return_tensors=\"pt\").to(model.device)\nwith torch.no_grad():\n    out = model.generate(**inputs, max_length=128, num_beams=4)\nprint(tokenizer.decode(out[0], skip_special_tokens=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:48.857926Z","iopub.execute_input":"2025-12-12T11:47:48.858218Z","iopub.status.idle":"2025-12-12T11:47:49.168156Z","shell.execute_reply.started":"2025-12-12T11:47:48.858186Z","shell.execute_reply":"2025-12-12T11:47:49.167548Z"}},"outputs":[{"name":"stdout","text":"Hello.This is related to Notebook Translation.\n","output_type":"stream"}],"execution_count":72},{"cell_type":"markdown","source":"# Evaluation metrics","metadata":{}},{"cell_type":"markdown","source":"# Finding  BLEU AND CHRF:\n\n1. BLEU: BLEU checks how many n-grams from the candidate sentence also appear in the reference sentence.\n2. CHRF: Instead of words, CHRF compares character n-grams.","metadata":{}},{"cell_type":"code","source":"model.device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:49.169002Z","iopub.execute_input":"2025-12-12T11:47:49.169599Z","iopub.status.idle":"2025-12-12T11:47:49.175204Z","shell.execute_reply.started":"2025-12-12T11:47:49.169580Z","shell.execute_reply":"2025-12-12T11:47:49.174224Z"}},"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}],"execution_count":73},{"cell_type":"code","source":"n_samples = 10 # no of samples of test used for Evaluation\n\n# Checking for gpu Device\ntry:\n    model_device = next(model.parameters()).device\nexcept StopIteration:\n    model_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"Model device:\", model_device)\n\n# Finding Prediction and Reference List\n\npreds = []\nrefs = []\n\nfor i in range(min(n_samples, len(raw_dataset[\"test\"]))):\n    row = raw_dataset[\"test\"][i]\n\n    # handle multiple possible row formats\n    if isinstance(row, dict) and \"translation\" in row:\n        trans = row[\"translation\"]\n        # trans might be a dict or a string; handle both\n        if isinstance(trans, dict):\n            eng = trans.get(\"en\") or trans.get(\"eng\") or \"\"\n            hi_ref = trans.get(\"hi\") or trans.get(\"hin\") or \"\"\n        else:\n            # sometimes translation is a string (rare) — treat as source\n            eng = str(trans)\n            hi_ref = \"\"\n    elif isinstance(row, dict):\n        # maybe keys are directly 'en' and 'hi'\n        eng = row.get(\"en\") or row.get(\"eng\") or row.get(\"source\") or \"\"\n        hi_ref = row.get(\"hi\") or row.get(\"hin\") or row.get(\"target\") or \"\"\n    else:\n        # fallback: row itself might be the translation dict-like\n        try:\n            eng = row[\"en\"]\n            hi_ref = row[\"hi\"]\n        except Exception:\n            # last resort: stringify\n            eng = str(row)\n            hi_ref = \"\"\n\n    eng = (eng or \"\").strip()\n    hi_ref = (hi_ref or \"\").strip()\n    refs.append(eng if eng else \"\")  # keep alignment\n\n    # add required tags\n    tagged = f\"{SRC_TAG} {TGT_TAG} {eng}\"\n\n    # tokenize -> torch tensors -> move to model device\n    tokenized = tokenizer(tagged, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n    tokenized = {k: v.to(model_device) for k, v in tokenized.items()}\n\n    # generate\n    with torch.no_grad():\n        out_ids = model.generate(**tokenized, max_length=128, num_beams=4, early_stopping=True)\n\n    pred_text = tokenizer.decode(out_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True).strip()\n    preds.append(pred_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:47:49.176088Z","iopub.execute_input":"2025-12-12T11:47:49.176763Z","iopub.status.idle":"2025-12-12T11:48:05.449771Z","shell.execute_reply.started":"2025-12-12T11:47:49.176745Z","shell.execute_reply":"2025-12-12T11:48:05.449188Z"}},"outputs":[{"name":"stdout","text":"Model device: cuda:0\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"\n#BLEU:\nbleu = sacrebleu.corpus_bleu(preds, [refs])\n#CHRF:\nchrf = sacrebleu.corpus_chrf(preds, [refs])\n\nprint(f\"\\nEvaluated {len(preds)} samples\")\nprint(\"BLEU:\", bleu.score)\nprint(\"CHRF:\", chrf.score)\n\n# show a few examples\nfor i in range(min(5, len(preds))):\n    print(f\"\\n=== SAMPLE {i+1} ===\")\n    print(\"SRC :\", (raw_dataset[\"test\"][i].get(\"translation\", raw_dataset[\"test\"][i]).get(\"hi\")\n                    if isinstance(raw_dataset[\"test\"][i], dict) and \"translation\" in raw_dataset[\"test\"][i]\n                    else (raw_dataset[\"test\"][i].get(\"hi\") if isinstance(raw_dataset[\"test\"][i], dict) else str(raw_dataset[\"test\"][i]))))\n    print(\"PRED:\", preds[i])\n    print(\"REF :\", refs[i])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:48:05.451624Z","iopub.execute_input":"2025-12-12T11:48:05.452115Z","iopub.status.idle":"2025-12-12T11:48:05.468936Z","shell.execute_reply.started":"2025-12-12T11:48:05.452096Z","shell.execute_reply":"2025-12-12T11:48:05.468257Z"}},"outputs":[{"name":"stdout","text":"\nEvaluated 10 samples\nBLEU: 83.49970389727707\nCHRF: 94.56870419019997\n\n=== SAMPLE 1 ===\nSRC : आपकी कार में ब्लैक बॉक्स?\nPRED: A black box in your car?\nREF : A black box in your car?\n\n=== SAMPLE 2 ===\nSRC : जबकि अमेरिका के सड़क योजनाकार, ध्वस्त होते हुए हाईवे सिस्टम को सुधारने के लिए धन की कमी से जूझ रहे हैं, वहीं बहुत-से लोग इसका समाधान छोटे से ब्लैक बॉक्स में देख रहे हैं, जो आपकी कार के डैशबोर्ड पर सफ़ाई से फिट हो जाता है।\nPRED: As America's road planners struggle to find the cash to lend a crumbling highway system. many are beginning to see a solution in a little black box that fits neatly by the dashboard of your car.\nREF : As America's road planners struggle to find the cash to mend a crumbling highway system, many are beginning to see a solution in a little black box that fits neatly by the dashboard of your car.\n\n=== SAMPLE 3 ===\nSRC : यह डिवाइस, जो मोटर-चालक द्वारा वाहन चलाए गए प्रत्येक मील को ट्रैक करती है तथा उस सूचना को अधिकारियों को संचारित करती है, आजकल अमेरिका की प्रमुख सड़कों का वित्त-पोषण करने के लिए पुराने हो चुके सिस्टम का जीर्णोद्धार करने के लिए वाशिंगटन और राज्य नियोजन कार्यालय के लिए एक विवादास्पद प्रयास का मुद्दा बन चुका है।\nPRED: The devices which track every mile a motorist drives and transmit that information to the bureaucrats are at the center of a controversial attempt in Washington and state planning offices to overhaul the outdated system for funding America's major roads.\nREF : The devices, which track every mile a motorist drives and transmit that information to bureaucrats, are at the center of a controversial attempt in Washington and state planning offices to overhaul the outdated system for funding America's major roads.\n\n=== SAMPLE 4 ===\nSRC : आम तौर पर हाईवे नियोजन जैसा उबाऊ काम भी अचानक गहन बहस तथा जीवंत गठबंधनों का मुद्दा बन गया है।\nPRED: The usually dull arena of highway planning has suddenly spawned intense debate and colorful alliances.\nREF : The usually dull arena of highway planning has suddenly spawned intense debate and colorful alliances.\n\n=== SAMPLE 5 ===\nSRC : आपने द्वारा ड्राइव किए गए मील, तथा संभवतः ड्राइव किए गए स्थान का विवरण रखने - और फिर इस सूचना का उपयोग टैक्स बिल तैयार करने के लिए - सरकार को इन ब्लैक बॉक्स का उपयोग करने की अनुमति देने के पक्ष में समर्थन जुटाने के लिए लिबरेटेरियन पर्यावरणीय समूहों के साथ मिल गए हैं।\nPRED: Libertarians have joined environmental groups in lobbying to allow the government to use the little boxes to keep track of the miles you drive and possibly where you drive them - then use the information to draw up a tax bill.\nREF : Libertarians have joined environmental groups in lobbying to allow government to use the little boxes to keep track of the miles you drive, and possibly where you drive them - then use the information to draw up a tax bill.\n","output_type":"stream"}],"execution_count":75},{"cell_type":"markdown","source":"# Finding BERTScore:\n\n* BERTScore: Uses BERT (or RoBERTa, or mBERT) embeddings to compare every token in candidate with every token in reference.","metadata":{}},{"cell_type":"code","source":"!pip install bert-score\nfrom bert_score import score\n\nP, R, F1 = score(preds, refs, lang=\"en\")\nprint(\"BERTScore F1:\", F1.mean().item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:55:13.284236Z","iopub.execute_input":"2025-12-12T11:55:13.284563Z","iopub.status.idle":"2025-12-12T11:55:24.125217Z","shell.execute_reply.started":"2025-12-12T11:55:13.284534Z","shell.execute_reply":"2025-12-12T11:55:24.124352Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bert-score in /usr/local/lib/python3.11/dist-packages (0.3.13)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.6.0+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.53.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.5)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (25.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2025.11.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.0.9)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.10.5)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->bert-score) (1.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"323a1c9a7f904a38bad6b81e301c3621"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d616a9ae5bf486fbe4631080f98e9d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbe8c4fa3aec4f448e4c102dc2ca6812"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78a1d3c5d7b444b183f18ef8d1fb64a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53c0e5d3a8e348bbb966935571eea86a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1ac9bbfd33b4bc4ab47fe8507470124"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"BERTScore F1: 0.7101126909255981\n","output_type":"stream"}],"execution_count":81},{"cell_type":"markdown","source":"# Finding BLEURT:\n* BLEURT computes a similarity score using a fine-tuned BERT model that predicts human judgment of translation quality.","metadata":{}},{"cell_type":"code","source":"# Required Packages for Bleurt\n!pip install evaluate\n!pip install git+https://github.com/google-research/bleurt.git\n\n# Calculating Bleurt\nimport evaluate\nbleurt = evaluate.load(\"bleurt\")\nresults = bleurt.compute(predictions=preds, references=refs)\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:55:32.093010Z","iopub.execute_input":"2025-12-12T11:55:32.093559Z","iopub.status.idle":"2025-12-12T11:55:49.685700Z","shell.execute_reply.started":"2025-12-12T11:55:32.093528Z","shell.execute_reply":"2025-12-12T11:55:49.684824Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.6)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.4.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.5)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.18)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\nCollecting git+https://github.com/google-research/bleurt.git\n  Cloning https://github.com/google-research/bleurt.git to /tmp/pip-req-build-nxp1au2n\n  Running command git clone --filter=blob:none --quiet https://github.com/google-research/bleurt.git /tmp/pip-req-build-nxp1au2n\n  Resolved https://github.com/google-research/bleurt.git to commit cebe7e6f996b40910cfaa520a63db47807e3bf5c\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (2.2.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (1.15.3)\nRequirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (2.18.0)\nRequirement already satisfied: tf-slim>=1.1 in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (1.1.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (0.2.0)\nRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from tf-slim>=1.1->BLEURT==0.0.2) (1.4.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->BLEURT==0.0.2) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->BLEURT==0.0.2) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->BLEURT==0.0.2) (2025.2)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (25.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (4.25.8)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (2.32.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (3.1.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (4.15.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (1.74.0)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (2.18.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (3.8.0)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (3.14.0)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (0.4.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->BLEURT==0.0.2) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->BLEURT==0.0.2) (14.2.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->BLEURT==0.0.2) (0.1.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->BLEURT==0.0.2) (0.16.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (2025.10.5)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->BLEURT==0.0.2) (3.8.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->BLEURT==0.0.2) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->BLEURT==0.0.2) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow->BLEURT==0.0.2) (3.0.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->BLEURT==0.0.2) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->BLEURT==0.0.2) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow->BLEURT==0.0.2) (0.1.2)\n","output_type":"stream"},{"name":"stderr","text":"WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.88cdcafd9cccc9d5927ab758a370025ca107402fa4e0cccccc70fa2add645f41.bleurt:Using default checkpoint 'bleurt-base-128' for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', config_name='bleurt-large-512').\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Reading checkpoint /root/.cache/huggingface/metrics/bleurt/default/downloads/extracted/887f2dc36c17f53c287f696681b8f7c947278407c1cf9f226662e16c8c0dc417/bleurt-base-128.\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Reading checkpoint /root/.cache/huggingface/metrics/bleurt/default/downloads/extracted/887f2dc36c17f53c287f696681b8f7c947278407c1cf9f226662e16c8c0dc417/bleurt-base-128.\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Config file found, reading.\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Config file found, reading.\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Will load checkpoint bert_custom\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Will load checkpoint bert_custom\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Loads full paths and checks that files exists.\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Loads full paths and checks that files exists.\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:... name:bert_custom\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:... name:bert_custom\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:... vocab_file:vocab.txt\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:... vocab_file:vocab.txt\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:... bert_config_file:bert_config.json\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:... bert_config_file:bert_config.json\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:... do_lower_case:True\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:... do_lower_case:True\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:... max_seq_length:128\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:... max_seq_length:128\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Creating BLEURT scorer.\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Creating BLEURT scorer.\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Creating WordPiece tokenizer.\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Creating WordPiece tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:WordPiece tokenizer instantiated.\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:WordPiece tokenizer instantiated.\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Creating Eager Mode predictor.\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Creating Eager Mode predictor.\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Loading model.\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Loading model.\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:BLEURT initialized.\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:BLEURT initialized.\n","output_type":"stream"},{"name":"stdout","text":"{'scores': [-1.360461950302124, -1.1840121746063232, -1.1456708908081055, -1.210099220275879, -1.1298375129699707, -1.3440830707550049, -1.1011061668395996, -1.187833309173584, -1.1758217811584473, -1.2730412483215332]}\n","output_type":"stream"}],"execution_count":82},{"cell_type":"markdown","source":"# Finding COMET\n* COMET predicts a score that strongly correlates with human judgment.","metadata":{}},{"cell_type":"code","source":"# Required package for Comet\n\nimport re  \n!pip install -q unbabel-comet\nfrom comet import download_model, load_from_checkpoint\n\n# choose model variable \ntranslation_model = globals().get(\"translation_model\", None) or globals().get(\"model\", None)\nif translation_model is None:\n    raise ValueError(\"No translation model found. Load your model into `model` or `translation_model` first.\")\n\n# device: try to get model device (handles DeviceMap too)\ntry:\n    model_device = next(translation_model.parameters()).device\nexcept StopIteration:\n    model_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nSRC_TAG = \"hin_Deva\" \nTGT_TAG = \"eng_Latn\"\n\nsrcs = []\npreds = []\nrefs = []\n\nn = 10\nfor i in range(min(n, len(raw_dataset[\"test\"]))):\n    row = raw_dataset[\"test\"][i]\n\n    # robust extraction of source & reference\n    if isinstance(row, dict) and \"translation\" in row:\n        trans = row[\"translation\"]\n        if isinstance(trans, dict):\n            src = (trans.get(\"hi\") or trans.get(\"hin\") or \"\").strip()\n            ref = (trans.get(\"en\") or trans.get(\"eng\") or \"\").strip()\n        else:\n            src = str(trans).strip()\n            ref = \"\"\n    elif isinstance(row, dict):\n        src = (row.get(\"hi\") or row.get(\"hin\") or row.get(\"source\") or \"\").strip()\n        ref = (row.get(\"en\") or row.get(\"eng\") or row.get(\"target\") or \"\").strip()\n    else:\n        # fallback\n        src = str(row).strip()\n        ref = \"\"\n\n    srcs.append(src)\n    refs.append(ref)\n\n    # add language tags required by IndicTrans2\n    tagged = f\"{SRC_TAG} {TGT_TAG} {src}\"\n\n    # tokenize -> PyTorch tensors -> move to model device\n    tokenized = tokenizer(tagged,\n                          return_tensors=\"pt\",\n                          truncation=True,\n                          padding=True,\n                          max_length=128)\n    tokenized = {k: v.to(model_device) for k, v in tokenized.items()}\n\n    # generate\n    with torch.no_grad():\n        out_ids = translation_model.generate(**tokenized, max_length=128, num_beams=4, early_stopping=True)\n\n    pred = tokenizer.decode(out_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True).strip()\n    preds.append(pred)\n    \n# Removing Eng_Latn\n_tag_re = re.compile(rf'^{re.escape(TGT_TAG)}[\\s\\-\\:\\,]*', flags=0)\n\n# Clean preds again (just in case)\nclean_preds = []\nfor p in preds:\n    p = _tag_re.sub(\"\", p).strip()\n    clean_preds.append(p)\n    \n# ---- COMET evaluation ----\nmodel_path = download_model(\"Unbabel/wmt22-comet-da\")\ncomet_model = load_from_checkpoint(model_path)\n\n# prepare data list for COMET\ndata = [{\"src\": s, \"mt\": p, \"ref\": r} for s, p, r in zip(srcs, preds, refs)]\n# note: comet_model.predict returns a dict; 'scores' contains numeric values\ncomet_out = comet_model.predict(data, batch_size=8)\ncomet_scores = comet_out[\"scores\"] if isinstance(comet_out, dict) and \"scores\" in comet_out else comet_out\n\nprint(\"Samples evaluated:\", len(clean_preds))\nprint(\"Mean COMET score:\", float(sum(comet_scores) / len(comet_scores)))\n\n# quick side-by-side preview\nfor i in range(len(preds)):\n    print(f\"\\n--- SAMPLE {i+1} ---\")\n    print(\"SRC :\", srcs[i])\n    print(\"PRED:\", clean_preds[i])\n    print(\"REF :\", refs[i])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T12:09:32.875642Z","iopub.execute_input":"2025-12-12T12:09:32.876488Z","iopub.status.idle":"2025-12-12T12:09:59.301148Z","shell.execute_reply.started":"2025-12-12T12:09:32.876457Z","shell.execute_reply":"2025-12-12T12:09:59.300306Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"389fdf4bf5c54ca98133d18549ed103a"}},"metadata":{}},{"name":"stderr","text":"INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../root/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\nINFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\nINFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\nINFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\nPredicting DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  4.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Samples evaluated: 10\nMean COMET score: 0.8047629356384277\n\n--- SAMPLE 1 ---\nSRC : आपकी कार में ब्लैक बॉक्स?\nPRED: The black box in your car\nREF : A black box in your car?\n\n--- SAMPLE 2 ---\nSRC : जबकि अमेरिका के सड़क योजनाकार, ध्वस्त होते हुए हाईवे सिस्टम को सुधारने के लिए धन की कमी से जूझ रहे हैं, वहीं बहुत-से लोग इसका समाधान छोटे से ब्लैक बॉक्स में देख रहे हैं, जो आपकी कार के डैशबोर्ड पर सफ़ाई से फिट हो जाता है।\nPRED: While road planners in the US are struggling to raise funds to repair the collapsing highway system many are looking for a solution in a small black box that fits neatly on your car dashboard.\nREF : As America's road planners struggle to find the cash to mend a crumbling highway system, many are beginning to see a solution in a little black box that fits neatly by the dashboard of your car.\n\n--- SAMPLE 3 ---\nSRC : यह डिवाइस, जो मोटर-चालक द्वारा वाहन चलाए गए प्रत्येक मील को ट्रैक करती है तथा उस सूचना को अधिकारियों को संचारित करती है, आजकल अमेरिका की प्रमुख सड़कों का वित्त-पोषण करने के लिए पुराने हो चुके सिस्टम का जीर्णोद्धार करने के लिए वाशिंगटन और राज्य नियोजन कार्यालय के लिए एक विवादास्पद प्रयास का मुद्दा बन चुका है।\nPRED: The device that tracks every mile driven by the motorist and transmits that information to the authorities has become the subject of a controversial effort for the Washington and State Planning Office to restore the outdated system of financing America's major roads.\nREF : The devices, which track every mile a motorist drives and transmit that information to bureaucrats, are at the center of a controversial attempt in Washington and state planning offices to overhaul the outdated system for funding America's major roads.\n\n--- SAMPLE 4 ---\nSRC : आम तौर पर हाईवे नियोजन जैसा उबाऊ काम भी अचानक गहन बहस तथा जीवंत गठबंधनों का मुद्दा बन गया है।\nPRED: In general, even boring work such as highway planning has suddenly become the subject of intense debate and lively alliances.\nREF : The usually dull arena of highway planning has suddenly spawned intense debate and colorful alliances.\n\n--- SAMPLE 5 ---\nSRC : आपने द्वारा ड्राइव किए गए मील, तथा संभवतः ड्राइव किए गए स्थान का विवरण रखने - और फिर इस सूचना का उपयोग टैक्स बिल तैयार करने के लिए - सरकार को इन ब्लैक बॉक्स का उपयोग करने की अनुमति देने के पक्ष में समर्थन जुटाने के लिए लिबरेटेरियन पर्यावरणीय समूहों के साथ मिल गए हैं।\nPRED: Libertarians have joined with environmental groups to mobilize support in favour of allowing the government to use these black boxes to keep track of the miles you've driven and possibly the place you've driven and then use this information to generate tax bills.\nREF : Libertarians have joined environmental groups in lobbying to allow government to use the little boxes to keep track of the miles you drive, and possibly where you drive them - then use the information to draw up a tax bill.\n\n--- SAMPLE 6 ---\nSRC : चाय पार्टी भौचक्की है।\nPRED: The tea party is a farce.\nREF : The tea party is aghast.\n\n--- SAMPLE 7 ---\nSRC : अमेरिकी नागरिक स्वतंत्रता संघ भी विभिन्न प्रकार के गोपनीयता मुद्दे उठाते हुए बहुत चिंतित है।\nPRED: The American Civil Liberties Union is also deeply concerned, raising a variety of privacy issues.\nREF : The American Civil Liberties Union is deeply concerned, too, raising a variety of privacy issues.\n\n--- SAMPLE 8 ---\nSRC : जबकि कांग्रेस इस बात पर सहमत नहीं हो सकी कि आगे की कार्यवाही करनी है या नहीं, बहुत से राज्य प्रतीक्षा नहीं कर रहे।\nPRED: While Congress could not agree on whether further action should be taken many states were not waiting.\nREF : And while Congress can't agree on whether to proceed, several states are not waiting.\n\n--- SAMPLE 9 ---\nSRC : वे यह खोज कर रहे हैं कि अगले दशक में वे किस तरह से ऐसी प्रणाली में जा सकते हैं, जिसमें चालक सड़क पर तय किए गए प्रत्येक मील के लिए भुगतान करे।\nPRED: They are exploring how in the next decade they can move to a system in which drivers pay for every mile they cover on the road.\nREF : They are exploring how, over the next decade, they can move to a system in which drivers pay per mile of road they roll over.\n\n--- SAMPLE 10 ---\nSRC : हजारों मोटर-चालकों ने टेस्ट ड्राइव के लिए पहले ही ब्लैक बॉक्स ले लिया है, जिसमें से कुछ में जी.पी.एस. मॉनीटरिंग है।\nPRED: Thousands of motorists have already taken the black box for test drives some of which have GPS monitoring.\nREF : Thousands of motorists have already taken the black boxes, some of which have GPS monitoring, for a test drive.\n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"# Saving the Model and tokenizer\nmodel.save_pretrained(\"pt_model\")\ntokenizer.save_pretrained(\"tokenizer\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:49:15.884636Z","iopub.execute_input":"2025-12-12T11:49:15.885242Z","iopub.status.idle":"2025-12-12T11:49:18.293302Z","shell.execute_reply.started":"2025-12-12T11:49:15.885213Z","shell.execute_reply":"2025-12-12T11:49:18.292668Z"}},"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"('tokenizer/tokenizer_config.json',\n 'tokenizer/special_tokens_map.json',\n 'tokenizer/dict.SRC.json',\n 'tokenizer/dict.TGT.json',\n 'tokenizer/model.SRC',\n 'tokenizer/model.TGT',\n 'tokenizer/added_tokens.json')"},"metadata":{}}],"execution_count":79}]}