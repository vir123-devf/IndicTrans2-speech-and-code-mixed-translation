{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# In this Notebook:- \n* We fine-tuned the IndicTrans2-en-indic-dist-200M model on the PHINC (Parallel Hinglish Social Media Code-Mixed Corpus for Machine Translation) for the EN→HINGLISH translation task. The resulting model was evaluated using a comprehensive suite of MT quality metrics, including BLEU, ChrF, COMET, BERTScore, and BLEURT.","metadata":{}},{"cell_type":"markdown","source":"[](http://)","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y numpy scipy pandas pyarrow datasets transformers\n!pip install --force-reinstall --no-cache-dir \\\n  numpy==1.26.4 \\\n  scipy==1.11.4 \\\n  pandas==2.1.4 \\\n  pyarrow==14.0.2 \\\n  datasets==2.16.1 \\\n  transformers==4.36.2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:34:27.575160Z","iopub.execute_input":"2025-12-18T13:34:27.575568Z","iopub.status.idle":"2025-12-18T13:35:08.596058Z","shell.execute_reply.started":"2025-12-18T13:34:27.575533Z","shell.execute_reply":"2025-12-18T13:35:08.595298Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\nFound existing installation: scipy 1.11.4\nUninstalling scipy-1.11.4:\n  Successfully uninstalled scipy-1.11.4\nFound existing installation: pandas 2.1.4\nUninstalling pandas-2.1.4:\n  Successfully uninstalled pandas-2.1.4\nFound existing installation: pyarrow 14.0.2\nUninstalling pyarrow-14.0.2:\n  Successfully uninstalled pyarrow-14.0.2\nFound existing installation: datasets 2.16.1\nUninstalling datasets-2.16.1:\n  Successfully uninstalled datasets-2.16.1\nFound existing installation: transformers 4.36.2\nUninstalling transformers-4.36.2:\n  Successfully uninstalled transformers-4.36.2\nCollecting numpy==1.26.4\n  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting scipy==1.11.4\n  Downloading scipy-1.11.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pandas==2.1.4\n  Downloading pandas-2.1.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nCollecting pyarrow==14.0.2\n  Downloading pyarrow-14.0.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\nCollecting datasets==2.16.1\n  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\nCollecting transformers==4.36.2\n  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting python-dateutil>=2.8.2 (from pandas==2.1.4)\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\nCollecting pytz>=2020.1 (from pandas==2.1.4)\n  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\nCollecting tzdata>=2022.1 (from pandas==2.1.4)\n  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting filelock (from datasets==2.16.1)\n  Downloading filelock-3.20.1-py3-none-any.whl.metadata (2.1 kB)\nCollecting pyarrow-hotfix (from datasets==2.16.1)\n  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\nCollecting dill<0.3.8,>=0.3.0 (from datasets==2.16.1)\n  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\nCollecting requests>=2.19.0 (from datasets==2.16.1)\n  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\nCollecting tqdm>=4.62.1 (from datasets==2.16.1)\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m285.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting xxhash (from datasets==2.16.1)\n  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\nCollecting multiprocess (from datasets==2.16.1)\n  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\nCollecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1)\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nCollecting aiohttp (from datasets==2.16.1)\n  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\nCollecting huggingface-hub>=0.19.4 (from datasets==2.16.1)\n  Downloading huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\nCollecting packaging (from datasets==2.16.1)\n  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting pyyaml>=5.1 (from datasets==2.16.1)\n  Downloading pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\nCollecting huggingface-hub>=0.19.4 (from datasets==2.16.1)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nCollecting regex!=2019.12.17 (from transformers==4.36.2)\n  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m203.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tokenizers<0.19,>=0.14 (from transformers==4.36.2)\n  Downloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting safetensors>=0.3.1 (from transformers==4.36.2)\n  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\nCollecting aiohappyeyeballs>=2.5.0 (from aiohttp->datasets==2.16.1)\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\nCollecting aiosignal>=1.4.0 (from aiohttp->datasets==2.16.1)\n  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\nCollecting attrs>=17.3.0 (from aiohttp->datasets==2.16.1)\n  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\nCollecting frozenlist>=1.1.1 (from aiohttp->datasets==2.16.1)\n  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\nCollecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.16.1)\n  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\nCollecting propcache>=0.2.0 (from aiohttp->datasets==2.16.1)\n  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==2.16.1)\n  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.1/75.1 kB\u001b[0m \u001b[31m287.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.19.4->datasets==2.16.1)\n  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.19.4->datasets==2.16.1)\n  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nCollecting six>=1.5 (from python-dateutil>=2.8.2->pandas==2.1.4)\n  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\nCollecting charset_normalizer<4,>=2 (from requests>=2.19.0->datasets==2.16.1)\n  Downloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\nCollecting idna<4,>=2.5 (from requests>=2.19.0->datasets==2.16.1)\n  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\nCollecting urllib3<3,>=1.21.1 (from requests>=2.19.0->datasets==2.16.1)\n  Downloading urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\nCollecting certifi>=2017.4.17 (from requests>=2.19.0->datasets==2.16.1)\n  Downloading certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\nCollecting multiprocess (from datasets==2.16.1)\n  Downloading multiprocess-0.70.17-py312-none-any.whl.metadata (7.2 kB)\n  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\nDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m284.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading scipy-1.11.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.8/35.8 MB\u001b[0m \u001b[31m278.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pandas-2.1.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m256.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pyarrow-14.0.2-cp312-cp312-manylinux_2_28_x86_64.whl (38.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m272.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading datasets-2.16.1-py3-none-any.whl (507 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m363.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m278.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m307.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m332.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m365.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m355.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m243.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m366.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m355.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m392.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m381.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m169.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.2/507.2 kB\u001b[0m \u001b[31m324.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m272.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m240.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.5/348.5 kB\u001b[0m \u001b[31m324.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading filelock-3.20.1-py3-none-any.whl (16 kB)\nDownloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m274.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\nDownloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m310.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nDownloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nDownloading attrs-25.4.0-py3-none-any.whl (67 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m301.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.4/159.4 kB\u001b[0m \u001b[31m326.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m290.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m314.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m356.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading idna-3.11-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m290.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.1/256.1 kB\u001b[0m \u001b[31m300.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.6/221.6 kB\u001b[0m \u001b[31m355.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\nDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m285.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading urllib3-2.6.2-py3-none-any.whl (131 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.2/131.2 kB\u001b[0m \u001b[31m302.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m381.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pytz, xxhash, urllib3, tzdata, typing-extensions, tqdm, six, safetensors, regex, pyyaml, pyarrow-hotfix, propcache, packaging, numpy, multidict, idna, hf-xet, fsspec, frozenlist, filelock, dill, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, scipy, requests, python-dateutil, pyarrow, multiprocess, aiosignal, pandas, huggingface-hub, aiohttp, tokenizers, transformers, datasets\n  Attempting uninstall: pytz\n    Found existing installation: pytz 2025.2\n    Uninstalling pytz-2025.2:\n      Successfully uninstalled pytz-2025.2\n  Attempting uninstall: xxhash\n    Found existing installation: xxhash 3.6.0\n    Uninstalling xxhash-3.6.0:\n      Successfully uninstalled xxhash-3.6.0\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.6.2\n    Uninstalling urllib3-2.6.2:\n      Successfully uninstalled urllib3-2.6.2\n  Attempting uninstall: tzdata\n    Found existing installation: tzdata 2025.3\n    Uninstalling tzdata-2025.3:\n      Successfully uninstalled tzdata-2025.3\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.15.0\n    Uninstalling typing_extensions-4.15.0:\n      Successfully uninstalled typing_extensions-4.15.0\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.67.1\n    Uninstalling tqdm-4.67.1:\n      Successfully uninstalled tqdm-4.67.1\n  Attempting uninstall: six\n    Found existing installation: six 1.17.0\n    Uninstalling six-1.17.0:\n      Successfully uninstalled six-1.17.0\n  Attempting uninstall: safetensors\n    Found existing installation: safetensors 0.7.0\n    Uninstalling safetensors-0.7.0:\n      Successfully uninstalled safetensors-0.7.0\n  Attempting uninstall: regex\n    Found existing installation: regex 2025.11.3\n    Uninstalling regex-2025.11.3:\n      Successfully uninstalled regex-2025.11.3\n  Attempting uninstall: pyyaml\n    Found existing installation: PyYAML 6.0.3\n    Uninstalling PyYAML-6.0.3:\n      Successfully uninstalled PyYAML-6.0.3\n  Attempting uninstall: pyarrow-hotfix\n    Found existing installation: pyarrow-hotfix 0.7\n    Uninstalling pyarrow-hotfix-0.7:\n      Successfully uninstalled pyarrow-hotfix-0.7\n  Attempting uninstall: propcache\n    Found existing installation: propcache 0.4.1\n    Uninstalling propcache-0.4.1:\n      Successfully uninstalled propcache-0.4.1\n  Attempting uninstall: packaging\n    Found existing installation: packaging 25.0\n    Uninstalling packaging-25.0:\n      Successfully uninstalled packaging-25.0\n  Attempting uninstall: multidict\n    Found existing installation: multidict 6.7.0\n    Uninstalling multidict-6.7.0:\n      Successfully uninstalled multidict-6.7.0\n  Attempting uninstall: idna\n    Found existing installation: idna 3.11\n    Uninstalling idna-3.11:\n      Successfully uninstalled idna-3.11\n  Attempting uninstall: hf-xet\n    Found existing installation: hf-xet 1.2.0\n    Uninstalling hf-xet-1.2.0:\n      Successfully uninstalled hf-xet-1.2.0\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2023.10.0\n    Uninstalling fsspec-2023.10.0:\n      Successfully uninstalled fsspec-2023.10.0\n  Attempting uninstall: frozenlist\n    Found existing installation: frozenlist 1.8.0\n    Uninstalling frozenlist-1.8.0:\n      Successfully uninstalled frozenlist-1.8.0\n  Attempting uninstall: filelock\n    Found existing installation: filelock 3.20.1\n    Uninstalling filelock-3.20.1:\n      Successfully uninstalled filelock-3.20.1\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.7\n    Uninstalling dill-0.3.7:\n      Successfully uninstalled dill-0.3.7\n  Attempting uninstall: charset_normalizer\n    Found existing installation: charset-normalizer 3.4.4\n    Uninstalling charset-normalizer-3.4.4:\n      Successfully uninstalled charset-normalizer-3.4.4\n  Attempting uninstall: certifi\n    Found existing installation: certifi 2025.11.12\n    Uninstalling certifi-2025.11.12:\n      Successfully uninstalled certifi-2025.11.12\n  Attempting uninstall: attrs\n    Found existing installation: attrs 25.4.0\n    Uninstalling attrs-25.4.0:\n      Successfully uninstalled attrs-25.4.0\n  Attempting uninstall: aiohappyeyeballs\n    Found existing installation: aiohappyeyeballs 2.6.1\n    Uninstalling aiohappyeyeballs-2.6.1:\n      Successfully uninstalled aiohappyeyeballs-2.6.1\n  Attempting uninstall: yarl\n    Found existing installation: yarl 1.22.0\n    Uninstalling yarl-1.22.0:\n      Successfully uninstalled yarl-1.22.0\n  Attempting uninstall: requests\n    Found existing installation: requests 2.32.5\n    Uninstalling requests-2.32.5:\n      Successfully uninstalled requests-2.32.5\n  Attempting uninstall: python-dateutil\n    Found existing installation: python-dateutil 2.9.0.post0\n    Uninstalling python-dateutil-2.9.0.post0:\n      Successfully uninstalled python-dateutil-2.9.0.post0\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.15\n    Uninstalling multiprocess-0.70.15:\n      Successfully uninstalled multiprocess-0.70.15\n  Attempting uninstall: aiosignal\n    Found existing installation: aiosignal 1.4.0\n    Uninstalling aiosignal-1.4.0:\n      Successfully uninstalled aiosignal-1.4.0\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.36.0\n    Uninstalling huggingface-hub-0.36.0:\n      Successfully uninstalled huggingface-hub-0.36.0\n  Attempting uninstall: aiohttp\n    Found existing installation: aiohttp 3.13.2\n    Uninstalling aiohttp-3.13.2:\n      Successfully uninstalled aiohttp-3.13.2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngrain 0.2.15 requires protobuf>=5.28.3, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.20.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nray 2.52.1 requires click!=8.3.*,>=7.0, but you have click 8.3.1 which is incompatible.\nnilearn 0.12.1 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\nlangchain 0.3.27 requires SQLAlchemy<3,>=1.4, but you have sqlalchemy 1.2.19 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.2 which is incompatible.\nxarray 2025.10.1 requires pandas>=2.2, but you have pandas 2.1.4 which is incompatible.\nipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.2.19 which is incompatible.\njaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\njaxlib 0.7.2 requires scipy>=1.13, but you have scipy 1.11.4 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\ntsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\njax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\njax 0.7.2 requires scipy>=1.13, but you have scipy 1.11.4 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\nbigframes 2.26.0 requires pyarrow>=15.0.2, but you have pyarrow 14.0.2 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nmizani 0.13.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\npytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 attrs-25.4.0 certifi-2025.11.12 charset_normalizer-3.4.4 datasets-2.16.1 dill-0.3.7 filelock-3.20.1 frozenlist-1.8.0 fsspec-2023.10.0 hf-xet-1.2.0 huggingface-hub-0.36.0 idna-3.11 multidict-6.7.0 multiprocess-0.70.15 numpy-1.26.4 packaging-25.0 pandas-2.1.4 propcache-0.4.1 pyarrow-14.0.2 pyarrow-hotfix-0.7 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.3 regex-2025.11.3 requests-2.32.5 safetensors-0.7.0 scipy-1.11.4 six-1.17.0 tokenizers-0.15.2 tqdm-4.67.1 transformers-4.36.2 typing-extensions-4.15.0 tzdata-2025.3 urllib3-2.6.2 xxhash-3.6.0 yarl-1.22.0\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"#Checking wheather GPU is working or not\n!nvidia-smi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:08.597676Z","iopub.execute_input":"2025-12-18T13:35:08.598573Z","iopub.status.idle":"2025-12-18T13:35:08.842021Z","shell.execute_reply.started":"2025-12-18T13:35:08.598535Z","shell.execute_reply":"2025-12-18T13:35:08.841321Z"}},"outputs":[{"name":"stdout","text":"Thu Dec 18 13:35:08 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   38C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   38C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# installing dataset and transformer\n!pip install datasets transformers[sentencepiece] sacrebleu -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:08.843496Z","iopub.execute_input":"2025-12-18T13:35:08.843857Z","iopub.status.idle":"2025-12-18T13:35:12.473851Z","shell.execute_reply.started":"2025-12-18T13:35:08.843822Z","shell.execute_reply":"2025-12-18T13:35:12.472887Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# to remove version conflict of Protobuf so, downgrade version of Protobuf\n!pip install protobuf==3.20.3 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:12.476242Z","iopub.execute_input":"2025-12-18T13:35:12.476512Z","iopub.status.idle":"2025-12-18T13:35:15.740173Z","shell.execute_reply.started":"2025-12-18T13:35:12.476482Z","shell.execute_reply":"2025-12-18T13:35:15.739397Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.12/dist-packages (3.20.3)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Importing all required modules\nimport os\nimport sys\nimport transformers\nimport torch  # pytorch Import\nimport sacrebleu\nfrom torch.amp import autocast, GradScaler\nfrom tqdm.auto import tqdm\nfrom transformers import DataCollatorForSeq2Seq\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom datasets import load_dataset # for loading the dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM # For getting Embedding\nfrom transformers import DataCollatorForSeq2Seq #getting sequential model and collator for loading batchwise of data\nfrom torch.optim import AdamW # Optimizer\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:15.741580Z","iopub.execute_input":"2025-12-18T13:35:15.742036Z","iopub.status.idle":"2025-12-18T13:35:15.747730Z","shell.execute_reply.started":"2025-12-18T13:35:15.741990Z","shell.execute_reply":"2025-12-18T13:35:15.747089Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# Indictrans2-en-indic-dist-200M Model\n* source: https://huggingface.co/ai4bharat/indictrans2-en-indic-dist-200M","metadata":{}},{"cell_type":"code","source":"# Enter Access Token and rerun\nfrom huggingface_hub import login\nlogin(new_session=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:15.748889Z","iopub.execute_input":"2025-12-18T13:35:15.749535Z","iopub.status.idle":"2025-12-18T13:35:15.761156Z","shell.execute_reply.started":"2025-12-18T13:35:15.749507Z","shell.execute_reply":"2025-12-18T13:35:15.760606Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# Note:\n* I was using the free version of Kaggle, and the memory limit was getting exhausted while training the 1B-parameter model. Because of this constraint, I switched to using the 200M-parameter model instead.","metadata":{}},{"cell_type":"code","source":"ckpt = \"ai4bharat/indictrans2-en-indic-dist-200M\" # Model Checkpoint \n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    ckpt,\n    trust_remote_code=True,                                         \n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    ckpt,\n    trust_remote_code=True\n)\n\n# Move safely to GPU\nmodel = model.to(torch.float16).to(\"cuda\")   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:15.762036Z","iopub.execute_input":"2025-12-18T13:35:15.762316Z","iopub.status.idle":"2025-12-18T13:35:25.155949Z","shell.execute_reply.started":"2025-12-18T13:35:15.762292Z","shell.execute_reply":"2025-12-18T13:35:25.155315Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.37k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cf3e8b08cce4856b0020eae65c18d5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_indictrans.py:   0%|          | 0.00/14.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc051ae583d94d3ba03a789b45f5bd42"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-en-indic-dist-200M:\n- configuration_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_indictrans.py:   0%|          | 0.00/79.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfef385d5842444b9a2a1ae129a38e54"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-en-indic-dist-200M:\n- modeling_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  _torch_pytree._register_pytree_node(\n/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  _torch_pytree._register_pytree_node(\n/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.10G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49f43eb111be43d7bddd582c2906d1fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d6ad55d7adf4c6d947510b4b39f920b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a867eacbabb4568beb64ebba5870429"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenization_indictrans.py:   0%|          | 0.00/8.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aa6643f2a12416ba180e01b1f92d0a5"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-en-indic-dist-200M:\n- tokenization_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"dict.SRC.json:   0%|          | 0.00/645k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a54074b0bb941ed8cc5c7a2567f22f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dict.TGT.json:   0%|          | 0.00/3.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef673c28f4b44ece84eaa55fd87a65cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.SRC:   0%|          | 0.00/759k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a783b0a9c55452f87f1b4415c23f63b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.TGT:   0%|          | 0.00/3.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec068049e2d64963b6d35653e6fb07dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32d572450d6845d7848ae8e3ad46c891"}},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"# The Dataset¶\n\n* Source: https://huggingface.co/datasets/LingoIITGN/PHINC","metadata":{}},{"cell_type":"code","source":"raw_dataset = load_dataset(\"LingoIITGN/PHINC\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:25.156928Z","iopub.execute_input":"2025-12-18T13:35:25.157213Z","iopub.status.idle":"2025-12-18T13:35:28.858824Z","shell.execute_reply.started":"2025-12-18T13:35:25.157174Z","shell.execute_reply":"2025-12-18T13:35:28.857862Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01c61a449d724c94b71be1e712d054e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f65116dda8fb472bb80553f14d745446"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b749213ebba45c68be00d4dd57ef944"}},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"raw_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:28.860017Z","iopub.execute_input":"2025-12-18T13:35:28.860297Z","iopub.status.idle":"2025-12-18T13:35:28.865599Z","shell.execute_reply.started":"2025-12-18T13:35:28.860272Z","shell.execute_reply":"2025-12-18T13:35:28.864862Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Sentence', 'English_Translation'],\n        num_rows: 13738\n    })\n})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"def convert_to_translation(example):\n    return {\n        \"translation\": {\n            \"en\": example[\"English_Translation\"],\n            \"hing\": example[\"Sentence\"]\n        }\n    }\n\nraw_dataset = raw_dataset.map(convert_to_translation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:28.869060Z","iopub.execute_input":"2025-12-18T13:35:28.869301Z","iopub.status.idle":"2025-12-18T13:35:32.712299Z","shell.execute_reply.started":"2025-12-18T13:35:28.869261Z","shell.execute_reply":"2025-12-18T13:35:32.710995Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13738 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ea92953b9d14e0c897f2a613ce1e901"}},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"raw_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:32.713655Z","iopub.execute_input":"2025-12-18T13:35:32.714094Z","iopub.status.idle":"2025-12-18T13:35:32.719498Z","shell.execute_reply.started":"2025-12-18T13:35:32.714047Z","shell.execute_reply":"2025-12-18T13:35:32.718658Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Sentence', 'English_Translation', 'translation'],\n        num_rows: 13738\n    })\n})"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"from datasets import DatasetDict\n\n# total rows\ntotal_rows = raw_dataset[\"train\"].num_rows\n\n# required splits\ntrain_rows = 12000\nvalid_rows = 538\ntest_rows = 1200\n\n# slice the dataset\ntrain_dataset = raw_dataset[\"train\"].select(range(0, train_rows))\nvalid_dataset = raw_dataset[\"train\"].select(range(train_rows, train_rows + valid_rows))\ntest_dataset  = raw_dataset[\"train\"].select(range(train_rows + valid_rows,\n                                                  train_rows + valid_rows + test_rows))\n\n# create final DatasetDict\nfinal_dataset = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": valid_dataset,\n    \"test\": test_dataset\n})\n\nfinal_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:32.720741Z","iopub.execute_input":"2025-12-18T13:35:32.721147Z","iopub.status.idle":"2025-12-18T13:35:33.397684Z","shell.execute_reply.started":"2025-12-18T13:35:32.721104Z","shell.execute_reply":"2025-12-18T13:35:33.396872Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Sentence', 'English_Translation', 'translation'],\n        num_rows: 12000\n    })\n    validation: Dataset({\n        features: ['Sentence', 'English_Translation', 'translation'],\n        num_rows: 538\n    })\n    test: Dataset({\n        features: ['Sentence', 'English_Translation', 'translation'],\n        num_rows: 1200\n    })\n})"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"raw_dataset = final_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:33.398874Z","iopub.execute_input":"2025-12-18T13:35:33.399131Z","iopub.status.idle":"2025-12-18T13:35:33.407494Z","shell.execute_reply.started":"2025-12-18T13:35:33.399099Z","shell.execute_reply":"2025-12-18T13:35:33.406833Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"# Observation for Statistics related to dataset","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport numpy as np\nimport math\nimport nltk\nnltk.download(\"punkt\")  # one-time\n\ndef add_stats(example):\n    text = example[\"translation\"][\"en\"]\n    # guard\n    if text is None: text = \"\"\n    text = text.strip() # Removes unwanted spacing\n    words = text.split()\n    # sentence count (approx)\n    sents = nltk.tokenize.sent_tokenize(text) if text else []\n    example[\"num_words\"] = len(words)\n    example[\"num_chars\"] = len(text)\n    example[\"num_sentences\"] = len(sents)\n    return example\n\nraw_dataset = raw_dataset.map(add_stats, batched=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:33.408380Z","iopub.execute_input":"2025-12-18T13:35:33.408649Z","iopub.status.idle":"2025-12-18T13:35:35.353393Z","shell.execute_reply.started":"2025-12-18T13:35:33.408599Z","shell.execute_reply":"2025-12-18T13:35:35.352623Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64c7ae7a81a24372bea3248ca33bb874"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/538 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"411a9dc2c3254fa8b348be1859496a48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e26f3b6000fd495b9b29b5ed5186d0eb"}},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"# Obtaining the Statistics:\n\ndef summary_stats(arr):\n    arr = np.array(arr)\n    return {\n        \"count\": int(arr.size),\n        \"min\": int(arr.min()) if arr.size>0 else None,\n        \"p1\": int(np.percentile(arr, 1)) if arr.size>0 else None,\n        \"p10\": int(np.percentile(arr, 10)) if arr.size>0 else None,\n        \"median\": float(np.median(arr)) if arr.size>0 else None,\n        \"mean\": float(arr.mean()) if arr.size>0 else None,\n        \"std\": float(arr.std(ddof=0)) if arr.size>0 else None,\n        \"p90\": int(np.percentile(arr, 90)) if arr.size>0 else None,\n        \"p99\": int(np.percentile(arr, 99)) if arr.size>0 else None,\n        \"max\": int(arr.max()) if arr.size>0 else None,\n    }\n\nfor split in raw_dataset:\n    d = raw_dataset[split]\n    print(f\"\\n=== {split.upper()} ===\")\n    print(\"Words:\", summary_stats(d[\"num_words\"]))\n    print(\"Chars:\", summary_stats(d[\"num_chars\"]))\n    print(\"Sentences:\", summary_stats(d[\"num_sentences\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:35.354368Z","iopub.execute_input":"2025-12-18T13:35:35.354646Z","iopub.status.idle":"2025-12-18T13:35:35.396078Z","shell.execute_reply.started":"2025-12-18T13:35:35.354621Z","shell.execute_reply":"2025-12-18T13:35:35.395359Z"}},"outputs":[{"name":"stdout","text":"\n=== TRAIN ===\nWords: {'count': 12000, 'min': 1, 'p1': 1, 'p10': 5, 'median': 11.0, 'mean': 12.335166666666666, 'std': 6.736442926764506, 'p90': 22, 'p99': 31, 'max': 46}\nChars: {'count': 12000, 'min': 1, 'p1': 1, 'p10': 30, 'median': 67.0, 'mean': 74.00516666666667, 'std': 38.514235116887484, 'p90': 130, 'p99': 167, 'max': 278}\nSentences: {'count': 12000, 'min': 1, 'p1': 1, 'p10': 1, 'median': 1.0, 'mean': 1.5585, 'std': 0.989062392706682, 'p90': 3, 'p99': 5, 'max': 16}\n\n=== VALIDATION ===\nWords: {'count': 538, 'min': 1, 'p1': 1, 'p10': 5, 'median': 11.0, 'mean': 11.962825278810408, 'std': 6.470587802969751, 'p90': 21, 'p99': 28, 'max': 36}\nChars: {'count': 538, 'min': 1, 'p1': 1, 'p10': 28, 'median': 69.0, 'mean': 73.38289962825279, 'std': 39.0196100416231, 'p90': 125, 'p99': 169, 'max': 221}\nSentences: {'count': 538, 'min': 1, 'p1': 1, 'p10': 1, 'median': 1.0, 'mean': 1.3717472118959109, 'std': 0.694709906589641, 'p90': 2, 'p99': 4, 'max': 6}\n\n=== TEST ===\nWords: {'count': 1200, 'min': 1, 'p1': 1, 'p10': 5, 'median': 11.0, 'mean': 12.205833333333333, 'std': 6.622069110599462, 'p90': 22, 'p99': 30, 'max': 37}\nChars: {'count': 1200, 'min': 1, 'p1': 1, 'p10': 30, 'median': 67.0, 'mean': 73.975, 'std': 37.8891942599294, 'p90': 129, 'p99': 165, 'max': 200}\nSentences: {'count': 1200, 'min': 1, 'p1': 1, 'p10': 1, 'median': 1.0, 'mean': 1.6225, 'std': 0.9814922737002738, 'p90': 3, 'p99': 5, 'max': 15}\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"from datasets import DatasetDict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:35.397116Z","iopub.execute_input":"2025-12-18T13:35:35.397393Z","iopub.status.idle":"2025-12-18T13:35:35.401045Z","shell.execute_reply.started":"2025-12-18T13:35:35.397369Z","shell.execute_reply":"2025-12-18T13:35:35.400171Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Train has min length of sentences as 0 ('min': 0) so, we Remove these row from dataset\ndef not_empty(example):\n    text = example[\"translation\"][\"en\"]\n    return text is not None and len(text.strip()) > 0\n    \nclean_train = raw_dataset[\"train\"].filter(not_empty)\nclean_val   = raw_dataset[\"validation\"].filter(not_empty)\nclean_test  = raw_dataset[\"test\"].filter(not_empty)\n\nraw_dataset = DatasetDict({\n    \"train\": clean_train,\n    \"validation\": clean_val,\n    \"test\": clean_test\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:35.401905Z","iopub.execute_input":"2025-12-18T13:35:35.402094Z","iopub.status.idle":"2025-12-18T13:35:35.612481Z","shell.execute_reply.started":"2025-12-18T13:35:35.402075Z","shell.execute_reply":"2025-12-18T13:35:35.611839Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/12000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f589df31d224ebc996cb1ce9e284e51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/538 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"467955ed4fc04322b386e6f273a40e0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6783379d027546b59f75f27ee73c7357"}},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"type(raw_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:35.613319Z","iopub.execute_input":"2025-12-18T13:35:35.613593Z","iopub.status.idle":"2025-12-18T13:35:35.618241Z","shell.execute_reply.started":"2025-12-18T13:35:35.613569Z","shell.execute_reply":"2025-12-18T13:35:35.617636Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"datasets.dataset_dict.DatasetDict"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"# compute p99 threshold('p99': 31 so, removing other outliers(longer than 31 words))\nword_lengths = np.array(raw_dataset[\"train\"][\"num_words\"])\np99_threshold = int(np.percentile(word_lengths, 99))\nprint(\"Removing sentences longer than:\", p99_threshold, \"words\")\nraw_dataset[\"train\"] = raw_dataset[\"train\"].filter(\n    lambda ex: ex[\"num_words\"] <= p99_threshold\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:35.619171Z","iopub.execute_input":"2025-12-18T13:35:35.619506Z","iopub.status.idle":"2025-12-18T13:35:36.023370Z","shell.execute_reply.started":"2025-12-18T13:35:35.619471Z","shell.execute_reply":"2025-12-18T13:35:36.022648Z"}},"outputs":[{"name":"stdout","text":"Removing sentences longer than: 31 words\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/12000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf40c65269c94d1d888d0151b438bf53"}},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"# Obtaining the desired Statistics\ndef summary_stats(arr):\n    arr = np.array(arr)\n    return {\n        \"count\": int(arr.size),\n        \"min\": int(arr.min()) if arr.size>0 else None,\n        \"p1\": int(np.percentile(arr, 1)) if arr.size>0 else None,\n        \"p10\": int(np.percentile(arr, 10)) if arr.size>0 else None,\n        \"median\": float(np.median(arr)) if arr.size>0 else None,\n        \"mean\": float(arr.mean()) if arr.size>0 else None,\n        \"std\": float(arr.std(ddof=0)) if arr.size>0 else None,\n        \"p90\": int(np.percentile(arr, 90)) if arr.size>0 else None,\n        \"p99\": int(np.percentile(arr, 99)) if arr.size>0 else None,\n        \"max\": int(arr.max()) if arr.size>0 else None,\n    }\n\nfor split in raw_dataset:\n    d = raw_dataset[split]\n    print(f\"\\n=== {split.upper()} ===\")\n    print(\"Words:\", summary_stats(d[\"num_words\"]))\n    print(\"Chars:\", summary_stats(d[\"num_chars\"]))\n    print(\"Sentences:\", summary_stats(d[\"num_sentences\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:36.024618Z","iopub.execute_input":"2025-12-18T13:35:36.025105Z","iopub.status.idle":"2025-12-18T13:35:36.200685Z","shell.execute_reply.started":"2025-12-18T13:35:36.025075Z","shell.execute_reply":"2025-12-18T13:35:36.199814Z"}},"outputs":[{"name":"stdout","text":"\n=== TRAIN ===\nWords: {'count': 11912, 'min': 1, 'p1': 1, 'p10': 5, 'median': 11.0, 'mean': 12.170752182672935, 'std': 6.478840709255029, 'p90': 22, 'p99': 29, 'max': 31}\nChars: {'count': 11912, 'min': 1, 'p1': 1, 'p10': 30, 'median': 67.0, 'mean': 73.26544660846206, 'std': 37.62196715365125, 'p90': 129, 'p99': 161, 'max': 278}\nSentences: {'count': 11912, 'min': 1, 'p1': 1, 'p10': 1, 'median': 1.0, 'mean': 1.542981867024849, 'std': 0.9439458074386832, 'p90': 3, 'p99': 5, 'max': 12}\n\n=== VALIDATION ===\nWords: {'count': 538, 'min': 1, 'p1': 1, 'p10': 5, 'median': 11.0, 'mean': 11.962825278810408, 'std': 6.470587802969751, 'p90': 21, 'p99': 28, 'max': 36}\nChars: {'count': 538, 'min': 1, 'p1': 1, 'p10': 28, 'median': 69.0, 'mean': 73.38289962825279, 'std': 39.0196100416231, 'p90': 125, 'p99': 169, 'max': 221}\nSentences: {'count': 538, 'min': 1, 'p1': 1, 'p10': 1, 'median': 1.0, 'mean': 1.3717472118959109, 'std': 0.694709906589641, 'p90': 2, 'p99': 4, 'max': 6}\n\n=== TEST ===\nWords: {'count': 1200, 'min': 1, 'p1': 1, 'p10': 5, 'median': 11.0, 'mean': 12.205833333333333, 'std': 6.622069110599462, 'p90': 22, 'p99': 30, 'max': 37}\nChars: {'count': 1200, 'min': 1, 'p1': 1, 'p10': 30, 'median': 67.0, 'mean': 73.975, 'std': 37.8891942599294, 'p90': 129, 'p99': 165, 'max': 200}\nSentences: {'count': 1200, 'min': 1, 'p1': 1, 'p10': 1, 'median': 1.0, 'mean': 1.6225, 'std': 0.9814922737002738, 'p90': 3, 'p99': 5, 'max': 15}\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# Sample Example\nraw_dataset['train'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:36.201673Z","iopub.execute_input":"2025-12-18T13:35:36.201988Z","iopub.status.idle":"2025-12-18T13:35:36.207830Z","shell.execute_reply.started":"2025-12-18T13:35:36.201958Z","shell.execute_reply":"2025-12-18T13:35:36.207140Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"{'Sentence': \"@someUSER congratulations on you celebrating british kid singers sophia grace's and rosie's 1st anniversary of a visit of your show .  how\",\n 'English_Translation': \"@some users congratulate you for celebrating British kid singers Sophia Grace's and Rosie's 1st anniversary visit of your show\",\n 'translation': {'en': \"@some users congratulate you for celebrating British kid singers Sophia Grace's and Rosie's 1st anniversary visit of your show\",\n  'hing': \"@someUSER congratulations on you celebrating british kid singers sophia grace's and rosie's 1st anniversary of a visit of your show .  how\"},\n 'num_words': 19,\n 'num_chars': 126,\n 'num_sentences': 1}"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"from datasets import DatasetDict\n\n# New desired sizes\nN_TRAIN = 2000\nN_VAL   = 150\nN_TEST  = 250\n\n# Downsample using .select()\nsmall_train = raw_dataset[\"train\"].select(range(N_TRAIN))\nsmall_val   = raw_dataset[\"validation\"].select(range(N_VAL))\nsmall_test  = raw_dataset[\"test\"].select(range(N_TEST))\n\n# Create a new DatasetDict\nsmall_dataset = DatasetDict({\n    \"train\": small_train,\n    \"validation\": small_val,\n    \"test\": small_test\n})\n\nsmall_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:36.208687Z","iopub.execute_input":"2025-12-18T13:35:36.209147Z","iopub.status.idle":"2025-12-18T13:35:36.226478Z","shell.execute_reply.started":"2025-12-18T13:35:36.209123Z","shell.execute_reply":"2025-12-18T13:35:36.225808Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Sentence', 'English_Translation', 'translation', 'num_words', 'num_chars', 'num_sentences'],\n        num_rows: 2000\n    })\n    validation: Dataset({\n        features: ['Sentence', 'English_Translation', 'translation', 'num_words', 'num_chars', 'num_sentences'],\n        num_rows: 150\n    })\n    test: Dataset({\n        features: ['Sentence', 'English_Translation', 'translation', 'num_words', 'num_chars', 'num_sentences'],\n        num_rows: 250\n    })\n})"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"model.eval() # Evaluation of model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:36.227474Z","iopub.execute_input":"2025-12-18T13:35:36.228198Z","iopub.status.idle":"2025-12-18T13:35:36.236738Z","shell.execute_reply.started":"2025-12-18T13:35:36.228166Z","shell.execute_reply":"2025-12-18T13:35:36.236014Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"IndicTransForConditionalGeneration(\n  (model): IndicTransModel(\n    (encoder): IndicTransEncoder(\n      (embed_tokens): Embedding(32322, 512, padding_idx=1)\n      (embed_positions): IndicTransSinusoidalPositionalEmbedding()\n      (layers): ModuleList(\n        (0-17): 18 x IndicTransEncoderLayer(\n          (self_attn): IndicTransAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): IndicTransDecoder(\n      (embed_tokens): Embedding(122672, 512, padding_idx=1)\n      (embed_positions): IndicTransSinusoidalPositionalEmbedding()\n      (layers): ModuleList(\n        (0-17): 18 x IndicTransDecoderLayer(\n          (self_attn): IndicTransAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): IndicTransAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=512, out_features=122672, bias=False)\n)"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"# Hinglish --> Hindi Translation\n\nSRC_TAG = \"eng_Latn\"   # Hinglish / Roman Hindi\nTGT_TAG = \"hin_Deva\"  # Hindi\ndef hinglish_to_hindi(sentences, batch_size=8, max_len=128):\n    outputs = []\n\n    device = model.device\n\n    for i in range(0, len(sentences), batch_size):\n        batch = sentences[i:i + batch_size]\n\n        tagged = [\n            f\"{SRC_TAG} {TGT_TAG} {text}\"\n            for text in batch\n        ]\n\n        inputs = tokenizer(\n            tagged,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=max_len\n        ).to(device)\n\n        with torch.no_grad():\n            generated = model.generate(\n                **inputs,\n                max_length=max_len,\n                num_beams=4,\n                use_cache=False,       \n                early_stopping=True\n            )\n\n        decoded = tokenizer.batch_decode(\n            generated,\n            skip_special_tokens=True\n        )\n\n        outputs.extend(decoded)\n\n    return outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:36.237714Z","iopub.execute_input":"2025-12-18T13:35:36.237990Z","iopub.status.idle":"2025-12-18T13:35:36.249700Z","shell.execute_reply.started":"2025-12-18T13:35:36.237961Z","shell.execute_reply":"2025-12-18T13:35:36.249001Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"def convert_split(split):\n    hinglish_sentences = [ex[\"hing\"] for ex in split[\"translation\"]]\n    hindi_sentences = hinglish_to_hindi(hinglish_sentences)\n\n    new_translation = []\n    for i in range(len(split)):\n        new_translation.append({\n            \"en\": split[\"translation\"][i][\"en\"],\n            \"hing\": split[\"translation\"][i][\"hing\"],\n            \"hi\": hindi_sentences[i]\n        })\n\n    return split.remove_columns(\"translation\").add_column(\n        \"translation\", new_translation\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:36.250682Z","iopub.execute_input":"2025-12-18T13:35:36.251040Z","iopub.status.idle":"2025-12-18T13:35:36.265398Z","shell.execute_reply.started":"2025-12-18T13:35:36.251015Z","shell.execute_reply":"2025-12-18T13:35:36.264578Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"small_dataset[\"train\"] = convert_split(small_dataset[\"train\"])\nsmall_dataset[\"validation\"] = convert_split(small_dataset[\"validation\"])\nsmall_dataset[\"test\"] = convert_split(small_dataset[\"test\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:35:36.266293Z","iopub.execute_input":"2025-12-18T13:35:36.266579Z","iopub.status.idle":"2025-12-18T13:48:20.534150Z","shell.execute_reply.started":"2025-12-18T13:35:36.266549Z","shell.execute_reply":"2025-12-18T13:48:20.533340Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"021638fc2b0a4f18899cd9021067e1cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"583f8e23b3e14aa98d8aa3e08e3b6c7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"988ddd914e41493bad4cab6db4c1ef87"}},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"small_dataset[\"train\"][\"translation\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:48:20.535217Z","iopub.execute_input":"2025-12-18T13:48:20.535777Z","iopub.status.idle":"2025-12-18T13:48:20.558365Z","shell.execute_reply.started":"2025-12-18T13:48:20.535747Z","shell.execute_reply":"2025-12-18T13:48:20.557682Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"{'en': \"@some users congratulate you for celebrating British kid singers Sophia Grace's and Rosie's 1st anniversary visit of your show\",\n 'hi': '@someUSER आपको ब्रिटिश बाल गायक सोफिया ग्रेस और रोजी की अपने शो की यात्रा की पहली वर्षगांठ मनाने के लिए बधाई ।',\n 'hing': \"@someUSER congratulations on you celebrating british kid singers sophia grace's and rosie's 1st anniversary of a visit of your show .  how\"}"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"small_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:48:20.562916Z","iopub.execute_input":"2025-12-18T13:48:20.563448Z","iopub.status.idle":"2025-12-18T13:48:20.571266Z","shell.execute_reply.started":"2025-12-18T13:48:20.563419Z","shell.execute_reply":"2025-12-18T13:48:20.570612Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Sentence', 'English_Translation', 'num_words', 'num_chars', 'num_sentences', 'translation'],\n        num_rows: 2000\n    })\n    validation: Dataset({\n        features: ['Sentence', 'English_Translation', 'num_words', 'num_chars', 'num_sentences', 'translation'],\n        num_rows: 150\n    })\n    test: Dataset({\n        features: ['Sentence', 'English_Translation', 'num_words', 'num_chars', 'num_sentences', 'translation'],\n        num_rows: 250\n    })\n})"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"raw_dataset = small_dataset  # As said in Task 2(for 2000 pair of sentence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:48:20.572056Z","iopub.execute_input":"2025-12-18T13:48:20.572412Z","iopub.status.idle":"2025-12-18T13:48:20.581431Z","shell.execute_reply.started":"2025-12-18T13:48:20.572372Z","shell.execute_reply":"2025-12-18T13:48:20.580698Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"# Applying Tokenization:- How we obtain Embedding\n* Attention Mask = Padding Mask x look Ahead Mask\n* Input_ids = input_ids are tokenized text converted into numeric indices from tokenizer vocabulary.\n* The model converts input_ids to embeddings internally through an embedding layer.\n\n# Pipeline\n* Text → Tokens → IDs → Embeddings → Transformer\n* \"I love India\"\n*      ↓              (tokenization)\n* [\"I\",\"love\",\"India\"]\n*      ↓              (vocab lookup)\n* [34, 91, 2563]  ← input_ids\n*      ↓\n* [embedding vectors] ← actual embeddings used by model","metadata":{}},{"cell_type":"code","source":"# sample Example;\ntext = \"Hello Myself Virendra. A final year student at NIT Surat.\"\ntokenizer(\"eng_Latn hin_Deva \" + text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:50:08.558600Z","iopub.execute_input":"2025-12-18T13:50:08.558981Z","iopub.status.idle":"2025-12-18T13:50:08.565340Z","shell.execute_reply.started":"2025-12-18T13:50:08.558952Z","shell.execute_reply":"2025-12-18T13:50:08.564712Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [4, 15, 7951, 23463, 8660, 11258, 5933, 85, 55, 910, 195, 1410, 48, 349, 6601, 8308, 85, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"# Tokenize the Target language(Hinglish) - Example\nwith tokenizer.as_target_tokenizer():\n    print(tokenizer(\"Gud mrng sir aapko Mahashivratri ki hardik mangalkamnaye\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:50:12.402987Z","iopub.execute_input":"2025-12-18T13:50:12.403314Z","iopub.status.idle":"2025-12-18T13:50:12.409593Z","shell.execute_reply.started":"2025-12-18T13:50:12.403287Z","shell.execute_reply":"2025-12-18T13:50:12.408822Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [2513, 36544, 5506, 7881, 49984, 79063, 4586, 25487, 54737, 79673, 61438, 8014, 51561, 14775, 60875, 59614, 32513, 85120, 6473, 81543, 7130, 15937, 5172, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:3860: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":" # Preprocess Fxn for Tokenization:","metadata":{}},{"cell_type":"code","source":"# tags for IndicTrans2 English->Hindi\nSRC_TAG = \"eng_Latn\"\nTGT_TAG = \"hin_Deva\"\n\nmax_input_length = 128\nmax_target_length = 128\nsource_lang = \"en\"\ntarget_lang = \"hi\"\n\ndef preprocess_function(examples):\n\n    inputs = [f\"{SRC_TAG} {TGT_TAG} {ex[source_lang].strip() if ex[source_lang] else ''}\"\n              for ex in examples[\"translation\"]]\n    targets = [ex[target_lang].strip() if ex[target_lang] else \"\" \n               for ex in examples[\"translation\"]]\n\n    # tokenize source (each string already prefixed with tags)\n    model_inputs = tokenizer(\n        inputs,\n        max_length=max_input_length,\n        truncation=True,\n        padding=True,   \n    )\n\n    # Tokenizer for Target lang(Hinglish)\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            [f\"{TGT_TAG} {t}\" if t else f\"{TGT_TAG} \" for t in targets],\n            max_length=max_target_length,\n            truncation=True,\n            padding=True,\n        )\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:50:15.533681Z","iopub.execute_input":"2025-12-18T13:50:15.534307Z","iopub.status.idle":"2025-12-18T13:50:15.540613Z","shell.execute_reply.started":"2025-12-18T13:50:15.534272Z","shell.execute_reply":"2025-12-18T13:50:15.539995Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# Obtaining token of two rows of Train dataset\nprint(preprocess_function(raw_dataset[\"train\"][:2])) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:50:18.884705Z","iopub.execute_input":"2025-12-18T13:50:18.885494Z","iopub.status.idle":"2025-12-18T13:50:18.891967Z","shell.execute_reply.started":"2025-12-18T13:50:18.885459Z","shell.execute_reply":"2025-12-18T13:50:18.891200Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [[1, 1, 1, 1, 4, 15, 5912, 23, 3468, 964, 8063, 31, 22, 6070, 1197, 4363, 14055, 21086, 18902, 4638, 23, 10, 10190, 1567, 4638, 23, 204, 811, 3096, 743, 8, 63, 654, 2], [4, 15, 5912, 27756, 20257, 25260, 15664, 71, 114, 14, 201, 90, 22, 26, 3, 49, 36, 5705, 305, 82, 469, 42, 514, 2962, 17935, 5051, 328, 3204, 947, 876, 1075, 16082, 832, 2]], 'attention_mask': [[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[105948, 59836, 2134, 5172, 43144, 14066, 1869, 48770, 5658, 24159, 3532, 708, 4657, 1772, 9478, 26131, 438, 350, 10241, 17, 8323, 12, 219, 1735, 12, 2153, 12, 1972, 559, 33391, 25137, 9, 44, 5541, 6, 2, 1, 1, 1], [105948, 59836, 2134, 5172, 43144, 14066, 273, 9937, 14066, 97, 6355, 1084, 6355, 888, 44, 51, 8864, 2220, 252, 321, 4286, 170, 100, 42507, 458, 20, 163, 36, 26535, 4443, 13326, 409, 20866, 10762, 495, 25102, 1715, 17709, 2]]}\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"# Setting  hyperparameter Values\nbatch_size = 16\nlearning_rate = 2e-5\nweight_decay = 0.01\nnum_train_epochs = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:50:21.402424Z","iopub.execute_input":"2025-12-18T13:50:21.403056Z","iopub.status.idle":"2025-12-18T13:50:21.406693Z","shell.execute_reply.started":"2025-12-18T13:50:21.403025Z","shell.execute_reply":"2025-12-18T13:50:21.405939Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:50:24.171569Z","iopub.execute_input":"2025-12-18T13:50:24.171922Z","iopub.status.idle":"2025-12-18T13:50:24.176390Z","shell.execute_reply.started":"2025-12-18T13:50:24.171890Z","shell.execute_reply":"2025-12-18T13:50:24.175764Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"# 1) Tokenization of dataset and DataCollator\ntokenized_datasets = raw_dataset.map(preprocess_function, batched=True,\n                                    remove_columns=raw_dataset[\"train\"].column_names)\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")\n\n# 2)train_dataloader and test_dataloader\n\ntrain_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=batch_size,\n                              shuffle=True, collate_fn=data_collator, num_workers=4)\nvalidation_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=batch_size,\n                                   shuffle=False, collate_fn=data_collator, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:50:26.612211Z","iopub.execute_input":"2025-12-18T13:50:26.612513Z","iopub.status.idle":"2025-12-18T13:50:30.867714Z","shell.execute_reply.started":"2025-12-18T13:50:26.612484Z","shell.execute_reply":"2025-12-18T13:50:30.866947Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f5b6f60481d4b6a8ba3837adcc0ab4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"937b1d12957a4dc6aca2a4b53f8b5f26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21ad62b51eb347ac96d85633607d483f"}},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"model.float()        # make sure parameters are float32\nmodel.to(device)\n\n# 3) Recreate optimizer (must be created after model param dtypes are finalized)\noptimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\n# 4) Mixed precision utilities\nscaler = GradScaler()\ngrad_accum = 2\nnum_epochs = max(1, int(num_train_epochs))\n\nallowed_keys = {\"input_ids\", \"attention_mask\", \"labels\", \"decoder_input_ids\", \"decoder_attention_mask\"}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:50:30.868962Z","iopub.execute_input":"2025-12-18T13:50:30.869254Z","iopub.status.idle":"2025-12-18T13:50:30.910745Z","shell.execute_reply.started":"2025-12-18T13:50:30.869227Z","shell.execute_reply":"2025-12-18T13:50:30.910182Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# 5) Training loop\nmodel.train()\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n    running_loss = 0.0\n    steps = 0\n    for batch in tqdm(train_dataloader, desc=\"Training\"):\n        # keep only model inputs and move to device\n        model_batch = {k: v.to(device) for k, v in batch.items() if k in allowed_keys and isinstance(v, torch.Tensor)}\n\n        with autocast(\"cuda\"):                  # activations in fp16, params stay fp32\n            outputs = model(**model_batch)\n            loss = outputs.loss / grad_accum\n\n        scaler.scale(loss).backward()\n\n        if (steps + 1) % grad_accum == 0:\n            # Unscale the Optimizer\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n\n        running_loss += loss.item() * grad_accum\n        steps += 1\n\n    avg_train = running_loss / max(1, steps)\n    print(f\"Train loss: {avg_train:.4f}\")\n\n    # 6) Validation\n    model.eval()\n    vloss = 0.0\n    vsteps = 0\n    with torch.no_grad():\n        for vb in tqdm(validation_dataloader, desc=\"Validation\"):\n            vb_batch = {k: v.to(device) for k, v in vb.items() if k in allowed_keys and isinstance(v, torch.Tensor)}\n            with autocast(\"cuda\"):\n                out = model(**vb_batch)\n            vloss += out.loss.item()\n            vsteps += 1\n    vloss = vloss / max(1, vsteps)\n    print(f\"Validation loss: {vloss:.4f}\")\n    model.train()\n\nprint(\"Training finished.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:50:32.317100Z","iopub.execute_input":"2025-12-18T13:50:32.317618Z","iopub.status.idle":"2025-12-18T13:51:14.043044Z","shell.execute_reply.started":"2025-12-18T13:50:32.317585Z","shell.execute_reply":"2025-12-18T13:51:14.041930Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c80e48da15984a4f882e12fc8650c562"}},"metadata":{}},{"name":"stdout","text":"Train loss: 9.1854\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"747fc3a731fa433fb56a276c5323096d"}},"metadata":{}},{"name":"stdout","text":"Validation loss: 7.9155\nTraining finished.\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"URL_PATTERN = r\"https?://\\S+\"\nHANDLE_PATTERN = r\"@\\w+\"\nTECH_WORDS = [\n    \"ai/ml\",\n    \"ai\",\n    \"ml\",\n    \"artificial intelligence\",\n    \"machine learning\",\n    \"data science\",\n    \"deep learning\"\n]\nSOCIAL_WORDS = [\n    \"really\",\n    \"amazing\",\n    \"awesome\",\n    \"emotional\",\n    \"touching\",\n    \"bhai\",\n    \"and\",\n    \"sir\",\n    \"madam\",\n    \"fan\",\n    \"fans\",\n    \"love\",\n    \"respect\",\n    \"support\"\n]\n\nWORD_PATTERN = r\"\\b(\" + \"|\".join(map(re.escape, TECH_WORDS + SOCIAL_WORDS)) + r\")\\b\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T15:47:27.669588Z","iopub.execute_input":"2025-12-18T15:47:27.669994Z","iopub.status.idle":"2025-12-18T15:47:27.675063Z","shell.execute_reply.started":"2025-12-18T15:47:27.669958Z","shell.execute_reply":"2025-12-18T15:47:27.674173Z"}},"outputs":[],"execution_count":248},{"cell_type":"code","source":"import re\nimport torch\n\n# ----------------------------------\n# Token protection (ROBUST)\n# ----------------------------------\ndef protect_tokens(text):\n    protected = {}\n    idx = 0\n    patterns = [\n    URL_PATTERN,       # URLs\n    HANDLE_PATTERN,    # @handles\n    WORD_PATTERN       # single words (tech + social)\n   ]\n   \n    for pattern in patterns:\n        def repl(match):\n            nonlocal idx\n            key = f\"XQZPLCH{idx}XQZ\"\n            protected[key] = match.group()\n            idx += 1\n            return key\n\n        text = re.sub(pattern, repl, text, flags=re.IGNORECASE)\n\n    return text, protected\n\n\ndef restore_tokens(text, protected):\n    for k, v in protected.items():\n        text = text.replace(k, v)\n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T15:47:29.899844Z","iopub.execute_input":"2025-12-18T15:47:29.900142Z","iopub.status.idle":"2025-12-18T15:47:29.906356Z","shell.execute_reply.started":"2025-12-18T15:47:29.900112Z","shell.execute_reply":"2025-12-18T15:47:29.905662Z"}},"outputs":[],"execution_count":249},{"cell_type":"code","source":"def en_to_hi(\n    text,\n    model,\n    tokenizer,\n    src_tag=\"eng_Latn\",\n    tgt_tag=\"hin_Deva\",\n    max_length=128\n):\n    # Protect non-linguistic / technical tokens\n    safe_text, protected = protect_tokens(text)\n\n    #  Add language tags\n    tagged = f\"{src_tag} {tgt_tag} {safe_text}\"\n\n    #  Tokenize\n    inputs = tokenizer(tagged, return_tensors=\"pt\").to(model.device)\n\n    #  Generate (IndicTrans2-safe settings)\n    with torch.no_grad():\n        out = model.generate(\n            **inputs,\n            max_length=max_length,\n            num_beams=4,       # REQUIRED\n            use_cache=False   # REQUIRED\n        )\n\n    #  Decode Hindi\n    hindi_text = tokenizer.decode(out[0], skip_special_tokens=True)\n\n    # Restore protected tokens\n    hindi_text = restore_tokens(hindi_text, protected)\n\n    return hindi_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T16:01:31.535930Z","iopub.execute_input":"2025-12-18T16:01:31.536331Z","iopub.status.idle":"2025-12-18T16:01:31.542321Z","shell.execute_reply.started":"2025-12-18T16:01:31.536292Z","shell.execute_reply":"2025-12-18T16:01:31.541549Z"}},"outputs":[],"execution_count":274},{"cell_type":"code","source":"text= \"@BeingSalmanKhan brother you are making us emotional. because of  few people you are just leaving all of us.\"\nHindi_text = en_to_hi(text, model, tokenizer)  \nHindi_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T16:01:36.530056Z","iopub.execute_input":"2025-12-18T16:01:36.530389Z","iopub.status.idle":"2025-12-18T16:01:37.880162Z","shell.execute_reply.started":"2025-12-18T16:01:36.530356Z","shell.execute_reply":"2025-12-18T16:01:37.879500Z"}},"outputs":[{"execution_count":275,"output_type":"execute_result","data":{"text/plain":"'@BeingSalmanKhan भाई आप हमें emotional बना रहे हैं । कुछ लोगों की वजह से आप हम सभी को छोड़ रहे हैं ।'"},"metadata":{}}],"execution_count":275},{"cell_type":"code","source":"import re\n\nTECH_WORDS = [\n    \"ai\", \"ml\", \"ai/ml\",\n    \"artificial intelligence\",\n    \"machine learning\",\n    \"emotional\"\n]\n\ndef protect_technical_words(text):\n    protected = {}\n    for i, word in enumerate(TECH_WORDS):\n        token = f\"<TECH{i}>\"\n        pattern = re.compile(re.escape(word), re.IGNORECASE)\n        if pattern.search(text):\n            protected[token] = word\n            text = pattern.sub(token, text)\n    return text, protected\n\ndef restore_technical_words(text, protected):\n    for token, word in protected.items():\n        text = text.replace(token.lower(), word)\n        text = text.replace(token, word) \n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T16:01:41.510499Z","iopub.execute_input":"2025-12-18T16:01:41.510847Z","iopub.status.idle":"2025-12-18T16:01:41.517025Z","shell.execute_reply.started":"2025-12-18T16:01:41.510813Z","shell.execute_reply":"2025-12-18T16:01:41.516266Z"}},"outputs":[],"execution_count":276},{"cell_type":"code","source":"def iast_to_hinglish(text):\n    # --- diacritics ---\n    diacritics = {\n        \"ā\": \"a\",\n        \"ī\": \"i\",\n        \"ū\": \"u\",\n        \"ṃ\": \"n\",\n        \"ṅ\": \"ng\",\n        \"ñ\": \"ny\",\n        \"ṭ\": \"t\",\n        \"ḍ\": \"d\",\n        \"ṇ\": \"n\",\n        \"ś\": \"sh\",\n        \"ṣ\": \"sh\",\n        \"ṛ\": \"r\",\n        \"ḥ\": \"h\",\n    }\n    for k, v in diacritics.items():\n        text = text.replace(k, v)\n\n    # --- Hinglish lexical fixes ---\n    fixes = {\n        \" nama \": \" naam \",\n        \" naama \": \" naam \",\n        \" vartamana \": \" vartaman \",\n        \" vartamaana \": \" vartaman \",\n        \" men \": \" mein \",\n        \" hu~\": \" hoon\",\n        \" hun\": \" hoon\",\n        \" aura \": \" aur \",\n        \" yaha \": \" yeh \",\n        \" bahuta \": \" bahut \",\n        \" anubhava \": \" anubhav \",\n        \"|\": \".\",\n    }\n    for k, v in fixes.items():\n        text = text.replace(k, v)\n\n    return text.lower().strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T16:01:45.048215Z","iopub.execute_input":"2025-12-18T16:01:45.048528Z","iopub.status.idle":"2025-12-18T16:01:45.054469Z","shell.execute_reply.started":"2025-12-18T16:01:45.048496Z","shell.execute_reply":"2025-12-18T16:01:45.053684Z"}},"outputs":[],"execution_count":277},{"cell_type":"code","source":"!pip install indic-transliteration","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T16:01:49.483030Z","iopub.execute_input":"2025-12-18T16:01:49.483388Z","iopub.status.idle":"2025-12-18T16:01:53.185873Z","shell.execute_reply.started":"2025-12-18T16:01:49.483353Z","shell.execute_reply":"2025-12-18T16:01:53.185071Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: indic-transliteration in /usr/local/lib/python3.12/dist-packages (2.3.75)\nRequirement already satisfied: backports.functools-lru-cache in /usr/local/lib/python3.12/dist-packages (from indic-transliteration) (2.0.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from indic-transliteration) (2025.11.3)\nRequirement already satisfied: typer in /usr/local/lib/python3.12/dist-packages (from indic-transliteration) (0.20.0)\nRequirement already satisfied: toml in /usr/local/lib/python3.12/dist-packages (from indic-transliteration) (0.10.2)\nRequirement already satisfied: roman in /usr/local/lib/python3.12/dist-packages (from indic-transliteration) (5.2)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer->indic-transliteration) (8.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from typer->indic-transliteration) (4.15.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer->indic-transliteration) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer->indic-transliteration) (14.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer->indic-transliteration) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer->indic-transliteration) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer->indic-transliteration) (0.1.2)\n","output_type":"stream"}],"execution_count":278},{"cell_type":"code","source":"from indic_transliteration import sanscript\nfrom indic_transliteration.sanscript import transliterate\n\ndef hindi_to_hinglish(hindi_text):\n    # protect technical terms\n    safe_text, protected = protect_technical_words(hindi_text)\n\n    # Hindi → IAST\n    iast = transliterate(safe_text, sanscript.DEVANAGARI, sanscript.IAST)\n\n    # IAST → Hinglish\n    hinglish = iast_to_hinglish(iast)\n\n    # restore technical terms\n    hinglish = restore_technical_words(hinglish, protected)\n\n    return hinglish\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T16:01:53.187806Z","iopub.execute_input":"2025-12-18T16:01:53.188121Z","iopub.status.idle":"2025-12-18T16:01:53.193928Z","shell.execute_reply.started":"2025-12-18T16:01:53.188088Z","shell.execute_reply":"2025-12-18T16:01:53.193048Z"}},"outputs":[],"execution_count":279},{"cell_type":"code","source":"print(hindi_to_hinglish(Hindi_text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T16:01:54.245290Z","iopub.execute_input":"2025-12-18T16:01:54.245624Z","iopub.status.idle":"2025-12-18T16:01:54.251536Z","shell.execute_reply.started":"2025-12-18T16:01:54.245591Z","shell.execute_reply":"2025-12-18T16:01:54.250711Z"}},"outputs":[{"name":"stdout","text":"@beingsalmankhan bhai apa hamen emotional bana rahe hain . kucha logon ki vajaha se apa hama sabhi ko chor̤a rahe hain .\n","output_type":"stream"}],"execution_count":280},{"cell_type":"markdown","source":"# Evaluation metrics","metadata":{}},{"cell_type":"markdown","source":"# Finding  BLEU AND CHRF:\n\n1. BLEU: BLEU checks how many n-grams from the candidate sentence also appear in the reference sentence.\n2. CHRF: Instead of words, CHRF compares character n-grams.","metadata":{}},{"cell_type":"code","source":"model.device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T16:02:05.854223Z","iopub.execute_input":"2025-12-18T16:02:05.854535Z","iopub.status.idle":"2025-12-18T16:02:05.859803Z","shell.execute_reply.started":"2025-12-18T16:02:05.854504Z","shell.execute_reply":"2025-12-18T16:02:05.859087Z"}},"outputs":[{"execution_count":281,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}],"execution_count":281},{"cell_type":"code","source":"len(raw_dataset[\"test\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T16:02:10.439137Z","iopub.execute_input":"2025-12-18T16:02:10.439939Z","iopub.status.idle":"2025-12-18T16:02:10.445019Z","shell.execute_reply.started":"2025-12-18T16:02:10.439903Z","shell.execute_reply":"2025-12-18T16:02:10.444431Z"}},"outputs":[{"execution_count":282,"output_type":"execute_result","data":{"text/plain":"250"},"metadata":{}}],"execution_count":282},{"cell_type":"code","source":"n_samples = 10\n\n# determine model device safely\ntry:\n    model_device = next(model.parameters()).device\nexcept StopIteration:\n    model_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"Model device:\", model_device)\n\npreds = []\nrefs = []\n\nfor i in range(min(n_samples, len(raw_dataset[\"test\"]))):\n    row = raw_dataset[\"test\"][i]\n\n    # handle multiple possible row formats\n    if isinstance(row, dict) and \"translation\" in row:\n        trans = row[\"translation\"]\n        # trans might be a dict or a string; handle both\n        if isinstance(trans, dict):\n            eng = trans.get(\"en\") or trans.get(\"eng\") or \"\"\n            hi_ref = trans.get(\"hing\") or trans.get(\"hin\") or \"\"\n        else:\n            # sometimes translation is a string (rare) — treat as source\n            eng = str(trans)\n            hi_ref = \"\"\n    elif isinstance(row, dict):\n        # maybe keys are directly 'en' and 'hi'\n        eng = row.get(\"en\") or row.get(\"eng\") or row.get(\"source\") or \"\"\n        hi_ref = row.get(\"hing\") or row.get(\"hin\") or row.get(\"target\") or \"\"\n    else:\n        # fallback: row itself might be the translation dict-like\n        try:\n            eng = row[\"en\"]\n            hi_ref = row[\"hing\"]\n        except Exception:\n            # last resort: stringify\n            eng = str(row)\n            hi_ref = \"\"\n\n    eng = (eng or \"\").strip()\n    hi_ref = (hi_ref or \"\").strip()\n    refs.append(hi_ref if hi_ref else \"\")  # keep alignment\n    safe_text, protected = protect_tokens(eng)\n    # add required tags\n    tagged = f\"{SRC_TAG} {TGT_TAG} {eng}\"\n\n    # tokenize -> torch tensors -> move to model device\n    tokenized = tokenizer(tagged, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n    tokenized = {k: v.to(model_device) for k, v in tokenized.items()}\n\n    # generate\n    with torch.no_grad():\n        out_ids = model.generate(\n           **tokenized,\n            max_length=128,\n            num_beams=4,        \n            use_cache=False,  \n            early_stopping=True\n        )\n    \n    Hindi_text = en_to_hi(eng, model, tokenizer)    \n\n    hinglish = hindi_to_hinglish(Hindi_text)\n    preds.append(hinglish)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T16:05:27.128398Z","iopub.execute_input":"2025-12-18T16:05:27.128692Z"}},"outputs":[{"name":"stdout","text":"Model device: cuda:0\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"preds[0:4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T16:02:37.917380Z","iopub.execute_input":"2025-12-18T16:02:37.917669Z","iopub.status.idle":"2025-12-18T16:02:37.922830Z","shell.execute_reply.started":"2025-12-18T16:02:37.917641Z","shell.execute_reply":"2025-12-18T16:02:37.922106Z"}},"outputs":[{"execution_count":284,"output_type":"execute_result","data":{"text/plain":"['@beingsalmankhan bhai apa hamen emotional bana rahe hain . kucha logon ke karana apa hama sabhi ko chor̤a rahe hain .',\n 'really touching and amazing. mujhe bacapana ki yada dilaya .',\n '@0__1 unake hatha and unake samana jo bhi ve karate hain vaha una para nirbhara karata hai .',\n '@trisha_naik elpha ne mahana rshiyon ka dhyana bhatakaya hai .']"},"metadata":{}}],"execution_count":284},{"cell_type":"code","source":"len(preds)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T16:02:37.923752Z","iopub.execute_input":"2025-12-18T16:02:37.924041Z","iopub.status.idle":"2025-12-18T16:02:37.937513Z","shell.execute_reply.started":"2025-12-18T16:02:37.924013Z","shell.execute_reply":"2025-12-18T16:02:37.936843Z"}},"outputs":[{"execution_count":285,"output_type":"execute_result","data":{"text/plain":"10"},"metadata":{}}],"execution_count":285},{"cell_type":"code","source":"#BLEU:\nbleu = sacrebleu.corpus_bleu(preds, [refs])\n#CHRF:\nchrf = sacrebleu.corpus_chrf(preds, [refs])\n\nprint(f\"\\nEvaluated {len(preds)} samples\")\nprint(\"BLEU:\", bleu.score)\nprint(\"CHRF:\", chrf.score)\n\nfor i in range(min(5, len(preds))):\n    print(f\"\\n=== SAMPLE {i+1} ===\")\n    print(\"SRC :\", (raw_dataset[\"test\"][i].get(\"translation\", raw_dataset[\"test\"][i]).get(\"en\")\n                    if isinstance(raw_dataset[\"test\"][i], dict) and \"translation\" in raw_dataset[\"test\"][i]\n                    else (raw_dataset[\"test\"][i].get(\"en\") if isinstance(raw_dataset[\"test\"][i], dict) else str(raw_dataset[\"test\"][i]))))\n    print(\"PRED:\", preds[i])\n    print(\"REF :\", refs[i])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T16:03:08.557854Z","iopub.execute_input":"2025-12-18T16:03:08.558229Z","iopub.status.idle":"2025-12-18T16:03:08.573318Z","shell.execute_reply.started":"2025-12-18T16:03:08.558195Z","shell.execute_reply":"2025-12-18T16:03:08.572494Z"}},"outputs":[{"name":"stdout","text":"\nEvaluated 10 samples\nBLEU: 11.522239129499384\nCHRF: 33.45488174566334\n\n=== SAMPLE 1 ===\nSRC : @BeingSalmanKhan brother you are making us emotional. because of  few people you are just leaving all of us.\nPRED: @beingsalmankhan bhai apa hamen emotional bana rahe hain . kucha logon ke karana apa hama sabhi ko chor̤a rahe hain .\nREF : @BeingSalmanKhan bhai ab aap emotional kar rahe ho. Kuch logo ki wajah se sabko chhod ke chale jaoge !\n\n=== SAMPLE 2 ===\nSRC : really touching and amazing. reminded me of my childhood.\nPRED: really touching and amazing. mujhe bacapana ki yada dilaya .\nREF : really touching and amazing .  .  bachpan ki yaad aa gayi .  .  .\n\n=== SAMPLE 3 ===\nSRC : @0__1 their hand and their luggage, it is upto them whatever they do.\nPRED: @0__1 unake hatha and unake samana jo bhi ve karate hain vaha una para nirbhara karata hai .\nREF : @0__1 unka haath, aur unka samaan jo chahe kare\n\n=== SAMPLE 4 ===\nSRC : @trisha_naik  Elf  has distracted the great sages.\nPRED: @trisha_naik elpha ne mahana rshiyon ka dhyana bhatakaya hai .\nREF : @trisha_naik Apsaraye to bade bade rishi muniyo ka dhyaan bhang kar chuki hai\n\n=== SAMPLE 5 ===\nSRC : rajdeep, sagarika, rana ayyub, barkha and ravish https://twitter.com/dhaikilokatweet/status/653972513249169409\nPRED: rajadipa sagarika rena ayuba barakha and ravisha https://twitter.com/dhaikilokatweet/status/653972513249169409\nREF : Rajdeep, Sagarika, Rana Ayyub, Barkha and Ravishhttps://twitter.com/dhaikilokatweet/status/653972513249169409 …\n","output_type":"stream"}],"execution_count":287},{"cell_type":"markdown","source":"# Finding BERTScore:\n\n* BERTScore: Uses BERT (or RoBERTa, or mBERT) embeddings to compare every token in candidate with every token in reference.","metadata":{}},{"cell_type":"code","source":"!pip install bert-score\nfrom bert_score import score\n\nP, R, F1 = score(preds, refs, lang=\"eng\")\nprint(\"BERTScore F1:\", F1.mean().item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T16:03:14.395949Z","iopub.execute_input":"2025-12-18T16:03:14.396264Z","iopub.status.idle":"2025-12-18T16:03:19.434453Z","shell.execute_reply.started":"2025-12-18T16:03:14.396231Z","shell.execute_reply":"2025-12-18T16:03:19.433674Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bert-score in /usr/local/lib/python3.12/dist-packages (0.3.13)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.8.0+cu126)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.1.4)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.36.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from bert-score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.5)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2023.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.4.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (2025.11.3)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.15.2)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2025.11.12)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=3.0.0->bert-score) (1.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"BERTScore F1: 0.7211230993270874\n","output_type":"stream"}],"execution_count":288},{"cell_type":"markdown","source":"# Finding BLEURT:\n* BLEURT computes a similarity score using a fine-tuned BERT model that predicts human judgment of translation quality.","metadata":{}},{"cell_type":"code","source":"# Required Packages for Bleurt\n!pip install evaluate\n!pip install git+https://github.com/google-research/bleurt.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T15:55:14.635953Z","iopub.execute_input":"2025-12-18T15:55:14.636971Z","iopub.status.idle":"2025-12-18T15:55:24.618639Z","shell.execute_reply.started":"2025-12-18T15:55:14.636926Z","shell.execute_reply":"2025-12-18T15:55:24.617554Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.16.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.5)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.1)\nRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (0.7)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.13.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.3)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nCollecting git+https://github.com/google-research/bleurt.git\n  Cloning https://github.com/google-research/bleurt.git to /tmp/pip-req-build-1b80ub54\n  Running command git clone --filter=blob:none --quiet https://github.com/google-research/bleurt.git /tmp/pip-req-build-1b80ub54\n  Resolved https://github.com/google-research/bleurt.git to commit cebe7e6f996b40910cfaa520a63db47807e3bf5c\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from BLEURT==0.0.2) (2.1.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from BLEURT==0.0.2) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from BLEURT==0.0.2) (1.11.4)\nRequirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (from BLEURT==0.0.2) (2.19.0)\nRequirement already satisfied: tf-slim>=1.1 in /usr/local/lib/python3.12/dist-packages (from BLEURT==0.0.2) (1.1.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from BLEURT==0.0.2) (0.2.1)\nRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from tf-slim>=1.1->BLEURT==0.0.2) (1.4.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->BLEURT==0.0.2) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->BLEURT==0.0.2) (2025.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.12/dist-packages (from pandas->BLEURT==0.0.2) (2025.3)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->BLEURT==0.0.2) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow->BLEURT==0.0.2) (25.9.23)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow->BLEURT==0.0.2) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow->BLEURT==0.0.2) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->BLEURT==0.0.2) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow->BLEURT==0.0.2) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow->BLEURT==0.0.2) (25.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow->BLEURT==0.0.2) (4.25.8)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->BLEURT==0.0.2) (2.32.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow->BLEURT==0.0.2) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->BLEURT==0.0.2) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->BLEURT==0.0.2) (3.1.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow->BLEURT==0.0.2) (4.15.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->BLEURT==0.0.2) (2.0.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow->BLEURT==0.0.2) (1.75.1)\nRequirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->BLEURT==0.0.2) (2.19.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->BLEURT==0.0.2) (3.10.0)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->BLEURT==0.0.2) (3.15.1)\nRequirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow->BLEURT==0.0.2) (0.5.3)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow->BLEURT==0.0.2) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow->BLEURT==0.0.2) (14.2.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow->BLEURT==0.0.2) (0.1.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow->BLEURT==0.0.2) (0.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (2025.11.12)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow->BLEURT==0.0.2) (3.9)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow->BLEURT==0.0.2) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow->BLEURT==0.0.2) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow->BLEURT==0.0.2) (3.0.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow->BLEURT==0.0.2) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow->BLEURT==0.0.2) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow->BLEURT==0.0.2) (0.1.2)\n","output_type":"stream"}],"execution_count":263},{"cell_type":"code","source":"# Calculating Bleurt\nimport evaluate\nbleurt = evaluate.load(\"bleurt\")\nresults = bleurt.compute(predictions=preds, references=refs)\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T14:28:11.332692Z","iopub.execute_input":"2025-12-18T14:28:11.333275Z","iopub.status.idle":"2025-12-18T14:28:53.819591Z","shell.execute_reply.started":"2025-12-18T14:28:11.333243Z","shell.execute_reply":"2025-12-18T14:28:53.818725Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b92a183b88104608b5aee52af34ea782"}},"metadata":{}},{"name":"stderr","text":"Using default checkpoint 'bleurt-base-128' for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', config_name='bleurt-large-512').\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/405M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9de0c12df84641bf8519609079030706"}},"metadata":{}},{"name":"stdout","text":"INFO:tensorflow:Reading checkpoint /root/.cache/huggingface/metrics/bleurt/default/downloads/extracted/887f2dc36c17f53c287f696681b8f7c947278407c1cf9f226662e16c8c0dc417/bleurt-base-128.\nINFO:tensorflow:Config file found, reading.\nINFO:tensorflow:Will load checkpoint bert_custom\nINFO:tensorflow:Loads full paths and checks that files exists.\nINFO:tensorflow:... name:bert_custom\nINFO:tensorflow:... vocab_file:vocab.txt\nINFO:tensorflow:... bert_config_file:bert_config.json\nINFO:tensorflow:... do_lower_case:True\nINFO:tensorflow:... max_seq_length:128\nINFO:tensorflow:Creating BLEURT scorer.\nINFO:tensorflow:Creating WordPiece tokenizer.\nINFO:tensorflow:WordPiece tokenizer instantiated.\nINFO:tensorflow:Creating Eager Mode predictor.\nINFO:tensorflow:Loading model.\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1766068126.935431     222 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2346 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1766068126.939369     222 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:BLEURT initialized.\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:BLEURT initialized.\n","output_type":"stream"},{"name":"stdout","text":"{'scores': [-0.3804433047771454, -0.5875755548477173, 0.025112386792898178, -0.1274239718914032, -0.41122981905937195, -0.13586753606796265, -1.0298868417739868, -0.8302490711212158, -1.600035548210144, -0.4865165948867798, -0.43631744384765625, -0.0038480497896671295, -0.6158275008201599, -0.7650679349899292, -0.4761647880077362, -0.906033456325531, -0.5395939946174622, -0.5018869042396545, -1.2332695722579956, -0.7155565023422241, -0.641909658908844, -1.3033435344696045, -0.48062774538993835, -1.6419179439544678, -0.8180795311927795, -1.4687597751617432, -0.935701847076416, -0.7306153178215027, -0.4692401587963104, -0.060972604900598526, -0.7518864870071411, -0.7861565947532654, -0.06227077916264534, -0.45538830757141113, -1.8166522979736328, -0.45611000061035156, -0.527264416217804, -0.5909863114356995, -1.8587026596069336, -1.099261999130249, -0.6316080689430237, -0.8847617506980896, -0.017818201333284378, -0.31022441387176514, -1.09712815284729, -0.3065151572227478, -1.4421381950378418, -0.02796507254242897, -0.5348250269889832, -0.9223517775535583, 0.0037236176431179047, -0.9232638478279114, -0.46818745136260986, -0.28701382875442505, -0.6023994088172913, -1.2056549787521362, -1.2501721382141113, -1.338416337966919, -0.5424264073371887, -0.10962490737438202, -1.1886839866638184, -0.1862114667892456, -0.6478641629219055, -0.7442018389701843, 0.0033666156232357025, -0.8176786303520203, -1.2916544675827026, -0.21965397894382477, -0.71455317735672, -0.9402922987937927, -0.33060315251350403, -0.4384707510471344, -0.2728809416294098, -0.7948885560035706, -0.8064823746681213, -0.8377973437309265, 0.16124410927295685, -0.28695663809776306, -0.6920647025108337, -0.25233933329582214, -0.5441645979881287, -0.5480700731277466, -0.1265263855457306, -1.766023874282837, -0.6718705296516418, -0.627566397190094, -1.4564523696899414, -1.971604347229004, -0.9513941407203674, -0.42572084069252014, -0.5799024105072021, -0.7010412812232971, -0.380761057138443, -0.16884152591228485, -0.6922271847724915, -0.13463999330997467, -0.5734808444976807, 0.1800736039876938, -0.3217548727989197, -0.48713141679763794]}\n","output_type":"stream"}],"execution_count":141},{"cell_type":"markdown","source":"# Finding COMET\n* COMET predicts a score that strongly correlates with human judgment.","metadata":{}},{"cell_type":"code","source":"# Required package for Comet\n!pip install -q unbabel-comet\nfrom comet import download_model, load_from_checkpoint\n\n\n# choose model variable \ntranslation_model = globals().get(\"translation_model\", None) or globals().get(\"model\", None)\nif translation_model is None:\n    raise ValueError(\"No translation model found. Load your model into `model` or `translation_model` first.\")\n\n# device: try to get model device (handles DeviceMap too)\ntry:\n    model_device = next(translation_model.parameters()).device\nexcept StopIteration:\n    model_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nSRC_TAG = \"eng_Latn\"\nTGT_TAG = \"hin_Deva\"\n\nsrcs = []\npreds = []\nrefs = []\n\nn = 10\nfor i in range(min(n, len(raw_dataset[\"test\"]))):\n    row = raw_dataset[\"test\"][i]\n\n    # robust extraction of source & reference\n    if isinstance(row, dict) and \"translation\" in row:\n        trans = row[\"translation\"]\n        if isinstance(trans, dict):\n            src = (trans.get(\"en\") or trans.get(\"eng\") or \"\").strip()\n            ref = (trans.get(\"hing\") or trans.get(\"hing\") or \"\").strip()\n        else:\n            src = str(trans).strip()\n            ref = \"\"\n    elif isinstance(row, dict):\n        src = (row.get(\"en\") or row.get(\"eng\") or row.get(\"source\") or \"\").strip()\n        ref = (row.get(\"hing\") or row.get(\"hing\") or row.get(\"target\") or \"\").strip()\n    else:\n        # fallback\n        src = str(row).strip()\n        ref = \"\"\n\n    srcs.append(src)\n    refs.append(ref)\n\n    # add language tags required by IndicTrans2\n    tagged = f\"{SRC_TAG} {TGT_TAG} {src}\"\n\n    # tokenize -> PyTorch tensors -> move to model device\n    tokenized = tokenizer(tagged,\n                          return_tensors=\"pt\",\n                          truncation=True,\n                          padding=True,\n                          max_length=128)\n    tokenized = {k: v.to(model_device) for k, v in tokenized.items()}\n\n    # generate\n    with torch.no_grad():\n        out_ids = translation_model.generate(**tokenized, max_length=128, num_beams=4, early_stopping=True)\n\n    Hindi_text = en_to_hi(eng, model, tokenizer)    \n\n    hinglish = hindi_to_hinglish(Hindi_text)\n    preds.append(hinglish)\n\n# ---- COMET evaluation ----\nmodel_path = download_model(\"Unbabel/wmt22-comet-da\")\ncomet_model = load_from_checkpoint(model_path)\n\n# prepare data list for COMET\ndata = [{\"src\": s, \"mt\": p, \"ref\": r} for s, p, r in zip(srcs, preds, refs)]\n# note: comet_model.predict returns a dict; 'scores' contains numeric values\ncomet_out = comet_model.predict(data, batch_size=8)\ncomet_scores = comet_out[\"scores\"] if isinstance(comet_out, dict) and \"scores\" in comet_out else comet_out\n\nprint(\"Samples evaluated:\", len(preds))\nprint(\"Mean COMET score:\", float(sum(comet_scores) / len(comet_scores)))\n\n# preview\nfor i in range(5):\n    print(f\"\\n--- SAMPLE {i+1} ---\")\n    print(\"SRC :\", srcs[i])\n    print(\"PRED:\", preds[i])\n    print(\"REF :\", refs[i])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T16:04:12.252713Z","iopub.execute_input":"2025-12-18T16:04:12.253628Z","iopub.status.idle":"2025-12-18T16:04:47.274307Z","shell.execute_reply.started":"2025-12-18T16:04:12.253584Z","shell.execute_reply":"2025-12-18T16:04:47.273417Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"980c1c22da764c928b1250deb1e8c666"}},"metadata":{}},{"name":"stderr","text":"INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../root/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n/usr/local/lib/python3.12/dist-packages/pytorch_lightning/core/saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\nINFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\nINFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\nPredicting DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  6.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Samples evaluated: 10\nMean COMET score: 0.45171141922473906\n\n--- SAMPLE 1 ---\nSRC : @BeingSalmanKhan brother you are making us emotional. because of  few people you are just leaving all of us.\nPRED: ghrnita apa isa taraha se bata karenge to apake satha kauna baithega and biyara pi lo\nREF : @BeingSalmanKhan bhai ab aap emotional kar rahe ho. Kuch logo ki wajah se sabko chhod ke chale jaoge !\n\n--- SAMPLE 2 ---\nSRC : really touching and amazing. reminded me of my childhood.\nPRED: ghrnita apa isa taraha se bata karenge to kauna apake satha baithega and biyara pi lo\nREF : really touching and amazing .  .  bachpan ki yaad aa gayi .  .  .\n\n--- SAMPLE 3 ---\nSRC : @0__1 their hand and their luggage, it is upto them whatever they do.\nPRED: ghrnapurna. apa isa taraha se bata karenge. to kauna apake satha baithega and biyara pi lo.\nREF : @0__1 unka haath, aur unka samaan jo chahe kare\n\n--- SAMPLE 4 ---\nSRC : @trisha_naik  Elf  has distracted the great sages.\nPRED: ghrnita apa isa taraha se bata karenge to apake satha kauna baithega and biyara pi lo\nREF : @trisha_naik Apsaraye to bade bade rishi muniyo ka dhyaan bhang kar chuki hai\n\n--- SAMPLE 5 ---\nSRC : rajdeep, sagarika, rana ayyub, barkha and ravish https://twitter.com/dhaikilokatweet/status/653972513249169409\nPRED: ghrnaspada apa isa taraha bata karenge to kauna apake satha baithega and sharaba pite hain\nREF : Rajdeep, Sagarika, Rana Ayyub, Barkha and Ravishhttps://twitter.com/dhaikilokatweet/status/653972513249169409 …\n","output_type":"stream"}],"execution_count":289},{"cell_type":"code","source":"# Saving the Model and tokenizer\nmodel.save_pretrained(\"pt_model\")\ntokenizer.save_pretrained(\"tokenizer\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T16:04:47.275970Z","iopub.execute_input":"2025-12-18T16:04:47.276307Z","iopub.status.idle":"2025-12-18T16:04:49.462428Z","shell.execute_reply.started":"2025-12-18T16:04:47.276274Z","shell.execute_reply":"2025-12-18T16:04:49.461777Z"}},"outputs":[{"execution_count":290,"output_type":"execute_result","data":{"text/plain":"('tokenizer/tokenizer_config.json',\n 'tokenizer/special_tokens_map.json',\n 'tokenizer/dict.SRC.json',\n 'tokenizer/dict.TGT.json',\n 'tokenizer/model.SRC',\n 'tokenizer/model.TGT',\n 'tokenizer/added_tokens.json')"},"metadata":{}}],"execution_count":290}]}