{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================\n# Setup: Install and Import Libraries\n# ================================\n!pip install -q peft\n\nimport os\nimport torch\nfrom datasets import load_dataset, DatasetDict\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\nfrom torch.utils.data import DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tqdm.auto import tqdm\nfrom torch.optim import AdamW\n# Evaluation metrics\n!pip install -q sacrebleu bert-score evaluate unbabel-comet\n\nimport sacrebleu\nfrom bert_score import score as bert_score\nimport evaluate\nfrom comet import download_model, load_from_checkpoint\n\n\n# PEFT (LoRA)\nfrom peft import LoraConfig, get_peft_model, TaskType\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:13:36.062826Z","iopub.execute_input":"2025-12-19T14:13:36.063494Z","iopub.status.idle":"2025-12-19T14:13:42.996237Z","shell.execute_reply.started":"2025-12-19T14:13:36.063458Z","shell.execute_reply":"2025-12-19T14:13:42.995179Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Enter Access Token and rerun\nfrom huggingface_hub import login\nlogin(new_session=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:13:42.998152Z","iopub.execute_input":"2025-12-19T14:13:42.998963Z","iopub.status.idle":"2025-12-19T14:13:43.004545Z","shell.execute_reply.started":"2025-12-19T14:13:42.998888Z","shell.execute_reply":"2025-12-19T14:13:43.003636Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ================================\n# Load Base Model and Tokenizer\n# ================================\nckpt = \"ai4bharat/indictrans2-en-indic-1B\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(ckpt, trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(ckpt)\n\n# Use GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nprint(\"Model loaded on\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:13:43.005559Z","iopub.execute_input":"2025-12-19T14:13:43.005995Z","iopub.status.idle":"2025-12-19T14:13:48.720012Z","shell.execute_reply.started":"2025-12-19T14:13:43.005934Z","shell.execute_reply":"2025-12-19T14:13:48.719290Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"The repository ai4bharat/indictrans2-en-indic-1B contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/ai4bharat/indictrans2-en-indic-1B .\n You can inspect the repository content at https://hf.co/ai4bharat/indictrans2-en-indic-1B.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"name":"stdout","text":"Model loaded on cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ================================\n# Prepare Domain-Specific Data\n# ================================\n\nraw_data = load_dataset(\"LingoIITGN/PHINC\")\ndef convert_to_translation(example):\n    return {\n        \"translation\": {\n            \"en\": example[\"English_Translation\"],\n            \"hing\": example[\"Sentence\"]\n        }\n    }\n\nraw_data = raw_data.map(convert_to_translation)\n\nraw_data = raw_data[\"train\"]  # use train split\n\n# 300 statements --> 10000 is taking too long and my net speed is not high(reason)\ndomain_train = raw_data.shuffle(seed=42).select(range(300))\ndomain_val   = raw_data.shuffle(seed=42).select(range(100, 150))  # small dev set\n\nprint(domain_train[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:27:04.216512Z","iopub.execute_input":"2025-12-19T14:27:04.217113Z","iopub.status.idle":"2025-12-19T14:27:05.579936Z","shell.execute_reply.started":"2025-12-19T14:27:04.217076Z","shell.execute_reply":"2025-12-19T14:27:05.579303Z"}},"outputs":[{"name":"stdout","text":"{'Sentence': \"@205HiDeeps arey Ashuuu sir ki khaas ' Twamily ' ki member kyu ni baat krungi yar ! Kamaal krte ho ap b\", 'English_Translation': \"@205HiDeeps oh shishu sir's special 'twamily' member, why will I not talk! you too do amazing\", 'translation': {'en': \"@205HiDeeps oh shishu sir's special 'twamily' member, why will I not talk! you too do amazing\", 'hing': \"@205HiDeeps arey Ashuuu sir ki khaas ' Twamily ' ki member kyu ni baat krungi yar ! Kamaal krte ho ap b\"}}\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"raw_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:27:15.590294Z","iopub.execute_input":"2025-12-19T14:27:15.591041Z","iopub.status.idle":"2025-12-19T14:27:15.595602Z","shell.execute_reply.started":"2025-12-19T14:27:15.591008Z","shell.execute_reply":"2025-12-19T14:27:15.594925Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Sentence', 'English_Translation', 'translation'],\n    num_rows: 13738\n})"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"import torch\nfrom datasets import Dataset\n\n# -------------------------------\n# Language tags\n# -------------------------------\nSRC_TAG = \"eng_Latn\"     # Hinglish (Latin)\nTGT_TAG = \"hin_Deva\"     # Hindi (Devanagari)\n\nMAX_LEN = 128\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:27:18.331071Z","iopub.execute_input":"2025-12-19T14:27:18.331672Z","iopub.status.idle":"2025-12-19T14:27:18.346690Z","shell.execute_reply.started":"2025-12-19T14:27:18.331641Z","shell.execute_reply":"2025-12-19T14:27:18.345867Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"IndicTransForConditionalGeneration(\n  (model): IndicTransModel(\n    (encoder): IndicTransEncoder(\n      (embed_tokens): Embedding(32322, 1024, padding_idx=1)\n      (embed_positions): IndicTransSinusoidalPositionalEmbedding()\n      (layers): ModuleList(\n        (0-17): 18 x IndicTransEncoderLayer(\n          (self_attn): IndicTransAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=8192, bias=True)\n          (fc2): Linear(in_features=8192, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): IndicTransDecoder(\n      (embed_tokens): Embedding(122672, 1024, padding_idx=1)\n      (embed_positions): IndicTransSinusoidalPositionalEmbedding()\n      (layers): ModuleList(\n        (0-17): 18 x IndicTransDecoderLayer(\n          (self_attn): IndicTransAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): IndicTransAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1024, out_features=8192, bias=True)\n          (fc2): Linear(in_features=8192, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=1024, out_features=122672, bias=False)\n)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"import re\nimport torch\n\n# ----------------------------------\n# Patterns\n# ----------------------------------\nURL_PATTERN = r\"https?://\\S+\"\nHANDLE_PATTERN = r\"@\\w+\"\n\nTECH_WORDS = [\n    \"ai/ml\", \"ai\", \"ml\",\n    \"artificial intelligence\",\n    \"machine learning\",\n    \"data science\",\n    \"deep learning\"\n]\n\nSOCIAL_WORDS = [\n    \"really\", \"amazing\", \"awesome\", \"emotional\",\n    \"touching\", \"bhai\", \"and\", \"sir\", \"madam\",\n    \"fan\", \"fans\", \"love\", \"respect\", \"support\"\n]\n\nWORD_PATTERN = r\"\\b(\" + \"|\".join(map(re.escape, TECH_WORDS + SOCIAL_WORDS)) + r\")\\b\"\n\n\n# ----------------------------------\n# Token protection\n# ----------------------------------\ndef protect_tokens(text):\n    protected = {}\n    idx = 0\n\n    patterns = [\n        URL_PATTERN,      # URLs\n        HANDLE_PATTERN,   # @handles\n        WORD_PATTERN      # tech + social words\n    ]\n\n    for pattern in patterns:\n        def repl(match):\n            nonlocal idx\n            key = f\"XQZPLCH{idx}XQZ\"\n            protected[key] = match.group()\n            idx += 1\n            return key\n\n        text = re.sub(pattern, repl, text, flags=re.IGNORECASE)\n\n    return text, protected\n\n\ndef restore_tokens(text, protected):\n    for k, v in protected.items():\n        text = text.replace(k, v)\n    return text\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:27:21.564920Z","iopub.execute_input":"2025-12-19T14:27:21.565703Z","iopub.status.idle":"2025-12-19T14:27:21.572043Z","shell.execute_reply.started":"2025-12-19T14:27:21.565667Z","shell.execute_reply":"2025-12-19T14:27:21.571301Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def hinglish_to_hindi_batch(\n    sentences,\n    model,\n    tokenizer,\n    batch_size=32,\n    src_tag=\"eng_Latn\",\n    tgt_tag=\"hin_Deva\",\n    max_length=96\n):\n    hindi_outputs = []\n\n    for i in range(0, len(sentences), batch_size):\n        batch = sentences[i:i + batch_size]\n\n        # Protect tokens (sentence-wise)\n        safe_batch = []\n        protected_maps = []\n\n        for s in batch:\n            safe_text, protected = protect_tokens(s)\n            safe_batch.append(f\"{src_tag} {tgt_tag} {safe_text}\")\n            protected_maps.append(protected)\n\n        #  Tokenize batch\n        inputs = tokenizer(\n            safe_batch,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=max_length\n        ).to(model.device)\n\n        # Generate translations\n        with torch.no_grad():\n            output_ids = model.generate(\n                **inputs,\n                max_length=max_length,\n                num_beams=1,        # IndicTrans2 stable\n                use_cache=False,   # REQUIRED\n                early_stopping=True\n            )\n\n        # Decode batch\n        decoded = tokenizer.batch_decode(\n            output_ids,\n            skip_special_tokens=True\n        )\n\n        #  Restore protected tokens (sentence-wise)\n        for text, protected in zip(decoded, protected_maps):\n            restored = restore_tokens(text, protected)\n            hindi_outputs.append(restored)\n\n    return hindi_outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:27:48.098453Z","iopub.execute_input":"2025-12-19T14:27:48.099201Z","iopub.status.idle":"2025-12-19T14:27:48.105409Z","shell.execute_reply.started":"2025-12-19T14:27:48.099165Z","shell.execute_reply":"2025-12-19T14:27:48.104769Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def add_hi_column(\n    examples,\n    model,\n    tokenizer,\n    batch_size=16\n):\n    \"\"\"\n    Adds a 'hi' (Hindi) field to each translation dict\n    by converting Hinglish ('hing') → Hindi using IndicTrans2.\n    \"\"\"\n\n    # examples[\"translation\"] is a LIST of dicts (batched=True)\n    translations = examples[\"translation\"]\n\n    # 1️⃣ Collect Hinglish sentences\n    hinglish_sentences = [\n        t.get(\"hing\", \"\") for t in translations\n    ]\n\n    # 2️⃣ Convert Hinglish → Hindi (BATCH)\n    hindi_sentences = hinglish_to_hindi_batch(\n        sentences=hinglish_sentences,\n        model=model,\n        tokenizer=tokenizer,\n        batch_size=batch_size\n    )\n\n    # 3️⃣ Rebuild translation dicts with added 'hi'\n    new_translations = []\n    for t, hi in zip(translations, hindi_sentences):\n        t_new = dict(t)   # shallow copy (safe)\n        t_new[\"hi\"] = hi\n        new_translations.append(t_new)\n\n    # 4️⃣ Return updated batch\n    return {\n        \"translation\": new_translations\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:27:50.163668Z","iopub.execute_input":"2025-12-19T14:27:50.164385Z","iopub.status.idle":"2025-12-19T14:27:50.169435Z","shell.execute_reply.started":"2025-12-19T14:27:50.164350Z","shell.execute_reply":"2025-12-19T14:27:50.168694Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"domain_train = domain_train.map(\n    add_hi_column,\n    batched=True,\n    batch_size=64,\n    keep_in_memory=True,\n    fn_kwargs={\n        \"model\": model,\n        \"tokenizer\": tokenizer,\n        \"batch_size\":16\n    }\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:27:52.428353Z","iopub.execute_input":"2025-12-19T14:27:52.429039Z","iopub.status.idle":"2025-12-19T14:35:41.103852Z","shell.execute_reply.started":"2025-12-19T14:27:52.429009Z","shell.execute_reply":"2025-12-19T14:35:41.103227Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b428132a54040669b181d73e8e7a7e1"}},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"domain_val = domain_val.map(\n    add_hi_column,\n    batched=True,\n    batch_size=32,\n    keep_in_memory=True,\n    fn_kwargs={\n        \"model\": model,\n        \"tokenizer\": tokenizer,\n        \"batch_size\": 32   # translation batch size (GPU-friendly)\n    }\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:35:41.105111Z","iopub.execute_input":"2025-12-19T14:35:41.105387Z","iopub.status.idle":"2025-12-19T14:37:27.706410Z","shell.execute_reply.started":"2025-12-19T14:35:41.105358Z","shell.execute_reply":"2025-12-19T14:37:27.705883Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b6822e240b44a6ea536eaaebbe18720"}},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"SRC_TAG = \"eng_Latn\"\nTGT_TAG = \"hin_Deva\"\n\nsource_lang = \"en\"\ntarget_lang = \"hi\"\n\nmax_input_length = 128\nmax_target_length = 128\n\ndef preprocess_function(examples):\n    # -------- source side --------\n    sources = [\n        f\"{SRC_TAG} {TGT_TAG} {ex[source_lang].strip()}\"\n        for ex in examples[\"translation\"]\n    ]\n\n    # -------- target side (IMPORTANT) --------\n    targets = [\n        f\"{SRC_TAG} {TGT_TAG} {ex[target_lang].strip()}\"\n        for ex in examples[\"translation\"]\n    ]\n\n    # tokenize source\n    model_inputs = tokenizer(\n        sources,\n        max_length=max_input_length,\n        truncation=True,\n        padding=\"max_length\"\n    )\n\n    # tokenize target (WITH TAGS)\n    labels = tokenizer(\n        targets,\n        max_length=max_target_length,\n        truncation=True,\n        padding=\"max_length\"\n    )\n\n    # replace pad tokens with -100 for loss masking\n    labels[\"input_ids\"] = [\n        [(tok if tok != tokenizer.pad_token_id else -100) for tok in label]\n        for label in labels[\"input_ids\"]\n    ]\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:37:35.967530Z","iopub.execute_input":"2025-12-19T14:37:35.967839Z","iopub.status.idle":"2025-12-19T14:37:35.973938Z","shell.execute_reply.started":"2025-12-19T14:37:35.967809Z","shell.execute_reply":"2025-12-19T14:37:35.973355Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"tokenized_train = domain_train.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=domain_train.column_names\n)\n\ntokenized_val = domain_val.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=domain_val.column_names\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:37:40.020146Z","iopub.execute_input":"2025-12-19T14:37:40.020470Z","iopub.status.idle":"2025-12-19T14:37:42.204697Z","shell.execute_reply.started":"2025-12-19T14:37:40.020440Z","shell.execute_reply":"2025-12-19T14:37:42.203894Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cc107d437c944e4b85eab9a3da91e5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11e22a362b7e4c568e7795f6d8264cb5"}},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"# DataLoader setup\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")\ntrain_loader = DataLoader(tokenized_train, batch_size=16, shuffle=True, collate_fn=data_collator, num_workers=2)\nval_loader   = DataLoader(tokenized_val,   batch_size=16, shuffle=False, collate_fn=data_collator, num_workers=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:38:26.611786Z","iopub.execute_input":"2025-12-19T14:38:26.612190Z","iopub.status.idle":"2025-12-19T14:38:26.616909Z","shell.execute_reply.started":"2025-12-19T14:38:26.612161Z","shell.execute_reply":"2025-12-19T14:38:26.616235Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# ================================\n# Configure LoRA Adapters\n# ================================\nfrom peft import LoraConfig, TaskType\n\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM,\n    r=4,                     \n    lora_alpha=8,            # scaling\n    lora_dropout=0.05,\n    bias=\"none\",\n\n    # ONLY query & value projections\n    target_modules=[\n        \"q_proj\",\n        \"v_proj\"\n    ]\n)\nmodel.gradient_checkpointing_enable()\nmodel.config.use_cache = False\n\n\n# Wrap the model with LoRA adapters\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()  # (Optional) shows how many params are trainable\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:38:28.765589Z","iopub.execute_input":"2025-12-19T14:38:28.766165Z","iopub.status.idle":"2025-12-19T14:38:28.886693Z","shell.execute_reply.started":"2025-12-19T14:38:28.766116Z","shell.execute_reply":"2025-12-19T14:38:28.885914Z"}},"outputs":[{"name":"stderr","text":"You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 884,736 || all params: 1,116,428,288 || trainable%: 0.0792\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# ================================\n# Training Loop with LoRA Fine-Tuning (AMP-safe)\n# ================================\n\nfrom torch.amp import autocast, GradScaler\nfrom torch.optim import AdamW\n\noptimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\nscaler = GradScaler(\"cuda\")\n\nnum_epochs = 1\nmodel.train()\n\nfor epoch in range(num_epochs):\n    total_loss = 0.0\n\n    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n        # Move batch to GPU\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        optimizer.zero_grad(set_to_none=True)\n\n        # Mixed Precision Forward Pass\n        with autocast(\"cuda\"):\n            outputs = model(**batch)\n            loss = outputs.loss\n\n        # Backward pass with gradient scaling\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1} | Train Loss: {avg_loss:.4f}\")\n\n    # ================================\n    # Validation\n    # ================================\n    model.eval()\n    val_loss = 0.0\n\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            with autocast(\"cuda\"):\n                outputs = model(**batch)\n                val_loss += outputs.loss.item()\n\n    val_loss /= len(val_loader)\n    print(f\"Epoch {epoch+1} | Validation Loss: {val_loss:.4f}\")\n\n    model.train()\n\nprint(\" LoRA Fine-tuning complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:38:31.536909Z","iopub.execute_input":"2025-12-19T14:38:31.537305Z","iopub.status.idle":"2025-12-19T14:38:52.188679Z","shell.execute_reply.started":"2025-12-19T14:38:31.537272Z","shell.execute_reply":"2025-12-19T14:38:52.187654Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Training Epoch 1:   0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0c66e09fc6e4dfcbc6d993f63d220d2"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 12.7065\nEpoch 1 | Validation Loss: 12.8074\n LoRA Fine-tuning complete.\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin()  # paste your HF token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:38:55.143426Z","iopub.execute_input":"2025-12-19T14:38:55.143789Z","iopub.status.idle":"2025-12-19T14:38:55.160280Z","shell.execute_reply.started":"2025-12-19T14:38:55.143744Z","shell.execute_reply":"2025-12-19T14:38:55.159430Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8026a6b80f74f35a3969b4db75c354c"}},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"repo_id = \"Vir123-dev/indictrans2_en_hing_finetune_1B\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:39:07.855933Z","iopub.execute_input":"2025-12-19T14:39:07.856651Z","iopub.status.idle":"2025-12-19T14:39:07.860386Z","shell.execute_reply.started":"2025-12-19T14:39:07.856616Z","shell.execute_reply":"2025-12-19T14:39:07.859563Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Save LoRA adapter + push to hub\nmodel.push_to_hub(\n    repo_id,\n    commit_message=\"LoRA fine-tuned IndicTrans2 on Domain-1 (EN-HINGLISH(converted to HI))\"\n)\n\n# Save tokenizer (IMPORTANT)\ntokenizer.push_to_hub(repo_id)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:39:13.624782Z","iopub.execute_input":"2025-12-19T14:39:13.625588Z","iopub.status.idle":"2025-12-19T14:39:22.801678Z","shell.execute_reply.started":"2025-12-19T14:39:13.625554Z","shell.execute_reply":"2025-12-19T14:39:22.801050Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bd834591a18444fb43f277ecda0080c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f053d61bbe01436c9c91c39e763a9637"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c7827c0d8b44efd82351354021089dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c44c14eb84f40a6a03550ee6a42a74c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcf693510b4e490bb3a40976ebaec60a"}},"metadata":{}},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Vir123-dev/indictrans2_en_hing_finetune_1B/commit/1e34844f03f3f320f224b7bd1b666150d7ba7861', commit_message='Upload tokenizer', commit_description='', oid='1e34844f03f3f320f224b7bd1b666150d7ba7861', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Vir123-dev/indictrans2_en_hing_finetune_1B', endpoint='https://huggingface.co', repo_type='model', repo_id='Vir123-dev/indictrans2_en_hing_finetune_1B'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":41}]}