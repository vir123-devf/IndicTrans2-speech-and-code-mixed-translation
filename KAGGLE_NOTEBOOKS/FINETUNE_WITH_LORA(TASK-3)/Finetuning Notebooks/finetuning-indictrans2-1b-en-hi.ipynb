{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================\n# Setup: Install and Import Libraries\n# ================================\n!pip install -q peft\n\nimport os\nimport torch\nfrom datasets import load_dataset, DatasetDict\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\nfrom torch.utils.data import DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tqdm.auto import tqdm\nfrom torch.optim import AdamW\n# Evaluation metrics\n!pip install -q sacrebleu bert-score evaluate unbabel-comet\n\nimport sacrebleu\nfrom bert_score import score as bert_score\nimport evaluate\nfrom comet import download_model, load_from_checkpoint\n\n\n# PEFT (LoRA)\nfrom peft import LoraConfig, get_peft_model, TaskType\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T06:04:22.463826Z","iopub.execute_input":"2025-12-19T06:04:22.464419Z","iopub.status.idle":"2025-12-19T06:05:13.775677Z","shell.execute_reply.started":"2025-12-19T06:04:22.464386Z","shell.execute_reply":"2025-12-19T06:05:13.774919Z"}},"outputs":[{"name":"stderr","text":"2025-12-19 06:04:46.171426: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766124286.407112      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766124286.476652      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766124287.016587      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766124287.016620      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766124287.016623      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766124287.016626      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.0/91.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.4/101.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m529.7/529.7 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngrain 0.2.15 requires protobuf>=5.28.3, but you have protobuf 4.25.8 which is incompatible.\nray 2.52.1 requires click!=8.3.*,>=7.0, but you have click 8.3.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\njaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\njax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\npytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Enter Access Token and rerun\nfrom huggingface_hub import login\nlogin(new_session=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T06:05:13.777438Z","iopub.execute_input":"2025-12-19T06:05:13.778209Z","iopub.status.idle":"2025-12-19T06:05:13.804582Z","shell.execute_reply.started":"2025-12-19T06:05:13.778170Z","shell.execute_reply":"2025-12-19T06:05:13.803673Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66cf293711af4fb08c1f63c8e515fa0e"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# ================================\n# Load Base Model and Tokenizer\n# ================================\nckpt = \"ai4bharat/indictrans2-en-indic-1B\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(ckpt, trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(ckpt)\n\n# Use GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nprint(\"Model loaded on\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T06:06:20.336607Z","iopub.execute_input":"2025-12-19T06:06:20.337393Z","iopub.status.idle":"2025-12-19T06:07:00.994113Z","shell.execute_reply.started":"2025-12-19T06:06:20.337356Z","shell.execute_reply":"2025-12-19T06:07:00.993301Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.37k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a48de4efdcbe4d4bbddb9f80bd52ef00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_indictrans.py:   0%|          | 0.00/14.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f693857a81a44682b1ee293036e5841d"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-en-indic-1B:\n- configuration_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_indictrans.py:   0%|          | 0.00/79.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a5c6a870d35483d8fdc6cca720221a1"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-en-indic-1B:\n- modeling_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afbdf713fa544c36ab47c480dde2f97b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b01b07167fc44a489e615d894edbca0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74cce6ad4a7f4c12802da1e1004ecebe"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository ai4bharat/indictrans2-en-indic-1B contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/ai4bharat/indictrans2-en-indic-1B .\n You can inspect the repository content at https://hf.co/ai4bharat/indictrans2-en-indic-1B.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"tokenization_indictrans.py:   0%|          | 0.00/8.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ceaa53415754fdabd37948e95c67e1d"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-en-indic-1B:\n- tokenization_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"dict.SRC.json:   0%|          | 0.00/645k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8cc7fc5a18a490289c922a4858ae557"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dict.TGT.json:   0%|          | 0.00/3.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16bfb2372d4e4077aba15e16e8d2de58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.SRC:   0%|          | 0.00/759k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ed7cd72b4124bf0ae63c82fce90b93d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.TGT:   0%|          | 0.00/3.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b939c64dbb464bd1a0a7da29e791e806"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09ba08759f9343f0bbe88b6e0e37deeb"}},"metadata":{}},{"name":"stdout","text":"Model loaded on cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ================================\n# Prepare Domain-Specific Data\n# ================================\n\nraw_data = load_dataset(\"atrisaxena/mini-iitb-english-hindi\")\nraw_data = raw_data[\"train\"]  # use train split\n\n# 10,000 statements\ndomain_train = raw_data.shuffle(seed=42).select(range(10000))\ndomain_val   = raw_data.shuffle(seed=42).select(range(100, 150))  # small dev set\n\nprint(domain_train[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T06:07:09.341976Z","iopub.execute_input":"2025-12-19T06:07:09.342291Z","iopub.status.idle":"2025-12-19T06:07:13.426789Z","shell.execute_reply.started":"2025-12-19T06:07:09.342258Z","shell.execute_reply":"2025-12-19T06:07:13.426231Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/193 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eab05f3854e049188270139b35e70e05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train.parquet:   0%|          | 0.00/2.87M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac9305596f8248899e69f7f205b81185"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation.parquet:   0%|          | 0.00/84.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f60730558a8a479d836dda365eb1867f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test.parquet:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99da4adcfd8f4d0ead8387d52b1d9314"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bac1bd1693847faa1996840ace73dab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/520 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"000a9db9a91c4d55832c5061a7ae680a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2507 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"691bd5ed85f04098ae703e90b5b9f773"}},"metadata":{}},{"name":"stdout","text":"{'translation': {'en': 'memento', 'hi': 'अभिज्ञा'}}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ================================\n# Tokenization Function (same format as Task 2)\n# ================================\nSRC_TAG = \"eng_Latn\"\nTGT_TAG = \"hin_Deva\"\nsource_lang = \"en\"; target_lang = \"hi\"\nmax_input_length = 128; max_target_length = 128\n\ndef preprocess_function(examples):\n    inputs = [f\"{SRC_TAG} {TGT_TAG} {ex[source_lang].strip()}\" for ex in examples[\"translation\"]]\n    targets = [ex[target_lang].strip() for ex in examples[\"translation\"]]\n    model_inputs = tokenizer(inputs, max_length=max_input_length,\n                             truncation=True, padding=\"max_length\")\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer([f\"{TGT_TAG} {t}\" for t in targets],\n                           max_length=max_target_length, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Tokenize domain data\ntokenized_train = domain_train.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\ntokenized_val   = domain_val.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T06:07:28.719965Z","iopub.execute_input":"2025-12-19T06:07:28.720286Z","iopub.status.idle":"2025-12-19T06:07:34.035633Z","shell.execute_reply.started":"2025-12-19T06:07:28.720254Z","shell.execute_reply":"2025-12-19T06:07:34.034851Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9f88fe5248d45c78afdbd1a6747c3a1"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef3a5438d5274bd1a685970ba88aa305"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# DataLoader setup\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")\ntrain_loader = DataLoader(tokenized_train, batch_size=16, shuffle=True, collate_fn=data_collator, num_workers=2)\nval_loader   = DataLoader(tokenized_val,   batch_size=16, shuffle=False, collate_fn=data_collator, num_workers=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T06:07:40.059192Z","iopub.execute_input":"2025-12-19T06:07:40.059498Z","iopub.status.idle":"2025-12-19T06:07:40.064393Z","shell.execute_reply.started":"2025-12-19T06:07:40.059467Z","shell.execute_reply":"2025-12-19T06:07:40.063661Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ================================\n# Configure LoRA Adapters\n# ================================\nfrom peft import LoraConfig, TaskType\n\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM,\n    r=4,                     \n    lora_alpha=8,            # scaling\n    lora_dropout=0.05,\n    bias=\"none\",\n\n    # ONLY query & value projections\n    target_modules=[\n        \"q_proj\",\n        \"v_proj\"\n    ]\n)\nmodel.gradient_checkpointing_enable()\nmodel.config.use_cache = False\n\n\n# Wrap the model with LoRA adapters\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()  # (Optional) shows how many params are trainable\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T06:08:38.492513Z","iopub.execute_input":"2025-12-19T06:08:38.492865Z","iopub.status.idle":"2025-12-19T06:08:38.597738Z","shell.execute_reply.started":"2025-12-19T06:08:38.492833Z","shell.execute_reply":"2025-12-19T06:08:38.597033Z"}},"outputs":[{"name":"stderr","text":"You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 884,736 || all params: 1,116,428,288 || trainable%: 0.0792\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ================================\n# Training Loop with LoRA Fine-Tuning (AMP-safe)\n# ================================\n\nfrom torch.amp import autocast, GradScaler\nfrom torch.optim import AdamW\n\noptimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\nscaler = GradScaler(\"cuda\")\n\nnum_epochs = 1\nmodel.train()\n\nfor epoch in range(num_epochs):\n    total_loss = 0.0\n\n    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n        # Move batch to GPU\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        optimizer.zero_grad(set_to_none=True)\n\n        # Mixed Precision Forward Pass\n        with autocast(\"cuda\"):\n            outputs = model(**batch)\n            loss = outputs.loss\n\n        # Backward pass with gradient scaling\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1} | Train Loss: {avg_loss:.4f}\")\n\n    # ================================\n    # Validation\n    # ================================\n    model.eval()\n    val_loss = 0.0\n\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            with autocast(\"cuda\"):\n                outputs = model(**batch)\n                val_loss += outputs.loss.item()\n\n    val_loss /= len(val_loader)\n    print(f\"Epoch {epoch+1} | Validation Loss: {val_loss:.4f}\")\n\n    model.train()\n\nprint(\" LoRA Fine-tuning complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T06:08:43.273456Z","iopub.execute_input":"2025-12-19T06:08:43.273775Z","iopub.status.idle":"2025-12-19T06:18:14.747668Z","shell.execute_reply.started":"2025-12-19T06:08:43.273745Z","shell.execute_reply":"2025-12-19T06:18:14.746642Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Training Epoch 1:   0%|          | 0/625 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5445c53655004b81ae631dc25be42898"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 10.5181\nEpoch 1 | Validation Loss: 9.7900\n✅ LoRA Fine-tuning complete.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin()  # paste your HF token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T06:18:14.750087Z","iopub.execute_input":"2025-12-19T06:18:14.750386Z","iopub.status.idle":"2025-12-19T06:18:14.768630Z","shell.execute_reply.started":"2025-12-19T06:18:14.750345Z","shell.execute_reply":"2025-12-19T06:18:14.767772Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"871e89871e6d4e41aa6fbcd50e269b64"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"repo_id = \"Vir123-dev/indictrans2_en_hi_finetune_1B\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T06:18:25.606438Z","iopub.execute_input":"2025-12-19T06:18:25.606840Z","iopub.status.idle":"2025-12-19T06:18:25.610528Z","shell.execute_reply.started":"2025-12-19T06:18:25.606810Z","shell.execute_reply":"2025-12-19T06:18:25.610006Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Save LoRA adapter + push to hub\nmodel.push_to_hub(\n    repo_id,\n    commit_message=\"LoRA fine-tuned IndicTrans2 on Domain-1 (EN-HI)\"\n)\n\n# Save tokenizer (IMPORTANT)\ntokenizer.push_to_hub(repo_id)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T06:18:28.798664Z","iopub.execute_input":"2025-12-19T06:18:28.799367Z","iopub.status.idle":"2025-12-19T06:18:34.376130Z","shell.execute_reply.started":"2025-12-19T06:18:28.799338Z","shell.execute_reply":"2025-12-19T06:18:34.375368Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfa37a25768a4063aa1ff2374690e8f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd832372b109423994a8027a7b970e21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0002f52239e4f5388c28746f8f66a8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b88aaba1c38d4d3db8d03e8cbb1d279f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a29509f5c0a421c93f5e53cadee02f1"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Vir123-dev/indictrans2_en_hi_finetune_1B/commit/a9089c0ee726caaa514105ea46a5081b9abfa771', commit_message='Upload tokenizer', commit_description='', oid='a9089c0ee726caaa514105ea46a5081b9abfa771', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Vir123-dev/indictrans2_en_hi_finetune_1B', endpoint='https://huggingface.co', repo_type='model', repo_id='Vir123-dev/indictrans2_en_hi_finetune_1B'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":13}]}