{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================\n# Setup: Install and Import Libraries\n# ================================\n!pip install -q peft\n\nimport os\nimport torch\nfrom datasets import load_dataset, DatasetDict\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\nfrom torch.utils.data import DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tqdm.auto import tqdm\nfrom torch.optim import AdamW\n# Evaluation metrics\n!pip install -q sacrebleu bert-score evaluate unbabel-comet\n\nimport sacrebleu\nfrom bert_score import score as bert_score\nimport evaluate\nfrom comet import download_model, load_from_checkpoint\n\n\n# PEFT (LoRA)\nfrom peft import LoraConfig, get_peft_model, TaskType\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Enter Access Token and rerun\nfrom huggingface_hub import login\nlogin(new_session=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================\n# Load Base Model and Tokenizer\n# ================================\nckpt = \"ai4bharat/indictrans2-indic-en-1B\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(ckpt, trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(ckpt)\n\n# Use GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nprint(\"Model loaded on\", device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================\n# Prepare Domain-Specific Data\n# ================================\n\nraw_data = load_dataset(\"atrisaxena/mini-iitb-english-hindi\")\nraw_data = raw_data[\"train\"]  # use train split\n\n# 1000 statements\ndomain_train = raw_data.shuffle(seed=42).select(range(1000))\ndomain_val   = raw_data.shuffle(seed=42).select(range(100, 150))  # small dev set\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================\n# Tokenization Function (same format as Task 2)\n# ================================\nSRC_TAG = \"hin_Deva \"\nTGT_TAG = \"eng_Latn\"\nsource_lang = \"hi\"; target_lang = \"en\"\nmax_input_length = 128; max_target_length = 128\n\ndef preprocess_function(examples):\n    inputs = [f\"{SRC_TAG} {TGT_TAG} {ex[source_lang].strip()}\" for ex in examples[\"translation\"]]\n    targets = [ex[target_lang].strip() for ex in examples[\"translation\"]]\n    model_inputs = tokenizer(inputs, max_length=max_input_length,\n                             truncation=True, padding=\"max_length\")\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer([f\"{TGT_TAG} {t}\" for t in targets],\n                           max_length=max_target_length, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenize domain data\ntokenized_train = domain_train.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\ntokenized_val   = domain_val.map(preprocess_function, batched=True, remove_columns=[\"translation\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DataLoader setup\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")\ntrain_loader = DataLoader(tokenized_train, batch_size=16, shuffle=True, collate_fn=data_collator, num_workers=2)\nval_loader   = DataLoader(tokenized_val,   batch_size=16, shuffle=False, collate_fn=data_collator, num_workers=1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================\n# Configure LoRA Adapters\n# ================================\nfrom peft import LoraConfig, TaskType\n\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM,\n    r=4,                     \n    lora_alpha=8,            # scaling\n    lora_dropout=0.05,\n    bias=\"none\",\n\n    # ONLY query & value projections\n    target_modules=[\n        \"q_proj\",\n        \"v_proj\"\n    ]\n)\nmodel.gradient_checkpointing_enable()\nmodel.config.use_cache = False\n\n\n# Wrap the model with LoRA adapters\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()  # (Optional) shows how many params are trainable\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================\n# Training Loop with LoRA Fine-Tuning (AMP-safe)\n# ================================\n\nfrom torch.amp import autocast, GradScaler\nfrom torch.optim import AdamW\n\noptimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\nscaler = GradScaler(\"cuda\")\n\nnum_epochs = 1\nmodel.train()\n\nfor epoch in range(num_epochs):\n    total_loss = 0.0\n\n    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n        # Move batch to GPU\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        optimizer.zero_grad(set_to_none=True)\n\n        # Mixed Precision Forward Pass\n        with autocast(\"cuda\"):\n            outputs = model(**batch)\n            loss = outputs.loss\n\n        # Backward pass with gradient scaling\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1} | Train Loss: {avg_loss:.4f}\")\n\n    # ================================\n    # Validation\n    # ================================\n    model.eval()\n    val_loss = 0.0\n\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            with autocast(\"cuda\"):\n                outputs = model(**batch)\n                val_loss += outputs.loss.item()\n\n    val_loss /= len(val_loader)\n    print(f\"Epoch {epoch+1} | Validation Loss: {val_loss:.4f}\")\n\n    model.train()\n\nprint(\" LoRA Fine-tuning complete.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin()  # paste your HF token","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"repo_id = \"Vir123-dev/indictrans2_hi_en_finetune_1B\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save LoRA adapter + push to hub\nmodel.push_to_hub(\n    repo_id,\n    commit_message=\"LoRA fine-tuned IndicTrans2 on Domain-1 (HI-EN)\"\n)\n\n# Save tokenizer (IMPORTANT)\ntokenizer.push_to_hub(repo_id)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}