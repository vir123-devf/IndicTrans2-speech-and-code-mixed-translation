{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================\n# Setup: Install and Import Libraries\n# ================================\n!pip install -q peft\n\nimport os\nimport torch\nfrom datasets import load_dataset, DatasetDict\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\nfrom torch.utils.data import DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tqdm.auto import tqdm\nfrom torch.optim import AdamW\n# Evaluation metrics\n!pip install -q sacrebleu bert-score evaluate unbabel-comet\n\nimport sacrebleu\nfrom bert_score import score as bert_score\nimport evaluate\nfrom comet import download_model, load_from_checkpoint\n\n\n# PEFT (LoRA)\nfrom peft import LoraConfig, get_peft_model, TaskType\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T10:14:15.404257Z","iopub.execute_input":"2025-12-19T10:14:15.404981Z","iopub.status.idle":"2025-12-19T10:14:55.249466Z","shell.execute_reply.started":"2025-12-19T10:14:15.404950Z","shell.execute_reply":"2025-12-19T10:14:55.248809Z"}},"outputs":[{"name":"stderr","text":"2025-12-19 10:14:29.633390: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766139269.810610      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766139269.862036      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766139270.291987      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766139270.292022      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766139270.292024      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766139270.292027      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.0/91.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.4/101.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m118.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m529.7/529.7 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngrain 0.2.15 requires protobuf>=5.28.3, but you have protobuf 4.25.8 which is incompatible.\nray 2.52.1 requires click!=8.3.*,>=7.0, but you have click 8.3.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\njaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\njax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\npytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Enter Access Token and rerun\nfrom huggingface_hub import login\nlogin(new_session=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T10:14:55.251159Z","iopub.execute_input":"2025-12-19T10:14:55.251835Z","iopub.status.idle":"2025-12-19T10:14:55.273992Z","shell.execute_reply.started":"2025-12-19T10:14:55.251806Z","shell.execute_reply":"2025-12-19T10:14:55.273205Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b464ee7384ad47a68f25159df687e06f"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# ================================\n# Load Base Model and Tokenizer\n# ================================\nckpt = \"ai4bharat/indictrans2-indic-en-1B\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(ckpt, trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(ckpt)\n\n# Use GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nprint(\"Model loaded on\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T10:16:22.903097Z","iopub.execute_input":"2025-12-19T10:16:22.903413Z","iopub.status.idle":"2025-12-19T10:16:50.734983Z","shell.execute_reply.started":"2025-12-19T10:16:22.903381Z","shell.execute_reply":"2025-12-19T10:16:50.734183Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.37k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"710fbdaf8ddd474b893f07cd01fb266b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_indictrans.py:   0%|          | 0.00/14.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b38ecf2348d447d890c3724343cc733"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-indic-en-1B:\n- configuration_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_indictrans.py:   0%|          | 0.00/79.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4851ac4691074f3b86a8a1f557a87667"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-indic-en-1B:\n- modeling_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f9df124753049a6a3802b0ea89c4db8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f63dfbc5ebc340949b5a5594fe270e7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ce680af754f4611bed767e95470f7f8"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository ai4bharat/indictrans2-indic-en-1B contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/ai4bharat/indictrans2-indic-en-1B .\n You can inspect the repository content at https://hf.co/ai4bharat/indictrans2-indic-en-1B.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"tokenization_indictrans.py:   0%|          | 0.00/8.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"403b57d1dc364088bd41b2c823343a9f"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-indic-en-1B:\n- tokenization_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"dict.SRC.json:   0%|          | 0.00/3.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41b933d9cff14f21be2ee0deb696ec7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dict.TGT.json:   0%|          | 0.00/645k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ed54172ad224b7d8897073d982ebcd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.SRC:   0%|          | 0.00/3.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc35b293fde446bdb5e42eaa189c0d7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.TGT:   0%|          | 0.00/759k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff225efb15ee439580848fee8bbb9dba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d67b6dfcc1d148c289c9f3a90f94e906"}},"metadata":{}},{"name":"stdout","text":"Model loaded on cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ================================\n# Prepare Domain-Specific Data\n# ================================\n\nraw_data = load_dataset(\"atrisaxena/mini-iitb-english-hindi\")\nraw_data = raw_data[\"train\"]  # use train split\n\n# 500 statements --> 10,0000,1000 was giving error -- i tried but error was there --i tried by eliminating empty example then also it showed invalid source language tag\ndomain_train = raw_data.shuffle(seed=42).select(range(500))\ndomain_val   = raw_data.shuffle(seed=42).select(range(50, 75))  # small dev set\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T10:21:06.387527Z","iopub.execute_input":"2025-12-19T10:21:06.387908Z","iopub.status.idle":"2025-12-19T10:21:07.240880Z","shell.execute_reply.started":"2025-12-19T10:21:06.387877Z","shell.execute_reply":"2025-12-19T10:21:07.240261Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# ================================\n# Tokenization Function (IndicTrans2 â€“ ACTUAL FINAL FIX)\n# ================================\n\n\nSRC_TAG = \"hin_Deva\"\nTGT_TAG = \"eng_Latn\"\n\nsource_lang = \"hi\"\ntarget_lang = \"en\"\n\nmax_input_length = 128\nmax_target_length = 128\n\n\ndef preprocess_function(examples):\n    inputs = []\n    targets = []\n\n    for ex in examples[\"translation\"]:\n        # extract safely\n        src = ex.get(source_lang)\n        tgt = ex.get(target_lang)\n\n        if src is None or tgt is None:\n            continue\n\n        src = src.strip()\n        tgt = tgt.strip()\n\n        if not src or not tgt:\n            continue\n\n        # STRICT IndicTrans2 format: exactly two tags\n        inputs.append(f\"{SRC_TAG} {TGT_TAG} {src}\")\n        targets.append(tgt)\n\n    # ğŸš¨ ABSOLUTELY REQUIRED\n    # HuggingFace tokenizer + IndicTrans2 CANNOT handle empty input lists\n    if len(inputs) == 0:\n        return {}\n\n    model_inputs = tokenizer(\n        inputs,\n        max_length=max_input_length,\n        truncation=True,\n        padding=\"max_length\"\n    )\n\n    # Labels: NO language tags\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            targets,\n            max_length=max_target_length,\n            truncation=True,\n            padding=\"max_length\"\n        )\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T10:21:10.103317Z","iopub.execute_input":"2025-12-19T10:21:10.104037Z","iopub.status.idle":"2025-12-19T10:21:10.110392Z","shell.execute_reply.started":"2025-12-19T10:21:10.104005Z","shell.execute_reply":"2025-12-19T10:21:10.109549Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"tokenized_train = domain_train.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=domain_train.column_names\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T10:21:19.801303Z","iopub.execute_input":"2025-12-19T10:21:19.801835Z","iopub.status.idle":"2025-12-19T10:21:20.849697Z","shell.execute_reply.started":"2025-12-19T10:21:19.801802Z","shell.execute_reply":"2025-12-19T10:21:20.849136Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"tokenized_val = domain_val.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=domain_val.column_names\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T10:21:22.875203Z","iopub.execute_input":"2025-12-19T10:21:22.875912Z","iopub.status.idle":"2025-12-19T10:21:23.945621Z","shell.execute_reply.started":"2025-12-19T10:21:22.875877Z","shell.execute_reply":"2025-12-19T10:21:23.944852Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e77167fb36c47e1b9fa32522dde2494"}},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"print(domain_train[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T10:21:38.726977Z","iopub.execute_input":"2025-12-19T10:21:38.727287Z","iopub.status.idle":"2025-12-19T10:21:38.732124Z","shell.execute_reply.started":"2025-12-19T10:21:38.727257Z","shell.execute_reply":"2025-12-19T10:21:38.731397Z"}},"outputs":[{"name":"stdout","text":"{'translation': {'en': 'memento', 'hi': 'à¤…à¤­à¤¿à¤œà¥à¤à¤¾'}}\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# DataLoader setup\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")\ntrain_loader = DataLoader(tokenized_train, batch_size=16, shuffle=True, collate_fn=data_collator, num_workers=2)\nval_loader   = DataLoader(tokenized_val,   batch_size=16, shuffle=False, collate_fn=data_collator, num_workers=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T10:21:41.076165Z","iopub.execute_input":"2025-12-19T10:21:41.076694Z","iopub.status.idle":"2025-12-19T10:21:41.080972Z","shell.execute_reply.started":"2025-12-19T10:21:41.076662Z","shell.execute_reply":"2025-12-19T10:21:41.080227Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# ================================\n# Configure LoRA Adapters\n# ================================\nfrom peft import LoraConfig, TaskType\n\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM,\n    r=4,                     \n    lora_alpha=8,            # scaling\n    lora_dropout=0.05,\n    bias=\"none\",\n\n    # ONLY query & value projections\n    target_modules=[\n        \"q_proj\",\n        \"v_proj\"\n    ]\n)\nmodel.gradient_checkpointing_enable()\nmodel.config.use_cache = False\n\n\n# Wrap the model with LoRA adapters\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()  # (Optional) shows how many params are trainable\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T10:21:43.652757Z","iopub.execute_input":"2025-12-19T10:21:43.653072Z","iopub.status.idle":"2025-12-19T10:21:43.780156Z","shell.execute_reply.started":"2025-12-19T10:21:43.653039Z","shell.execute_reply":"2025-12-19T10:21:43.779605Z"}},"outputs":[{"name":"stderr","text":"You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 884,736 || all params: 1,023,891,456 || trainable%: 0.0864\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# ================================\n# Training Loop with LoRA Fine-Tuning (AMP-safe)\n# ================================\n\nfrom torch.amp import autocast, GradScaler\nfrom torch.optim import AdamW\n\noptimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\nscaler = GradScaler(\"cuda\")\n\nnum_epochs = 1\nmodel.train()\n\nfor epoch in range(num_epochs):\n    total_loss = 0.0\n\n    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n        # Move batch to GPU\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        optimizer.zero_grad(set_to_none=True)\n\n        # Mixed Precision Forward Pass\n        with autocast(\"cuda\"):\n            outputs = model(**batch)\n            loss = outputs.loss\n\n        # Backward pass with gradient scaling\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1} | Train Loss: {avg_loss:.4f}\")\n\n    # ================================\n    # Validation\n    # ================================\n    model.eval()\n    val_loss = 0.0\n\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            with autocast(\"cuda\"):\n                outputs = model(**batch)\n                val_loss += outputs.loss.item()\n\n    val_loss /= len(val_loader)\n    print(f\"Epoch {epoch+1} | Validation Loss: {val_loss:.4f}\")\n\n    model.train()\n\nprint(\" LoRA Fine-tuning complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T10:22:43.370116Z","iopub.execute_input":"2025-12-19T10:22:43.370770Z","iopub.status.idle":"2025-12-19T10:23:11.291254Z","shell.execute_reply.started":"2025-12-19T10:22:43.370736Z","shell.execute_reply":"2025-12-19T10:23:11.290403Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Training Epoch 1:   0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d96d899b1a149f4bab2a48ddec4a254"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 11.0724\nEpoch 1 | Validation Loss: 10.9076\n LoRA Fine-tuning complete.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin()  # paste your HF token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T10:23:59.031306Z","iopub.execute_input":"2025-12-19T10:23:59.031823Z","iopub.status.idle":"2025-12-19T10:23:59.048310Z","shell.execute_reply.started":"2025-12-19T10:23:59.031778Z","shell.execute_reply":"2025-12-19T10:23:59.047625Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e3f7db5f3f24ea2b581aeaf05f1315b"}},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"repo_id = \"Vir123-dev/indictrans2_hi_en_finetune_1B\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T10:24:08.133163Z","iopub.execute_input":"2025-12-19T10:24:08.133721Z","iopub.status.idle":"2025-12-19T10:24:08.137176Z","shell.execute_reply.started":"2025-12-19T10:24:08.133690Z","shell.execute_reply":"2025-12-19T10:24:08.136442Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Save LoRA adapter + push to hub\nmodel.push_to_hub(\n    repo_id,\n    commit_message=\"LoRA fine-tuned IndicTrans2 on Domain-1 (HI-EN)\"\n)\n\n# Save tokenizer (IMPORTANT)\ntokenizer.push_to_hub(repo_id)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T10:24:10.332338Z","iopub.execute_input":"2025-12-19T10:24:10.332963Z","iopub.status.idle":"2025-12-19T10:24:17.260445Z","shell.execute_reply.started":"2025-12-19T10:24:10.332933Z","shell.execute_reply":"2025-12-19T10:24:17.259807Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47d787c868724d37826ef1042d3e6171"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4937babd28644fe98a28327c3fee6ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5df9b99e53a34a26a86e4120a8a90dcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07592717cb0440e1a11abb2cbcacc554"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c69d8ab275564fde9edfbd4086dabb1c"}},"metadata":{}},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Vir123-dev/indictrans2_hi_en_finetune_1B/commit/001d6db1c7c69461e0dc07f4f77727c6e6aafeb2', commit_message='Upload tokenizer', commit_description='', oid='001d6db1c7c69461e0dc07f4f77727c6e6aafeb2', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Vir123-dev/indictrans2_hi_en_finetune_1B', endpoint='https://huggingface.co', repo_type='model', repo_id='Vir123-dev/indictrans2_hi_en_finetune_1B'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":31}]}