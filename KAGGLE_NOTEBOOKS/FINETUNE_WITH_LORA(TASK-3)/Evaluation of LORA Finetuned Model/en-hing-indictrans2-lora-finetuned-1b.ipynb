{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# In this Notebook:- \n* We fine-tuned the IndicTrans2-en-indic-dist-200M model on the PHINC (Parallel Hinglish Social Media Code-Mixed Corpus for Machine Translation) for the EN→HINGLISH translation task. The resulting model was evaluated using a comprehensive suite of MT quality metrics, including BLEU, ChrF, COMET, BERTScore, and BLEURT.","metadata":{}},{"cell_type":"markdown","source":"[](http://)","metadata":{}},{"cell_type":"code","source":"pip uninstall -y numpy jax jaxlib\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:29:40.219525Z","iopub.execute_input":"2025-12-19T16:29:40.220333Z","iopub.status.idle":"2025-12-19T16:29:41.834906Z","shell.execute_reply.started":"2025-12-19T16:29:40.220281Z","shell.execute_reply":"2025-12-19T16:29:41.833858Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\n\u001b[33mWARNING: Skipping jax as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping jaxlib as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os\nos.environ[\"TRANSFORMERS_NO_JAX\"] = \"1\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:29:41.836743Z","iopub.execute_input":"2025-12-19T16:29:41.836982Z","iopub.status.idle":"2025-12-19T16:29:41.840877Z","shell.execute_reply.started":"2025-12-19T16:29:41.836950Z","shell.execute_reply":"2025-12-19T16:29:41.840173Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"!pip uninstall -y numpy scipy pandas pyarrow datasets transformers\n!pip install --force-reinstall --no-cache-dir \\\n  numpy==1.26.4 \\\n  scipy==1.11.4 \\\n  pandas==2.1.4 \\\n  pyarrow==14.0.2 \\\n  datasets==2.16.1 \\\n  transformers==4.36.2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:29:41.842810Z","iopub.execute_input":"2025-12-19T16:29:41.843346Z","iopub.status.idle":"2025-12-19T16:30:18.944086Z","shell.execute_reply.started":"2025-12-19T16:29:41.843298Z","shell.execute_reply":"2025-12-19T16:30:18.943381Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping numpy as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mFound existing installation: scipy 1.11.4\nUninstalling scipy-1.11.4:\n  Successfully uninstalled scipy-1.11.4\nFound existing installation: pandas 2.1.4\nUninstalling pandas-2.1.4:\n  Successfully uninstalled pandas-2.1.4\nFound existing installation: pyarrow 14.0.2\nUninstalling pyarrow-14.0.2:\n  Successfully uninstalled pyarrow-14.0.2\nFound existing installation: datasets 2.16.1\nUninstalling datasets-2.16.1:\n  Successfully uninstalled datasets-2.16.1\nFound existing installation: transformers 4.46.3\nUninstalling transformers-4.46.3:\n  Successfully uninstalled transformers-4.46.3\nCollecting numpy==1.26.4\n  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting scipy==1.11.4\n  Downloading scipy-1.11.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m274.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pandas==2.1.4\n  Downloading pandas-2.1.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nCollecting pyarrow==14.0.2\n  Downloading pyarrow-14.0.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\nCollecting datasets==2.16.1\n  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\nCollecting transformers==4.36.2\n  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m191.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting python-dateutil>=2.8.2 (from pandas==2.1.4)\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\nCollecting pytz>=2020.1 (from pandas==2.1.4)\n  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\nCollecting tzdata>=2022.1 (from pandas==2.1.4)\n  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting filelock (from datasets==2.16.1)\n  Downloading filelock-3.20.1-py3-none-any.whl.metadata (2.1 kB)\nCollecting pyarrow-hotfix (from datasets==2.16.1)\n  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\nCollecting dill<0.3.8,>=0.3.0 (from datasets==2.16.1)\n  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\nCollecting requests>=2.19.0 (from datasets==2.16.1)\n  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\nCollecting tqdm>=4.62.1 (from datasets==2.16.1)\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m303.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting xxhash (from datasets==2.16.1)\n  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\nCollecting multiprocess (from datasets==2.16.1)\n  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\nCollecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1)\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nCollecting aiohttp (from datasets==2.16.1)\n  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\nCollecting huggingface-hub>=0.19.4 (from datasets==2.16.1)\n  Downloading huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\nCollecting packaging (from datasets==2.16.1)\n  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting pyyaml>=5.1 (from datasets==2.16.1)\n  Downloading pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\nCollecting huggingface-hub>=0.19.4 (from datasets==2.16.1)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nCollecting regex!=2019.12.17 (from transformers==4.36.2)\n  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m273.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tokenizers<0.19,>=0.14 (from transformers==4.36.2)\n  Downloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting safetensors>=0.3.1 (from transformers==4.36.2)\n  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\nCollecting aiohappyeyeballs>=2.5.0 (from aiohttp->datasets==2.16.1)\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\nCollecting aiosignal>=1.4.0 (from aiohttp->datasets==2.16.1)\n  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\nCollecting attrs>=17.3.0 (from aiohttp->datasets==2.16.1)\n  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\nCollecting frozenlist>=1.1.1 (from aiohttp->datasets==2.16.1)\n  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\nCollecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.16.1)\n  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\nCollecting propcache>=0.2.0 (from aiohttp->datasets==2.16.1)\n  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==2.16.1)\n  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.1/75.1 kB\u001b[0m \u001b[31m321.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.19.4->datasets==2.16.1)\n  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.19.4->datasets==2.16.1)\n  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nCollecting six>=1.5 (from python-dateutil>=2.8.2->pandas==2.1.4)\n  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\nCollecting charset_normalizer<4,>=2 (from requests>=2.19.0->datasets==2.16.1)\n  Downloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\nCollecting idna<4,>=2.5 (from requests>=2.19.0->datasets==2.16.1)\n  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\nCollecting urllib3<3,>=1.21.1 (from requests>=2.19.0->datasets==2.16.1)\n  Downloading urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\nCollecting certifi>=2017.4.17 (from requests>=2.19.0->datasets==2.16.1)\n  Downloading certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\nCollecting multiprocess (from datasets==2.16.1)\n  Downloading multiprocess-0.70.17-py312-none-any.whl.metadata (7.2 kB)\n  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\nDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m262.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading scipy-1.11.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.8/35.8 MB\u001b[0m \u001b[31m322.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pandas-2.1.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m222.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pyarrow-14.0.2-cp312-cp312-manylinux_2_28_x86_64.whl (38.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m339.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading datasets-2.16.1-py3-none-any.whl (507 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m393.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m345.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m351.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m362.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m390.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m394.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m303.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m377.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m400.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m402.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m400.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m269.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.2/507.2 kB\u001b[0m \u001b[31m310.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m337.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m278.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.5/348.5 kB\u001b[0m \u001b[31m397.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading filelock-3.20.1-py3-none-any.whl (16 kB)\nDownloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m375.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\nDownloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m386.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nDownloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nDownloading attrs-25.4.0-py3-none-any.whl (67 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m326.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.4/159.4 kB\u001b[0m \u001b[31m367.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m331.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m350.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m367.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading idna-3.11-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m329.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.1/256.1 kB\u001b[0m \u001b[31m390.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.6/221.6 kB\u001b[0m \u001b[31m370.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\nDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m292.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading urllib3-2.6.2-py3-none-any.whl (131 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.2/131.2 kB\u001b[0m \u001b[31m316.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m380.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pytz, xxhash, urllib3, tzdata, typing-extensions, tqdm, six, safetensors, regex, pyyaml, pyarrow-hotfix, propcache, packaging, numpy, multidict, idna, hf-xet, fsspec, frozenlist, filelock, dill, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, scipy, requests, python-dateutil, pyarrow, multiprocess, aiosignal, pandas, huggingface-hub, aiohttp, tokenizers, transformers, datasets\n  Attempting uninstall: pytz\n    Found existing installation: pytz 2025.2\n    Uninstalling pytz-2025.2:\n      Successfully uninstalled pytz-2025.2\n  Attempting uninstall: xxhash\n    Found existing installation: xxhash 3.6.0\n    Uninstalling xxhash-3.6.0:\n      Successfully uninstalled xxhash-3.6.0\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.6.2\n    Uninstalling urllib3-2.6.2:\n      Successfully uninstalled urllib3-2.6.2\n  Attempting uninstall: tzdata\n    Found existing installation: tzdata 2025.3\n    Uninstalling tzdata-2025.3:\n      Successfully uninstalled tzdata-2025.3\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.15.0\n    Uninstalling typing_extensions-4.15.0:\n      Successfully uninstalled typing_extensions-4.15.0\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.67.1\n    Uninstalling tqdm-4.67.1:\n      Successfully uninstalled tqdm-4.67.1\n  Attempting uninstall: six\n    Found existing installation: six 1.17.0\n    Uninstalling six-1.17.0:\n      Successfully uninstalled six-1.17.0\n  Attempting uninstall: safetensors\n    Found existing installation: safetensors 0.7.0\n    Uninstalling safetensors-0.7.0:\n      Successfully uninstalled safetensors-0.7.0\n  Attempting uninstall: regex\n    Found existing installation: regex 2025.11.3\n    Uninstalling regex-2025.11.3:\n      Successfully uninstalled regex-2025.11.3\n  Attempting uninstall: pyyaml\n    Found existing installation: PyYAML 6.0.3\n    Uninstalling PyYAML-6.0.3:\n      Successfully uninstalled PyYAML-6.0.3\n  Attempting uninstall: pyarrow-hotfix\n    Found existing installation: pyarrow-hotfix 0.7\n    Uninstalling pyarrow-hotfix-0.7:\n      Successfully uninstalled pyarrow-hotfix-0.7\n  Attempting uninstall: propcache\n    Found existing installation: propcache 0.4.1\n    Uninstalling propcache-0.4.1:\n      Successfully uninstalled propcache-0.4.1\n  Attempting uninstall: packaging\n    Found existing installation: packaging 25.0\n    Uninstalling packaging-25.0:\n      Successfully uninstalled packaging-25.0\n  Attempting uninstall: multidict\n    Found existing installation: multidict 6.7.0\n    Uninstalling multidict-6.7.0:\n      Successfully uninstalled multidict-6.7.0\n  Attempting uninstall: idna\n    Found existing installation: idna 3.11\n    Uninstalling idna-3.11:\n      Successfully uninstalled idna-3.11\n  Attempting uninstall: hf-xet\n    Found existing installation: hf-xet 1.2.0\n    Uninstalling hf-xet-1.2.0:\n      Successfully uninstalled hf-xet-1.2.0\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2023.10.0\n    Uninstalling fsspec-2023.10.0:\n      Successfully uninstalled fsspec-2023.10.0\n  Attempting uninstall: frozenlist\n    Found existing installation: frozenlist 1.8.0\n    Uninstalling frozenlist-1.8.0:\n      Successfully uninstalled frozenlist-1.8.0\n  Attempting uninstall: filelock\n    Found existing installation: filelock 3.20.1\n    Uninstalling filelock-3.20.1:\n      Successfully uninstalled filelock-3.20.1\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.7\n    Uninstalling dill-0.3.7:\n      Successfully uninstalled dill-0.3.7\n  Attempting uninstall: charset_normalizer\n    Found existing installation: charset-normalizer 3.4.4\n    Uninstalling charset-normalizer-3.4.4:\n      Successfully uninstalled charset-normalizer-3.4.4\n  Attempting uninstall: certifi\n    Found existing installation: certifi 2025.11.12\n    Uninstalling certifi-2025.11.12:\n      Successfully uninstalled certifi-2025.11.12\n  Attempting uninstall: attrs\n    Found existing installation: attrs 25.4.0\n    Uninstalling attrs-25.4.0:\n      Successfully uninstalled attrs-25.4.0\n  Attempting uninstall: aiohappyeyeballs\n    Found existing installation: aiohappyeyeballs 2.6.1\n    Uninstalling aiohappyeyeballs-2.6.1:\n      Successfully uninstalled aiohappyeyeballs-2.6.1\n  Attempting uninstall: yarl\n    Found existing installation: yarl 1.22.0\n    Uninstalling yarl-1.22.0:\n      Successfully uninstalled yarl-1.22.0\n  Attempting uninstall: requests\n    Found existing installation: requests 2.32.5\n    Uninstalling requests-2.32.5:\n      Successfully uninstalled requests-2.32.5\n  Attempting uninstall: python-dateutil\n    Found existing installation: python-dateutil 2.9.0.post0\n    Uninstalling python-dateutil-2.9.0.post0:\n      Successfully uninstalled python-dateutil-2.9.0.post0\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.15\n    Uninstalling multiprocess-0.70.15:\n      Successfully uninstalled multiprocess-0.70.15\n  Attempting uninstall: aiosignal\n    Found existing installation: aiosignal 1.4.0\n    Uninstalling aiosignal-1.4.0:\n      Successfully uninstalled aiosignal-1.4.0\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.36.0\n    Uninstalling huggingface-hub-0.36.0:\n      Successfully uninstalled huggingface-hub-0.36.0\n  Attempting uninstall: aiohttp\n    Found existing installation: aiohttp 3.13.2\n    Uninstalling aiohttp-3.13.2:\n      Successfully uninstalled aiohttp-3.13.2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.20.3\n    Uninstalling tokenizers-0.20.3:\n      Successfully uninstalled tokenizers-0.20.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngemma 3.3.0 requires jax, which is not installed.\nkauldron 1.3.0 requires jax, which is not installed.\nclu 0.0.12 requires jax, which is not installed.\nclu 0.0.12 requires jaxlib, which is not installed.\ndopamine-rl 4.1.2 requires jax>=0.1.72, which is not installed.\ndopamine-rl 4.1.2 requires jaxlib>=0.1.51, which is not installed.\norbax-checkpoint 0.11.25 requires jax>=0.6.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\noptax 0.2.6 requires jax>=0.5.3, which is not installed.\noptax 0.2.6 requires jaxlib>=0.5.3, which is not installed.\nchex 0.1.90 requires jax>=0.4.27, which is not installed.\nchex 0.1.90 requires jaxlib>=0.4.27, which is not installed.\nflax 0.10.7 requires jax>=0.6.0, which is not installed.\ngrain 0.2.15 requires protobuf>=5.28.3, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.20.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nray 2.52.1 requires click!=8.3.*,>=7.0, but you have click 8.3.1 which is incompatible.\nnilearn 0.12.1 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\nlangchain 0.3.27 requires SQLAlchemy<3,>=1.4, but you have sqlalchemy 1.2.19 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.2 which is incompatible.\nxarray 2025.10.1 requires pandas>=2.2, but you have pandas 2.1.4 which is incompatible.\nipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.2.19 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\ntsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\nbigframes 2.26.0 requires pyarrow>=15.0.2, but you have pyarrow 14.0.2 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nmizani 0.13.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\npytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 attrs-25.4.0 certifi-2025.11.12 charset_normalizer-3.4.4 datasets-2.16.1 dill-0.3.7 filelock-3.20.1 frozenlist-1.8.0 fsspec-2023.10.0 hf-xet-1.2.0 huggingface-hub-0.36.0 idna-3.11 multidict-6.7.0 multiprocess-0.70.15 numpy-1.26.4 packaging-25.0 pandas-2.1.4 propcache-0.4.1 pyarrow-14.0.2 pyarrow-hotfix-0.7 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.3 regex-2025.11.3 requests-2.32.5 safetensors-0.7.0 scipy-1.11.4 six-1.17.0 tokenizers-0.15.2 tqdm-4.67.1 transformers-4.36.2 typing-extensions-4.15.0 tzdata-2025.3 urllib3-2.6.2 xxhash-3.6.0 yarl-1.22.0\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"#Checking wheather GPU is working or not\n!nvidia-smi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:18.945947Z","iopub.execute_input":"2025-12-19T16:30:18.946207Z","iopub.status.idle":"2025-12-19T16:30:19.184357Z","shell.execute_reply.started":"2025-12-19T16:30:18.946179Z","shell.execute_reply":"2025-12-19T16:30:19.183544Z"}},"outputs":[{"name":"stdout","text":"Fri Dec 19 16:30:18 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   35C    P8              8W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   37C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# installing dataset and transformer\n!pip install datasets transformers[sentencepiece] sacrebleu -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:19.185652Z","iopub.execute_input":"2025-12-19T16:30:19.186035Z","iopub.status.idle":"2025-12-19T16:30:22.461097Z","shell.execute_reply.started":"2025-12-19T16:30:19.186005Z","shell.execute_reply":"2025-12-19T16:30:22.460030Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# to remove version conflict of Protobuf so, downgrade version of Protobuf\n!pip install protobuf==3.20.3 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:22.462432Z","iopub.execute_input":"2025-12-19T16:30:22.462714Z","iopub.status.idle":"2025-12-19T16:30:25.450430Z","shell.execute_reply.started":"2025-12-19T16:30:22.462684Z","shell.execute_reply":"2025-12-19T16:30:25.449624Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.12/dist-packages (3.20.3)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Importing all required modules\nimport os\nimport sys\nimport transformers\nimport torch  # pytorch Import\nimport sacrebleu\nfrom torch.amp import autocast, GradScaler\nfrom tqdm.auto import tqdm\nfrom transformers import DataCollatorForSeq2Seq\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom datasets import load_dataset # for loading the dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM # For getting Embedding\nfrom transformers import DataCollatorForSeq2Seq #getting sequential model and collator for loading batchwise of data\nfrom torch.optim import AdamW # Optimizer\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:25.451806Z","iopub.execute_input":"2025-12-19T16:30:25.452161Z","iopub.status.idle":"2025-12-19T16:30:25.457558Z","shell.execute_reply.started":"2025-12-19T16:30:25.452118Z","shell.execute_reply":"2025-12-19T16:30:25.456928Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Enter Access Token and rerun\nfrom huggingface_hub import login\nlogin(new_session=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:25.458430Z","iopub.execute_input":"2025-12-19T16:30:25.458875Z","iopub.status.idle":"2025-12-19T16:30:25.486880Z","shell.execute_reply.started":"2025-12-19T16:30:25.458849Z","shell.execute_reply":"2025-12-19T16:30:25.486060Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cee2600301814b928c3bd5425ac47e1d"}},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"# Note:\n* I was using the free version of Kaggle, and the memory limit was getting exhausted while training the 1B-parameter model. Because of this constraint, I switched to using the 200M-parameter model instead.","metadata":{}},{"cell_type":"code","source":"pip install -U transformers==4.46.3\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:25.487595Z","iopub.execute_input":"2025-12-19T16:30:25.487858Z","iopub.status.idle":"2025-12-19T16:30:36.360385Z","shell.execute_reply.started":"2025-12-19T16:30:25.487828Z","shell.execute_reply":"2025-12-19T16:30:36.359652Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers==4.46.3\n  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (3.20.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (2.32.5)\nCollecting tokenizers<0.21,>=0.20 (from transformers==4.46.3)\n  Using cached tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (0.7.0)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.3) (2023.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.3) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.3) (1.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.3) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.3) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.3) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.3) (2025.11.12)\nUsing cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\nUsing cached tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\nInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.36.2\n    Uninstalling transformers-4.36.2:\n      Successfully uninstalled transformers-4.36.2\nSuccessfully installed tokenizers-0.20.3 transformers-4.46.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom peft import PeftModel\n\nBASE_MODEL = \"ai4bharat/indictrans2-en-indic-1B\"\nLORA_REPO  = \"Vir123-dev/indictrans2_en_hing_finetune_1B\"\n\n# Load tokenizer ONLY from base model\ntokenizer = AutoTokenizer.from_pretrained(\n    BASE_MODEL,\n    trust_remote_code=True\n)\n\n# Load base model\nbase_model = AutoModelForSeq2SeqLM.from_pretrained(\n    BASE_MODEL,\n    trust_remote_code=True\n)\n\n# Attach LoRA adapters\nmodel = PeftModel.from_pretrained(\n    base_model,\n    LORA_REPO\n)\n\nmodel = model.to(\"cuda\")\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:31:19.750122Z","iopub.execute_input":"2025-12-19T16:31:19.750518Z","iopub.status.idle":"2025-12-19T16:31:19.857906Z","shell.execute_reply.started":"2025-12-19T16:31:19.750487Z","shell.execute_reply":"2025-12-19T16:31:19.857022Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m \u001b[0;31m# docstyle-ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     from ..cache_utils import (\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mHQQQuantizedCache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/cache_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfiguration_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m from .utils import (\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mis_hqq_available\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'is_hqq_available' from 'transformers.utils' (/usr/local/lib/python3.12/dist-packages/transformers/utils/__init__.py)","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m \u001b[0;31m# docstyle-ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdynamic_module_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcustom_object_save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerationConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerationMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftAdapterMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepspeed_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_deepspeed_zero3_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mnote\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myou\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mneed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mrestart\u001b[0m \u001b[0myour\u001b[0m \u001b[0mruntime\u001b[0m \u001b[0mafter\u001b[0m \u001b[0minstallation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m \"\"\"\n\u001b[0m\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[0;31m# docstyle-ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m FLAX_IMPORT_ERROR = \"\"\"\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0mrequires\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mFLAX\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mit\u001b[0m \u001b[0mwas\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myour\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mCheckout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstructions\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.generation.configuration_utils because of the following error (look up to see its traceback):\ncannot import name 'is_hqq_available' from 'transformers.utils' (/usr/local/lib/python3.12/dist-packages/transformers/utils/__init__.py)","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_237/3566543054.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mBASE_MODEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ai4bharat/indictrans2-en-indic-1B\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mLORA_REPO\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m\"Vir123-dev/indictrans2_en_hing_finetune_1B\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.17.1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from .auto import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mAutoPeftModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m from .peft_model import (\n\u001b[1;32m     33\u001b[0m     \u001b[0mPeftModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPushToHubMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_user_agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaskType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_cache_to_layer_device_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloftq_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreplace_lora_weights_loftq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from .other import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mINCLUDE_LINEAR_LAYERS_SHORTHAND\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/utils/other.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpackaging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msafetensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstorage_ptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_auto_gptq_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_gptqmodel_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0minstallation\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mwww\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0minstall\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfollow\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mones\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0myour\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mnote\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myou\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mneed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mrestart\u001b[0m \u001b[0myour\u001b[0m \u001b[0mruntime\u001b[0m \u001b[0mafter\u001b[0m \u001b[0minstallation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m \"\"\"\n\u001b[0m\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[0;31m# docstyle-ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m FLAX_IMPORT_ERROR = \"\"\"\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0mrequires\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mFLAX\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mit\u001b[0m \u001b[0mwas\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myour\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mCheckout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstructions\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[0minstallation\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mgithub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mgoogle\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mflax\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfollow\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mones\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0myour\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nFailed to import transformers.generation.configuration_utils because of the following error (look up to see its traceback):\ncannot import name 'is_hqq_available' from 'transformers.utils' (/usr/local/lib/python3.12/dist-packages/transformers/utils/__init__.py)"],"ename":"RuntimeError","evalue":"Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nFailed to import transformers.generation.configuration_utils because of the following error (look up to see its traceback):\ncannot import name 'is_hqq_available' from 'transformers.utils' (/usr/local/lib/python3.12/dist-packages/transformers/utils/__init__.py)","output_type":"error"}],"execution_count":21},{"cell_type":"markdown","source":"# The Dataset¶\n\n* Source: https://huggingface.co/datasets/LingoIITGN/PHINC","metadata":{}},{"cell_type":"code","source":"raw_dataset = load_dataset(\"LingoIITGN/PHINC\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.497724Z","iopub.status.idle":"2025-12-19T16:30:36.497966Z","shell.execute_reply.started":"2025-12-19T16:30:36.497847Z","shell.execute_reply":"2025-12-19T16:30:36.497862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"raw_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.500299Z","iopub.status.idle":"2025-12-19T16:30:36.500638Z","shell.execute_reply.started":"2025-12-19T16:30:36.500520Z","shell.execute_reply":"2025-12-19T16:30:36.500536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_to_translation(example):\n    return {\n        \"translation\": {\n            \"en\": example[\"English_Translation\"],\n            \"hing\": example[\"Sentence\"]\n        }\n    }\n\nraw_dataset = raw_dataset.map(convert_to_translation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.501708Z","iopub.status.idle":"2025-12-19T16:30:36.502052Z","shell.execute_reply.started":"2025-12-19T16:30:36.501869Z","shell.execute_reply":"2025-12-19T16:30:36.501893Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"raw_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.503160Z","iopub.status.idle":"2025-12-19T16:30:36.503471Z","shell.execute_reply.started":"2025-12-19T16:30:36.503288Z","shell.execute_reply":"2025-12-19T16:30:36.503303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import DatasetDict\n\n# total rows\ntotal_rows = raw_dataset[\"train\"].num_rows\n\n# required splits\ntrain_rows = 12000\nvalid_rows = 538\ntest_rows = 1200\n\n# slice the dataset\ntrain_dataset = raw_dataset[\"train\"].select(range(0, train_rows))\nvalid_dataset = raw_dataset[\"train\"].select(range(train_rows, train_rows + valid_rows))\ntest_dataset  = raw_dataset[\"train\"].select(range(train_rows + valid_rows,\n                                                  train_rows + valid_rows + test_rows))\n\n# create final DatasetDict\nfinal_dataset = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": valid_dataset,\n    \"test\": test_dataset\n})\n\nfinal_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.505467Z","iopub.status.idle":"2025-12-19T16:30:36.505793Z","shell.execute_reply.started":"2025-12-19T16:30:36.505615Z","shell.execute_reply":"2025-12-19T16:30:36.505639Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"raw_dataset = final_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.506533Z","iopub.status.idle":"2025-12-19T16:30:36.506760Z","shell.execute_reply.started":"2025-12-19T16:30:36.506645Z","shell.execute_reply":"2025-12-19T16:30:36.506665Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Observation for Statistics related to dataset","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport numpy as np\nimport math\nimport nltk\nnltk.download(\"punkt\")  # one-time\n\ndef add_stats(example):\n    text = example[\"translation\"][\"en\"]\n    # guard\n    if text is None: text = \"\"\n    text = text.strip() # Removes unwanted spacing\n    words = text.split()\n    # sentence count (approx)\n    sents = nltk.tokenize.sent_tokenize(text) if text else []\n    example[\"num_words\"] = len(words)\n    example[\"num_chars\"] = len(text)\n    example[\"num_sentences\"] = len(sents)\n    return example\n\nraw_dataset = raw_dataset.map(add_stats, batched=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.508150Z","iopub.status.idle":"2025-12-19T16:30:36.508515Z","shell.execute_reply.started":"2025-12-19T16:30:36.508333Z","shell.execute_reply":"2025-12-19T16:30:36.508358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Obtaining the Statistics:\n\ndef summary_stats(arr):\n    arr = np.array(arr)\n    return {\n        \"count\": int(arr.size),\n        \"min\": int(arr.min()) if arr.size>0 else None,\n        \"p1\": int(np.percentile(arr, 1)) if arr.size>0 else None,\n        \"p10\": int(np.percentile(arr, 10)) if arr.size>0 else None,\n        \"median\": float(np.median(arr)) if arr.size>0 else None,\n        \"mean\": float(arr.mean()) if arr.size>0 else None,\n        \"std\": float(arr.std(ddof=0)) if arr.size>0 else None,\n        \"p90\": int(np.percentile(arr, 90)) if arr.size>0 else None,\n        \"p99\": int(np.percentile(arr, 99)) if arr.size>0 else None,\n        \"max\": int(arr.max()) if arr.size>0 else None,\n    }\n\nfor split in raw_dataset:\n    d = raw_dataset[split]\n    print(f\"\\n=== {split.upper()} ===\")\n    print(\"Words:\", summary_stats(d[\"num_words\"]))\n    print(\"Chars:\", summary_stats(d[\"num_chars\"]))\n    print(\"Sentences:\", summary_stats(d[\"num_sentences\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.509929Z","iopub.status.idle":"2025-12-19T16:30:36.510228Z","shell.execute_reply.started":"2025-12-19T16:30:36.510065Z","shell.execute_reply":"2025-12-19T16:30:36.510087Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import DatasetDict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.511687Z","iopub.status.idle":"2025-12-19T16:30:36.511953Z","shell.execute_reply.started":"2025-12-19T16:30:36.511830Z","shell.execute_reply":"2025-12-19T16:30:36.511845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train has min length of sentences as 0 ('min': 0) so, we Remove these row from dataset\ndef not_empty(example):\n    text = example[\"translation\"][\"en\"]\n    return text is not None and len(text.strip()) > 0\n    \nclean_train = raw_dataset[\"train\"].filter(not_empty)\nclean_val   = raw_dataset[\"validation\"].filter(not_empty)\nclean_test  = raw_dataset[\"test\"].filter(not_empty)\n\nraw_dataset = DatasetDict({\n    \"train\": clean_train,\n    \"validation\": clean_val,\n    \"test\": clean_test\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.513188Z","iopub.status.idle":"2025-12-19T16:30:36.513520Z","shell.execute_reply.started":"2025-12-19T16:30:36.513369Z","shell.execute_reply":"2025-12-19T16:30:36.513392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(raw_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.514802Z","iopub.status.idle":"2025-12-19T16:30:36.515612Z","shell.execute_reply.started":"2025-12-19T16:30:36.515386Z","shell.execute_reply":"2025-12-19T16:30:36.515411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# compute p99 threshold('p99': 31 so, removing other outliers(longer than 31 words))\nword_lengths = np.array(raw_dataset[\"train\"][\"num_words\"])\np99_threshold = int(np.percentile(word_lengths, 99))\nprint(\"Removing sentences longer than:\", p99_threshold, \"words\")\nraw_dataset[\"train\"] = raw_dataset[\"train\"].filter(\n    lambda ex: ex[\"num_words\"] <= p99_threshold\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.516888Z","iopub.status.idle":"2025-12-19T16:30:36.517223Z","shell.execute_reply.started":"2025-12-19T16:30:36.517042Z","shell.execute_reply":"2025-12-19T16:30:36.517057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Obtaining the desired Statistics\ndef summary_stats(arr):\n    arr = np.array(arr)\n    return {\n        \"count\": int(arr.size),\n        \"min\": int(arr.min()) if arr.size>0 else None,\n        \"p1\": int(np.percentile(arr, 1)) if arr.size>0 else None,\n        \"p10\": int(np.percentile(arr, 10)) if arr.size>0 else None,\n        \"median\": float(np.median(arr)) if arr.size>0 else None,\n        \"mean\": float(arr.mean()) if arr.size>0 else None,\n        \"std\": float(arr.std(ddof=0)) if arr.size>0 else None,\n        \"p90\": int(np.percentile(arr, 90)) if arr.size>0 else None,\n        \"p99\": int(np.percentile(arr, 99)) if arr.size>0 else None,\n        \"max\": int(arr.max()) if arr.size>0 else None,\n    }\n\nfor split in raw_dataset:\n    d = raw_dataset[split]\n    print(f\"\\n=== {split.upper()} ===\")\n    print(\"Words:\", summary_stats(d[\"num_words\"]))\n    print(\"Chars:\", summary_stats(d[\"num_chars\"]))\n    print(\"Sentences:\", summary_stats(d[\"num_sentences\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.518131Z","iopub.status.idle":"2025-12-19T16:30:36.518404Z","shell.execute_reply.started":"2025-12-19T16:30:36.518260Z","shell.execute_reply":"2025-12-19T16:30:36.518275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sample Example\nraw_dataset['train'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.519747Z","iopub.status.idle":"2025-12-19T16:30:36.519974Z","shell.execute_reply.started":"2025-12-19T16:30:36.519865Z","shell.execute_reply":"2025-12-19T16:30:36.519879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import DatasetDict\n\n# New desired sizes\nN_TRAIN = 2000\nN_VAL   = 150\nN_TEST  = 250\n\n# Downsample using .select()\nsmall_train = raw_dataset[\"train\"].select(range(N_TRAIN))\nsmall_val   = raw_dataset[\"validation\"].select(range(N_VAL))\nsmall_test  = raw_dataset[\"test\"].select(range(N_TEST))\n\n# Create a new DatasetDict\nsmall_dataset = DatasetDict({\n    \"train\": small_train,\n    \"validation\": small_val,\n    \"test\": small_test\n})\n\nsmall_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.521509Z","iopub.status.idle":"2025-12-19T16:30:36.521837Z","shell.execute_reply.started":"2025-12-19T16:30:36.521713Z","shell.execute_reply":"2025-12-19T16:30:36.521729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval() # Evaluation of model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.522474Z","iopub.status.idle":"2025-12-19T16:30:36.522793Z","shell.execute_reply.started":"2025-12-19T16:30:36.522616Z","shell.execute_reply":"2025-12-19T16:30:36.522640Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hinglish --> Hindi Translation\n\nSRC_TAG = \"eng_Latn\"   # Hinglish / Roman Hindi\nTGT_TAG = \"hin_Deva\"  # Hindi\ndef hinglish_to_hindi(sentences, batch_size=8, max_len=128):\n    outputs = []\n\n    device = model.device\n\n    for i in range(0, len(sentences), batch_size):\n        batch = sentences[i:i + batch_size]\n\n        tagged = [\n            f\"{SRC_TAG} {TGT_TAG} {text}\"\n            for text in batch\n        ]\n\n        inputs = tokenizer(\n            tagged,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=max_len\n        ).to(device)\n\n        with torch.no_grad():\n            generated = model.generate(\n                **inputs,\n                max_length=max_len,\n                num_beams=4,\n                use_cache=False,       \n                early_stopping=True\n            )\n\n        decoded = tokenizer.batch_decode(\n            generated,\n            skip_special_tokens=True\n        )\n\n        outputs.extend(decoded)\n\n    return outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.523708Z","iopub.status.idle":"2025-12-19T16:30:36.523940Z","shell.execute_reply.started":"2025-12-19T16:30:36.523824Z","shell.execute_reply":"2025-12-19T16:30:36.523837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_split(split):\n    hinglish_sentences = [ex[\"hing\"] for ex in split[\"translation\"]]\n    hindi_sentences = hinglish_to_hindi(hinglish_sentences)\n\n    new_translation = []\n    for i in range(len(split)):\n        new_translation.append({\n            \"en\": split[\"translation\"][i][\"en\"],\n            \"hing\": split[\"translation\"][i][\"hing\"],\n            \"hi\": hindi_sentences[i]\n        })\n\n    return split.remove_columns(\"translation\").add_column(\n        \"translation\", new_translation\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.525502Z","iopub.status.idle":"2025-12-19T16:30:36.525856Z","shell.execute_reply.started":"2025-12-19T16:30:36.525674Z","shell.execute_reply":"2025-12-19T16:30:36.525697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"small_dataset[\"train\"] = convert_split(small_dataset[\"train\"])\nsmall_dataset[\"validation\"] = convert_split(small_dataset[\"validation\"])\nsmall_dataset[\"test\"] = convert_split(small_dataset[\"test\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.527448Z","iopub.status.idle":"2025-12-19T16:30:36.527760Z","shell.execute_reply.started":"2025-12-19T16:30:36.527628Z","shell.execute_reply":"2025-12-19T16:30:36.527649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"small_dataset[\"train\"][\"translation\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.529191Z","iopub.status.idle":"2025-12-19T16:30:36.529493Z","shell.execute_reply.started":"2025-12-19T16:30:36.529354Z","shell.execute_reply":"2025-12-19T16:30:36.529379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"small_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.530575Z","iopub.status.idle":"2025-12-19T16:30:36.530849Z","shell.execute_reply.started":"2025-12-19T16:30:36.530735Z","shell.execute_reply":"2025-12-19T16:30:36.530750Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"raw_dataset = small_dataset  # As said in Task 2(for 2000 pair of sentence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.531899Z","iopub.status.idle":"2025-12-19T16:30:36.532194Z","shell.execute_reply.started":"2025-12-19T16:30:36.532067Z","shell.execute_reply":"2025-12-19T16:30:36.532088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\nTECH_WORDS = [\n    \"ai\", \"ml\", \"ai/ml\",\n    \"artificial intelligence\",\n    \"machine learning\",\n    \"emotional\"\n]\n\ndef protect_technical_words(text):\n    protected = {}\n    for i, word in enumerate(TECH_WORDS):\n        token = f\"<TECH{i}>\"\n        pattern = re.compile(re.escape(word), re.IGNORECASE)\n        if pattern.search(text):\n            protected[token] = word\n            text = pattern.sub(token, text)\n    return text, protected\n\ndef restore_technical_words(text, protected):\n    for token, word in protected.items():\n        text = text.replace(token.lower(), word)\n        text = text.replace(token, word) \n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.533483Z","iopub.status.idle":"2025-12-19T16:30:36.533781Z","shell.execute_reply.started":"2025-12-19T16:30:36.533649Z","shell.execute_reply":"2025-12-19T16:30:36.533677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def iast_to_hinglish(text):\n    # --- diacritics ---\n    diacritics = {\n        \"ā\": \"a\",\n        \"ī\": \"i\",\n        \"ū\": \"u\",\n        \"ṃ\": \"n\",\n        \"ṅ\": \"ng\",\n        \"ñ\": \"ny\",\n        \"ṭ\": \"t\",\n        \"ḍ\": \"d\",\n        \"ṇ\": \"n\",\n        \"ś\": \"sh\",\n        \"ṣ\": \"sh\",\n        \"ṛ\": \"r\",\n        \"ḥ\": \"h\",\n    }\n    for k, v in diacritics.items():\n        text = text.replace(k, v)\n\n    # --- Hinglish lexical fixes ---\n    fixes = {\n        \" nama \": \" naam \",\n        \" naama \": \" naam \",\n        \" vartamana \": \" vartaman \",\n        \" vartamaana \": \" vartaman \",\n        \" men \": \" mein \",\n        \" hu~\": \" hoon\",\n        \" hun\": \" hoon\",\n        \" aura \": \" aur \",\n        \" yaha \": \" yeh \",\n        \" bahuta \": \" bahut \",\n        \" anubhava \": \" anubhav \",\n        \"|\": \".\",\n    }\n    for k, v in fixes.items():\n        text = text.replace(k, v)\n\n    return text.lower().strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.534585Z","iopub.status.idle":"2025-12-19T16:30:36.534966Z","shell.execute_reply.started":"2025-12-19T16:30:36.534766Z","shell.execute_reply":"2025-12-19T16:30:36.534791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install indic-transliteration","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.536357Z","iopub.status.idle":"2025-12-19T16:30:36.536864Z","shell.execute_reply.started":"2025-12-19T16:30:36.536661Z","shell.execute_reply":"2025-12-19T16:30:36.536688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from indic_transliteration import sanscript\nfrom indic_transliteration.sanscript import transliterate\n\ndef hindi_to_hinglish(hindi_text):\n    # protect technical terms\n    safe_text, protected = protect_technical_words(hindi_text)\n\n    # Hindi → IAST\n    iast = transliterate(safe_text, sanscript.DEVANAGARI, sanscript.IAST)\n\n    # IAST → Hinglish\n    hinglish = iast_to_hinglish(iast)\n\n    # restore technical terms\n    hinglish = restore_technical_words(hinglish, protected)\n\n    return hinglish\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.538027Z","iopub.status.idle":"2025-12-19T16:30:36.538407Z","shell.execute_reply.started":"2025-12-19T16:30:36.538201Z","shell.execute_reply":"2025-12-19T16:30:36.538224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Hindi_text = \"@someUSER आपको ब्रिटिश बाल गायक सोफिया ग्रेस और रोजी की अपने शो की यात्रा की पहली वर्षगांठ मनाने के लिए बधाई ।\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.539814Z","iopub.status.idle":"2025-12-19T16:30:36.540159Z","shell.execute_reply.started":"2025-12-19T16:30:36.539985Z","shell.execute_reply":"2025-12-19T16:30:36.540008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(hindi_to_hinglish(Hindi_text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.541080Z","iopub.status.idle":"2025-12-19T16:30:36.541354Z","shell.execute_reply.started":"2025-12-19T16:30:36.541210Z","shell.execute_reply":"2025-12-19T16:30:36.541225Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation metrics","metadata":{}},{"cell_type":"markdown","source":"# Finding  BLEU AND CHRF:\n\n1. BLEU: BLEU checks how many n-grams from the candidate sentence also appear in the reference sentence.\n2. CHRF: Instead of words, CHRF compares character n-grams.","metadata":{}},{"cell_type":"code","source":"model.device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.542233Z","iopub.status.idle":"2025-12-19T16:30:36.542535Z","shell.execute_reply.started":"2025-12-19T16:30:36.542383Z","shell.execute_reply":"2025-12-19T16:30:36.542399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(raw_dataset[\"test\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.543468Z","iopub.status.idle":"2025-12-19T16:30:36.543783Z","shell.execute_reply.started":"2025-12-19T16:30:36.543580Z","shell.execute_reply":"2025-12-19T16:30:36.543594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_samples = 10\n\n# determine model device safely\ntry:\n    model_device = next(model.parameters()).device\nexcept StopIteration:\n    model_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"Model device:\", model_device)\n\npreds = []\nrefs = []\n\nfor i in range(min(n_samples, len(raw_dataset[\"test\"]))):\n    row = raw_dataset[\"test\"][i]\n\n    # handle multiple possible row formats\n    if isinstance(row, dict) and \"translation\" in row:\n        trans = row[\"translation\"]\n        # trans might be a dict or a string; handle both\n        if isinstance(trans, dict):\n            eng = trans.get(\"en\") or trans.get(\"eng\") or \"\"\n            hi_ref = trans.get(\"hing\") or trans.get(\"hin\") or \"\"\n        else:\n            # sometimes translation is a string (rare) — treat as source\n            eng = str(trans)\n            hi_ref = \"\"\n    elif isinstance(row, dict):\n        # maybe keys are directly 'en' and 'hi'\n        eng = row.get(\"en\") or row.get(\"eng\") or row.get(\"source\") or \"\"\n        hi_ref = row.get(\"hing\") or row.get(\"hin\") or row.get(\"target\") or \"\"\n    else:\n        # fallback: row itself might be the translation dict-like\n        try:\n            eng = row[\"en\"]\n            hi_ref = row[\"hing\"]\n        except Exception:\n            # last resort: stringify\n            eng = str(row)\n            hi_ref = \"\"\n\n    eng = (eng or \"\").strip()\n    hi_ref = (hi_ref or \"\").strip()\n    refs.append(hi_ref if hi_ref else \"\")  # keep alignment\n    safe_text, protected = protect_tokens(eng)\n    # add required tags\n    tagged = f\"{SRC_TAG} {TGT_TAG} {eng}\"\n\n    # tokenize -> torch tensors -> move to model device\n    tokenized = tokenizer(tagged, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n    tokenized = {k: v.to(model_device) for k, v in tokenized.items()}\n\n    # generate\n    with torch.no_grad():\n        out_ids = model.generate(\n           **tokenized,\n            max_length=128,\n            num_beams=4,        \n            use_cache=False,  \n            early_stopping=True\n        )\n    \n    Hindi_text = en_to_hi(eng, model, tokenizer)    \n\n    hinglish = hindi_to_hinglish(Hindi_text)\n    preds.append(hinglish)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.544521Z","iopub.status.idle":"2025-12-19T16:30:36.544819Z","shell.execute_reply.started":"2025-12-19T16:30:36.544652Z","shell.execute_reply":"2025-12-19T16:30:36.544673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds[0:4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.545752Z","iopub.status.idle":"2025-12-19T16:30:36.546068Z","shell.execute_reply.started":"2025-12-19T16:30:36.545888Z","shell.execute_reply":"2025-12-19T16:30:36.545919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(preds)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.546925Z","iopub.status.idle":"2025-12-19T16:30:36.547149Z","shell.execute_reply.started":"2025-12-19T16:30:36.547041Z","shell.execute_reply":"2025-12-19T16:30:36.547055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#BLEU:\nbleu = sacrebleu.corpus_bleu(preds, [refs])\n#CHRF:\nchrf = sacrebleu.corpus_chrf(preds, [refs])\n\nprint(f\"\\nEvaluated {len(preds)} samples\")\nprint(\"BLEU:\", bleu.score)\nprint(\"CHRF:\", chrf.score)\n\nfor i in range(min(5, len(preds))):\n    print(f\"\\n=== SAMPLE {i+1} ===\")\n    print(\"SRC :\", (raw_dataset[\"test\"][i].get(\"translation\", raw_dataset[\"test\"][i]).get(\"en\")\n                    if isinstance(raw_dataset[\"test\"][i], dict) and \"translation\" in raw_dataset[\"test\"][i]\n                    else (raw_dataset[\"test\"][i].get(\"en\") if isinstance(raw_dataset[\"test\"][i], dict) else str(raw_dataset[\"test\"][i]))))\n    print(\"PRED:\", preds[i])\n    print(\"REF :\", refs[i])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.548107Z","iopub.status.idle":"2025-12-19T16:30:36.548401Z","shell.execute_reply.started":"2025-12-19T16:30:36.548248Z","shell.execute_reply":"2025-12-19T16:30:36.548264Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Finding BERTScore:\n\n* BERTScore: Uses BERT (or RoBERTa, or mBERT) embeddings to compare every token in candidate with every token in reference.","metadata":{}},{"cell_type":"code","source":"!pip install bert-score\nfrom bert_score import score\n\nP, R, F1 = score(preds, refs, lang=\"eng\")\nprint(\"BERTScore F1:\", F1.mean().item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.549511Z","iopub.status.idle":"2025-12-19T16:30:36.549830Z","shell.execute_reply.started":"2025-12-19T16:30:36.549648Z","shell.execute_reply":"2025-12-19T16:30:36.549665Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Finding BLEURT:\n* BLEURT computes a similarity score using a fine-tuned BERT model that predicts human judgment of translation quality.","metadata":{}},{"cell_type":"code","source":"# Required Packages for Bleurt\n!pip install evaluate\n!pip install git+https://github.com/google-research/bleurt.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.551078Z","iopub.status.idle":"2025-12-19T16:30:36.551408Z","shell.execute_reply.started":"2025-12-19T16:30:36.551213Z","shell.execute_reply":"2025-12-19T16:30:36.551233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculating Bleurt\nimport evaluate\nbleurt = evaluate.load(\"bleurt\")\nresults = bleurt.compute(predictions=preds, references=refs)\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.552835Z","iopub.status.idle":"2025-12-19T16:30:36.553176Z","shell.execute_reply.started":"2025-12-19T16:30:36.552999Z","shell.execute_reply":"2025-12-19T16:30:36.553021Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Finding COMET\n* COMET predicts a score that strongly correlates with human judgment.","metadata":{}},{"cell_type":"code","source":"# Required package for Comet\n!pip install -q unbabel-comet\nfrom comet import download_model, load_from_checkpoint\n\n\n# choose model variable \ntranslation_model = globals().get(\"translation_model\", None) or globals().get(\"model\", None)\nif translation_model is None:\n    raise ValueError(\"No translation model found. Load your model into `model` or `translation_model` first.\")\n\n# device: try to get model device (handles DeviceMap too)\ntry:\n    model_device = next(translation_model.parameters()).device\nexcept StopIteration:\n    model_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nSRC_TAG = \"eng_Latn\"\nTGT_TAG = \"hin_Deva\"\n\nsrcs = []\npreds = []\nrefs = []\n\nn = 10\nfor i in range(min(n, len(raw_dataset[\"test\"]))):\n    row = raw_dataset[\"test\"][i]\n\n    # robust extraction of source & reference\n    if isinstance(row, dict) and \"translation\" in row:\n        trans = row[\"translation\"]\n        if isinstance(trans, dict):\n            src = (trans.get(\"en\") or trans.get(\"eng\") or \"\").strip()\n            ref = (trans.get(\"hing\") or trans.get(\"hing\") or \"\").strip()\n        else:\n            src = str(trans).strip()\n            ref = \"\"\n    elif isinstance(row, dict):\n        src = (row.get(\"en\") or row.get(\"eng\") or row.get(\"source\") or \"\").strip()\n        ref = (row.get(\"hing\") or row.get(\"hing\") or row.get(\"target\") or \"\").strip()\n    else:\n        # fallback\n        src = str(row).strip()\n        ref = \"\"\n\n    srcs.append(src)\n    refs.append(ref)\n\n    # add language tags required by IndicTrans2\n    tagged = f\"{SRC_TAG} {TGT_TAG} {src}\"\n\n    # tokenize -> PyTorch tensors -> move to model device\n    tokenized = tokenizer(tagged,\n                          return_tensors=\"pt\",\n                          truncation=True,\n                          padding=True,\n                          max_length=128)\n    tokenized = {k: v.to(model_device) for k, v in tokenized.items()}\n\n    # generate\n    with torch.no_grad():\n        out_ids = translation_model.generate(**tokenized, max_length=128, num_beams=4, early_stopping=True)\n\n    Hindi_text = en_to_hi(eng, model, tokenizer)    \n\n    hinglish = hindi_to_hinglish(Hindi_text)\n    preds.append(hinglish)\n\n# ---- COMET evaluation ----\nmodel_path = download_model(\"Unbabel/wmt22-comet-da\")\ncomet_model = load_from_checkpoint(model_path)\n\n# prepare data list for COMET\ndata = [{\"src\": s, \"mt\": p, \"ref\": r} for s, p, r in zip(srcs, preds, refs)]\n# note: comet_model.predict returns a dict; 'scores' contains numeric values\ncomet_out = comet_model.predict(data, batch_size=8)\ncomet_scores = comet_out[\"scores\"] if isinstance(comet_out, dict) and \"scores\" in comet_out else comet_out\n\nprint(\"Samples evaluated:\", len(preds))\nprint(\"Mean COMET score:\", float(sum(comet_scores) / len(comet_scores)))\n\n# preview\nfor i in range(5):\n    print(f\"\\n--- SAMPLE {i+1} ---\")\n    print(\"SRC :\", srcs[i])\n    print(\"PRED:\", preds[i])\n    print(\"REF :\", refs[i])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.554647Z","iopub.status.idle":"2025-12-19T16:30:36.554997Z","shell.execute_reply.started":"2025-12-19T16:30:36.554820Z","shell.execute_reply":"2025-12-19T16:30:36.554843Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Saving the Model and tokenizer\nmodel.save_pretrained(\"pt_model\")\ntokenizer.save_pretrained(\"tokenizer\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:30:36.555985Z","iopub.status.idle":"2025-12-19T16:30:36.556345Z","shell.execute_reply.started":"2025-12-19T16:30:36.556152Z","shell.execute_reply":"2025-12-19T16:30:36.556174Z"}},"outputs":[],"execution_count":null}]}